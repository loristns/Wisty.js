{"version":3,"file":"index.modern.js","sources":["../src/featurizers/featurizer.ts","../src/utils/levenshtein_distance.ts","../src/utils/fuzzy_match.ts","../src/utils/hashcode.ts","../src/utils/initialize_variable.ts","../src/utils/lstm.ts","../src/utils/trie.ts","../src/featurizers/action_featurizer.ts","../src/featurizers/bow.ts","../src/featurizers/use.ts","../src/featurizers/word_embedding.ts","../src/models/hcn.ts","../src/slots/slot.ts","../src/tools/parse_wistyml.ts","../src/slots/categorical_slot.ts","../src/tools/train_test_split.ts","../src/tools/nlu_formatter.ts","../src/tools/keyed_vectors.ts"],"sourcesContent":["import * as tf from '@tensorflow/tfjs';\n\ntype JSONSerializable = {[key: string]: any};\n\n/**\n * A stateful featurizer that turns queries into numerical representations.\n *\n * @abstract\n */\nexport class Featurizer {\n    /**\n     * An ID used by models for exportations.\n     */\n    readonly id: string;\n\n    /**\n     * The list of every action the model can take.\n     */\n    protected actions: any[];\n\n    /**\n     * The size of the vector returned by the featurizer.\n     * By default it's set to 1 which is the default for a featurizer that returns no features.\n     */\n    readonly size: number = 1;\n\n    /**\n     * Initialize the model, can be asynchronous async code.\n     *\n     * This method is executed by the model during it's initialization,\n     * it will also set the actions attribute.\n     */\n    async init(actions: any[]) {\n        this.actions = actions;\n    }\n\n    /**\n     * Featurizes and handle a text query.\n     *\n     * @remarks\n     * This method can directly return a 1D tensor to provide features to the model.\n     * Alternatively, it can returns data of any type if the Featurizer implement a custom\n     * getOptimizableFeatures method to handle those data.\n     * If this method doesn't return something, no features will be passed to the model.\n     */\n    // eslint-disable-next-line no-unused-vars, class-methods-use-this, no-empty-function\n    async handleQuery(query: string, data?: any): Promise<any> {}\n\n    /**\n     * Turn the data returned by handleQuery into an embedding vector.\n     * This function is used to expose featurizer variables to the model optimizer for training.\n     *\n     * Reimplementing this method is not necessary if your featurizer is not meant to be optimizable\n     * through gradient descent.\n     * In this case, just return the feature vector directly using the handleQuery method.\n     *\n     * @remarks\n     * It's important to keep this function stateless, it should only depend of its tensor argument\n     * and of featurizer's variables.\n     */\n    // eslint-disable-next-line class-methods-use-this\n    getOptimizableFeatures(data: any): tf.Tensor1D {\n        if (data === undefined) {\n            return tf.zeros([1]);\n        }\n\n        return <tf.Tensor1D> data;\n    }\n\n    /**\n     * Let the featurizer know what action the model has taken.\n     */\n    // eslint-disable-next-line no-unused-vars, class-methods-use-this\n    handleAction(action: any): void {}\n\n    /**\n     * Produce an action mask according to featurizer state.\n     * (Generally, this method is reimplemented in stateful featurizers)\n     *\n     * @returns An array of boolean mapping every actions availability.\n     */\n    getActionMask(): boolean[] {\n        return this.actions.map(() => true);\n    }\n\n    /**\n     * Resets the state of the featurizer (if the stateful feature is used).\n     */\n    // eslint-disable-next-line class-methods-use-this\n    resetDialog(): void {}\n\n    /**\n     * Load parameters extracted from a JSON-like document.\n     */\n    // eslint-disable-next-line no-unused-vars, class-methods-use-this\n    load(parameters: JSONSerializable) {}\n\n    /**\n     * Export the featurizer's internal parameters to be serialized along the model.\n     */\n    // eslint-disable-next-line class-methods-use-this\n    async export(): Promise<JSONSerializable> {\n        return {};\n    }\n}\n","/**\n * Compute the Levenshtein distance between two strings using the\n * [Wagner-Fisher algorithm](https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm).\n */\nexport function levenshteinDistance(s1: string, s2: string): number {\n    const d = Array.from(\n        Array(s1.length + 1),\n        () => new Array(s2.length + 1).fill(0)\n    );\n\n    for (let i = 1; i <= s1.length; i += 1) {\n        d[i][0] = i;\n    }\n\n    for (let j = 1; j <= s2.length; j += 1) {\n        d[0][j] = j;\n    }\n\n    for (let j = 0; j < s2.length; j += 1) {\n        for (let i = 0; i < s1.length; i += 1) {\n            const substitutionCost = (s1[i] !== s2[j]) ? 1 : 0;\n\n            d[i + 1][j + 1] = Math.min(\n                d[i][j + 1] + 1,\n                d[i + 1][j] + 1,\n                d[i][j] + substitutionCost\n            );\n        }\n    }\n\n    return d[s1.length][s2.length] / Math.max(s1.length, s2.length, 1);\n}\n","import { levenshteinDistance } from './levenshtein_distance';\n\ntype Match = { extract: string, score: number };\n\nexport function fuzzyMatch(text: string, substring: string): Match {\n    let bestMatch: Match = { extract: undefined, score: 0 };\n\n    for (let i = 0; i < text.length; i += 1) {\n        const extract = text.substring(i, i + substring.length);\n\n        const aMatch = {\n            extract,\n            score: 1 - levenshteinDistance(extract, substring)\n        };\n\n        if (aMatch.score === 1) {\n            return aMatch;\n        }\n\n        if (aMatch.score > bestMatch.score) {\n            bestMatch = aMatch;\n        }\n    }\n\n    return bestMatch;\n}\n","/* eslint-disable no-bitwise */\n\n/**\n * Hash a string.\n * Based on https://stackoverflow.com/a/7616484\n */\nexport function hashcode(input: string) {\n    let hash = 0;\n\n    for (let i = 0; i < input.length; i += 1) {\n        const chr = input.charCodeAt(i);\n\n        hash = ((hash << 5) - hash) + chr;\n        hash |= 0; // Convert to 32bit integer\n    }\n\n    return hash;\n}\n","import * as tf from '@tensorflow/tfjs';\n\n/**\n * Initialize a variable of a given shape.\n */\nexport function initializeVariable(shape: number[], scalar: boolean = false,\n    init: 'he'|'zeros'|'normal' = 'he'): tf.Variable {\n    return tf.tidy(() => {\n        let initializer;\n\n        switch (init) {\n        case 'he':\n            initializer = tf.initializers.heNormal({});\n            break;\n\n        case 'zeros':\n            initializer = tf.initializers.zeros();\n            break;\n\n        case 'normal':\n            initializer = tf.initializers.randomNormal({});\n            break;\n\n        default:\n            throw new Error(\n                `Expected parameter init to take value 'he', 'zeros' or 'normal' not '${init}'.`\n            );\n        }\n\n        let randomTensor = initializer.apply(shape);\n        if (scalar) randomTensor = randomTensor.asScalar();\n\n        return randomTensor.variable();\n    });\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { initializeVariable } from './initialize_variable';\n\ntype LSTMPrediction = {y: tf.Tensor1D, nc: tf.Tensor2D, nh: tf.Tensor2D};\n\n/**\n * An LSTM cell with a dense layer on its top.\n */\nexport class LSTM {\n    // LSTM parameters :\n    private lstmKernel: tf.Tensor;\n    private lstmBias: tf.Tensor;\n    private lstmForgetBias: tf.Tensor;\n    private lstmInitH: tf.Tensor;\n    private lstmInitC: tf.Tensor;\n\n    // Dense layer parameters :\n    private denseWeights: tf.Tensor;\n    private denseBias: tf.Tensor;\n\n    // Let dropout be public to allow to change its value when training/inference.\n    public dropout: number;\n\n    /**\n     * @param inputSize The dimension of the input data.\n     * @param hiddenSize The dimension of the output of the LSTM, passed to the dense layer.\n     * @param outputSize The dimension of the output data.\n     * @param dropout The dropout rate between the LSTM cell and the dense layer.\n     */\n    constructor(inputSize: number, hiddenSize: number, outputSize: number, dropout: number = 0.2) {\n        this.lstmKernel = initializeVariable([inputSize + hiddenSize, hiddenSize * 4]);\n        this.lstmBias = initializeVariable([hiddenSize * 4], false, 'zeros');\n        this.lstmForgetBias = initializeVariable([1], true, 'zeros'); // (scalar)\n        this.lstmInitH = initializeVariable([1, hiddenSize]);\n        this.lstmInitC = initializeVariable([1, hiddenSize]);\n\n        this.denseWeights = initializeVariable([hiddenSize, outputSize]);\n        this.denseBias = initializeVariable([outputSize], false, 'zeros');\n\n        this.dropout = dropout;\n    }\n\n    /**\n     * Gives the initial state values of the LSTM (c and h).\n     *\n     * @param clone If it is necessary to clone states variable or no.\n     */\n    initLSTM(clone: boolean = true): {c: tf.Tensor2D, h: tf.Tensor2D} {\n        return {\n            c: <tf.Tensor2D> (clone ? this.lstmInitC.clone() : this.lstmInitC),\n            h: <tf.Tensor2D> (clone ? this.lstmInitH.clone() : this.lstmInitH)\n        };\n    }\n\n    /**\n     * Make a prediction given an input and state values (c and h).\n     * @param x A vector of shape [inputSize].\n     * @param c LSTM's state value.\n     * @param h LSTM's last output value.\n     * @param mask A vector of ones and zeros of shape [outputSize].\n     */\n    predict(x: tf.Tensor1D, c: tf.Tensor2D, h: tf.Tensor2D, mask?: tf.Tensor1D,\n        temperature: number = 1): LSTMPrediction {\n        return tf.tidy(() => {\n            // Execute the LSTM cell.\n            const [nc, nh] = tf.basicLSTMCell(\n                <tf.Scalar> this.lstmForgetBias,\n                <tf.Tensor2D> this.lstmKernel,\n                <tf.Tensor1D> this.lstmBias,\n                <tf.Tensor2D> tf.stack([x]),\n                h, c\n            );\n\n            // Execute the dense layer on top of the LSTM cell.\n            let y = <tf.Tensor1D> tf\n                .dropout(nh, this.dropout)\n                .matMul(this.denseWeights)\n                .add(this.denseBias)\n                .squeeze()\n                .div(temperature)\n                .softmax()\n                .mul(mask ?? 1);\n\n            // Apply normalization after the mask to get probabilities.\n            y = y.div(tf.sum(y));\n\n            return { y, nc, nh };\n        });\n    }\n\n    /**\n     * Update the given model parameters.\n     */\n    load(weights: {[key: string]: any}) {\n        tf.tidy(() => {\n            // Convert every parameter to a tf variable tensor.\n            this.lstmKernel = tf.tensor(weights.lstmKernel).variable();\n            this.lstmBias = tf.tensor(weights.lstmBias).variable();\n            this.lstmForgetBias = tf.tensor(weights.lstmForgetBias).variable();\n            this.lstmInitH = tf.tensor(weights.lstmInitH).variable();\n            this.lstmInitC = tf.tensor(weights.lstmInitC).variable();\n            this.denseWeights = tf.tensor(weights.denseWeights).variable();\n            this.denseBias = tf.tensor(weights.denseBias).variable();\n        });\n    }\n\n    /**\n     * Return all the LSTM model parameters.\n     */\n    async export(): Promise<{[key: string]: any}> {\n        const exports = {\n            lstmKernel: await this.lstmKernel.array(),\n            lstmBias: await this.lstmBias.array(),\n            lstmForgetBias: await this.lstmForgetBias.array(),\n            lstmInitH: await this.lstmInitH.array(),\n            lstmInitC: await this.lstmInitC.array(),\n            denseWeights: await this.denseWeights.array(),\n            denseBias: await this.denseBias.array()\n        };\n\n        return exports;\n    }\n}\n","/**\n * Inspired by Trie.js\n * https://gist.github.com/tpae/72e1c54471e88b689f85ad2b3940a8f0\n */\n\n/**\n * A node holding a character and having a parent node and several children.\n */\nexport class TrieNode {\n    readonly parent: TrieNode;\n    readonly key: string;\n    readonly childs: {[key: string]: TrieNode};\n    ending: boolean;\n\n    constructor(key: string, parent: TrieNode) {\n        this.key = key;\n        this.parent = parent;\n\n        this.childs = {};\n        this.ending = false;\n    }\n\n    addChild(child: TrieNode) {\n        this.childs[child.key] = child;\n    }\n\n    setEnding() {\n        this.ending = true;\n    }\n}\n\n/**\n * A trie data structure.\n */\nexport class Trie {\n    root: TrieNode;\n\n    constructor() {\n        this.root = new TrieNode('', null);\n    }\n\n    /**\n     * Add a word to the trie.\n     */\n    add(word: string) {\n        let actualNode = this.root;\n\n        for (let i = 0; i < word.length; i += 1) {\n            if (actualNode.childs[word[i]] === undefined) {\n                actualNode.addChild(new TrieNode(word[i], actualNode));\n            }\n\n            actualNode = actualNode.childs[word[i]];\n\n            if (i === word.length - 1) {\n                actualNode.setEnding();\n            }\n        }\n    }\n\n    /**\n     * Split a text into a sequence of words based on the trie vocabulary.\n     *\n     * @param text A text to split into words\n     * @param unknownKey The word used when the word is not found in the trie\n     * @param ignoreTokens Words, not in the trie, to not replace by the unknown key.\n     */\n    split(text: string, unknownKey: string = undefined, ignoreTokens: string[] = []): string[] {\n        const splittedText = [];\n\n        function pushUnknown(word: string) {\n            if (splittedText[splittedText.length - 1] !== unknownKey\n                && !ignoreTokens.includes(word)) {\n                splittedText.push(unknownKey);\n            }\n        }\n\n        let node = this.root;\n        let word = '';\n\n        for (let i = 0; i < text.length; i += 1) {\n            const character = text[i];\n\n            if (node.childs[character] !== undefined) {\n                word += character;\n                node = node.childs[character];\n            } else {\n                if (node.ending) splittedText.push(word);\n                else pushUnknown(word);\n\n                if (this.root.childs[character] !== undefined) {\n                    word = character;\n                    node = this.root.childs[character];\n                } else {\n                    pushUnknown(word);\n                    word = '';\n                    node = this.root;\n                }\n            }\n        }\n\n        if (node.ending) splittedText.push(word);\n        else pushUnknown(word);\n\n        return splittedText;\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Featurizer } from './featurizer';\nimport { initializeVariable } from '../utils';\n\n/**\n * Parameters for ActionFeaturizer constructor.\n */\ninterface ActionFeaturizerArgs {\n    /**\n     * Enable the masking of LUS when the user has just talked.\n     * Enabled by default.\n     */\n    maskLUS?: boolean;\n\n    /**\n     * Enable the masking of the previous action.\n     * Enabled by default.\n     */\n    maskPreviousAction?: boolean;\n\n    /**\n     * The action the bot takes to let the user talk.\n     * Default to 'LUS' (acronym for Let User Speak).\n     */\n    LUSAction?: string;\n}\n\n/**\n * Rule-based featurizer improving model robustness.\n *\n * - Featurize the previous action the model has taken.\n * - Mask the LUS action when the user has just talked.\n *   (Force the model to reply at least once)\n * - Mask the previous action.\n *   (Prevent looping : the model can't take two times in a row the same action)\n */\nexport class ActionFeaturizer extends Featurizer {\n    readonly id = 'Action Featurizer';\n    size: number;\n\n    private LUSAction: any;\n    private maskLUS: boolean;\n    private maskPreviousAction: boolean;\n\n    private userTalked: boolean;\n    private previousAction: any;\n\n    private embeddings: tf.Tensor;\n\n    constructor({\n        maskLUS = true,\n        maskPreviousAction = true,\n        LUSAction = 'LUS'\n    }: ActionFeaturizerArgs = { maskLUS: true, maskPreviousAction: true, LUSAction: 'LUS' }) {\n        super();\n        this.maskLUS = maskLUS;\n        this.maskPreviousAction = maskPreviousAction;\n        this.LUSAction = LUSAction;\n\n        this.resetDialog();\n    }\n\n    async init(actions: any[]) {\n        await super.init(actions);\n\n        this.size = actions.length;\n        this.embeddings = initializeVariable([this.size, this.size]);\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor2D> {\n        return tf.tidy(() => {\n            this.userTalked = query !== '';\n\n            // One-hot encode the previous action.\n            return <tf.Tensor2D> tf.oneHot(\n                [this.actions.indexOf(this.previousAction)],\n                this.actions.length\n            );\n        });\n    }\n\n    getOptimizableFeatures(data: tf.Tensor2D): tf.Tensor1D {\n        return <tf.Tensor1D> data.matMul(this.embeddings).squeeze();\n    }\n\n    handleAction(action: any) {\n        // Store the new action if it's not the LUS action.\n        this.previousAction = action !== this.LUSAction ? action : this.previousAction;\n    }\n\n    getActionMask(): boolean[] {\n        const mask = super.getActionMask();\n\n        // Mask LUS when the user talk and the option is enabled.\n        if (this.maskLUS && this.userTalked) {\n            mask[this.actions.indexOf(this.LUSAction)] = false;\n        }\n\n        // Mask the previous action when the option is enabled and if applicable.\n        if (this.maskPreviousAction && this.actions.includes(this.previousAction)) {\n            mask[this.actions.indexOf(this.previousAction)] = false;\n        }\n\n        return mask;\n    }\n\n    resetDialog() {\n        this.userTalked = false;\n        this.previousAction = undefined;\n    }\n\n    load(parameters: {embeddings: number[][]}) {\n        this.embeddings = tf.tidy(() => tf.tensor(parameters.embeddings).variable());\n    }\n\n    async export(): Promise<{embeddings: number[][]}> {\n        return {\n            embeddings: <number[][]> await this.embeddings.array()\n        };\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Featurizer } from './featurizer';\nimport { hashcode } from '../utils';\n\n/**\n * Featurizes queries as bag of words.\n *\n * The algorithm uses the [hashing trick](https://en.wikipedia.org/wiki/Feature_hashing) to avoid\n * having to store a vocabulary in the memory.\n */\nexport class BOW extends Featurizer {\n    readonly id = 'Bag-of-Words';\n    readonly size: number;\n\n    /**\n     * @param size The vocabulary size you allow to the featurizer.\n     */\n    constructor(size: number) {\n        super();\n        this.size = size;\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor1D> {\n        return tf.tidy(() => {\n            const indexes = query.toLowerCase()\n                .split(/\\W/g)\n                .map((word) => hashcode(word) % this.size);\n\n            return <tf.Tensor1D> tf.oneHot(indexes, this.size).asType('float32').sum(0);\n        });\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport * as use from '@tensorflow-models/universal-sentence-encoder';\nimport { Featurizer } from './featurizer';\n\n/**\n * Featurizes queries using the Universal Sentence Encoder model.\n */\nexport class USE extends Featurizer {\n    readonly id = 'Universal Sentence Encoder';\n\n    private encoder: use.UniversalSentenceEncoder;\n    private emptyEncoding: tf.Tensor1D;\n\n    readonly size = 512;\n\n    async init(actions: any[]) {\n        await super.init(actions);\n        this.encoder = await use.load();\n\n        // Cache the empty string embed (for optimization purpose).\n        this.emptyEncoding = await this.encodeQuery('');\n    }\n\n    /**\n     * Encodes a query using the model.\n     */\n    private async encodeQuery(query: string): Promise<tf.Tensor1D> {\n        const embed = await this.encoder.embed([query]);\n        const squeezedEmbed = <tf.Tensor1D> embed.squeeze();\n        tf.dispose(embed);\n\n        return squeezedEmbed;\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor1D> {\n        // When the query is empty, return the cached empty query encoding.\n        if (!query) {\n            return this.emptyEncoding.clone();\n        }\n\n        return this.encodeQuery(query);\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Featurizer } from './featurizer';\nimport { KeyedVectors } from '../tools';\n\n/**\n * Featurize queries by pooling words embedding using SWEM-concat(*).\n *\n * (*): Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, Qinliang Su, Yizhe Zhang,\n *      Chunyuan Li, Ricardo Henao, Lawrence Carin- 2018.\n *      Baseline Needs More Love: On Simple Word-Embedding-Based Models and\n *      Associated Pooling Mechanisms.\n */\nexport class WordEmbedding extends Featurizer {\n    readonly id = 'Word Embedding';\n    readonly size: number;\n\n    private vectors: KeyedVectors;\n\n    /**\n     * @param vectors The keyed vectors storing the embeddings.\n     */\n    constructor(vectors: KeyedVectors) {\n        super();\n        this.size = 2 * vectors.size;\n        this.vectors = vectors;\n    }\n\n    async init(actions: any[]) {\n        await super.init(actions);\n\n        if (!this.vectors.isLoaded()) await this.vectors.load();\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor1D> {\n        return <tf.Tensor1D> tf.tidy(() => {\n            const tokens = this.vectors.tokenize(query);\n            const embeddings = tokens\n                .map((token) => this.vectors.get(token))\n                .filter((v) => v !== undefined);\n\n            // When there is no embeddable tokens, return a zeros vector.\n            if (embeddings.length === 0) {\n                return tf.zeros([this.size]);\n            }\n\n            const embeddingsMatrix = tf.stack(embeddings);\n\n            return tf.concat([embeddingsMatrix.mean(0), embeddingsMatrix.max(0)]);\n        });\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Featurizer } from '../featurizers';\nimport {\n    LSTM,\n    Story,\n    Stories,\n    Metrics\n} from '../utils';\n\ninterface SampleData {\n    targets: tf.Tensor1D,\n    predictions: tf.Tensor1D,\n    loss: number,\n    isFailing: boolean\n}\n\n/**\n * @callback\n */\ntype TrainingCallback = (metrics: Metrics) => void;\n\n/**\n * Parameters for HCN constructor.\n */\ninterface HCNConstructorArgs {\n    /**\n     * The list of actions the model can take.\n     * (keeping the order the same is important for pretrained models)\n     */\n    actions: string[];\n\n    /**\n     * The list of featurizers the model uses.\n     * (keeping the order the same is important for pretrained models)\n     */\n    featurizers: Featurizer[];\n\n    /**\n     * The output size of the LSTM cell.\n     * Default is set to 32 units.\n     */\n    hiddenSize?: number;\n\n    /**\n     * The optimization algorithm used for training.\n     * By default, Adam with a learning rate of 0.01 is used.\n     */\n    optimizer?: tf.Optimizer;\n\n    /**\n     * Temperature of the model softmax, used to calibrate confidence estimation.\n     * By default, the temperature is 1 but you usually want it higher to make less overconfident.\n     */\n    temperature?: number;\n\n    /**\n     * The percentage of units to dropout between the LSTM cell layer and the dense.\n     * Useful for regularizing the model. It's disabled by default (value = 0).\n     */\n    dropout?: number;\n}\n\n/**\n * Parameters for HCN train method.\n */\ninterface HCNTrainArgs {\n    /**\n     * Training stories to learn from.\n     */\n    stories: Stories;\n\n    /**\n     * Number of times the model will be passed the whole set of training stories during training.\n     * Default is set to 12 epochs.\n     */\n    nEpochs?: number;\n\n    /**\n     * After each epoch, this callback function will be executed with the metrics collected\n     * during the epoch.\n     */\n    onEpochEnd?: TrainingCallback;\n}\n\n/**\n * An implementation of Hybrid Code Networks(*) dialog manager.\n *\n * (*): Williams, Asadi, Zweig - 2017.\n *      Hybrid Code Networks: practical and efï¬cient end-to-end dialog control with supervised\n *      and reinforcement learning.\n */\nexport class HCN {\n    private actions: string[];\n    private featurizers: Featurizer[];\n    private optimizer: tf.Optimizer;\n\n    private inputSize: number;\n    private hiddenSize: number;\n    private outputSize: number;\n\n    private lstm: LSTM;\n    private lstmH: tf.Tensor2D;\n    private lstmC: tf.Tensor2D;\n    private lstmTemperature: number;\n    private lstmDropout: number;\n\n    /**\n     * Defines the model.\n     *\n     * To fully initialize the model, run the async *init()* method.\n     */\n    constructor({\n        actions,\n        featurizers,\n        hiddenSize = 32,\n        optimizer = tf.train.adam(0.01),\n        temperature = 1,\n        dropout = 0\n    }: HCNConstructorArgs) {\n        this.actions = actions;\n        this.featurizers = featurizers;\n        this.optimizer = optimizer;\n\n        this.hiddenSize = hiddenSize;\n        this.outputSize = actions.length;\n\n        this.lstmTemperature = temperature;\n        this.lstmDropout = dropout;\n    }\n\n    /**\n     * Initialize the model and its featurizers.\n     */\n    async init() {\n        // Initialize asynchronously all featurizers.\n        await Promise.all(\n            this.featurizers.map((featurizer) => featurizer.init(this.actions))\n        );\n\n        // The model input size is the sum of the sizes of features vectors.\n        this.inputSize = this.featurizers\n            .map((featurizer) => featurizer.size)\n            .reduce((acc, size) => acc + size, 1);\n\n        this.lstm = new LSTM(this.inputSize, this.hiddenSize, this.outputSize, this.lstmDropout);\n\n        this.resetDialog();\n    }\n\n    /**\n     * Resets the state of the model and its featurizers.\n     */\n    resetDialog() {\n        this.featurizers.forEach((featurizer) => featurizer.resetDialog());\n        ({ c: this.lstmC, h: this.lstmH } = this.lstm.initLSTM());\n    }\n\n    /**\n     * Get the data returned from every featurizer's handleQuery method.\n     */\n    private async handleQuery(query: string, data?: any): Promise<any[]> {\n        return Promise.all(\n            this.featurizers.map((featurizer) => featurizer.handleQuery(query, data))\n        );\n    }\n\n    /**\n     * Get the embedding vector resulted from every featurizers.\n     */\n    private getOptimizableFeatures(features: tf.Tensor[]): tf.Tensor1D {\n        return tf.tidy(() => {\n            const embeddings = this.featurizers.map(\n                (featurizer, idx) => featurizer.getOptimizableFeatures(features[idx])\n            );\n\n            // Add a zero to make tf.concat work consistently even with only one featurizer.\n            embeddings.push(tf.zeros([1]));\n\n            return <tf.Tensor1D> tf.concat(embeddings);\n        });\n    }\n\n    /**\n     * Inform every featurizers of the taken action.\n     */\n    private handleAction(action: string) {\n        this.featurizers.map((featurizer) => featurizer.handleAction(action));\n    }\n\n    /**\n     * Get the final action mask resulted from every featurizers.\n     */\n    private getActionMask(): tf.Tensor1D {\n        return tf.tidy(() => this.featurizers\n            // Get action mask and convert them to tensors.\n            .map((featurizer) => <tf.Tensor1D> tf.tensor(\n                featurizer.getActionMask(),\n                undefined, 'float32'\n            ))\n            // Compute the product of every masks.\n            .reduce((acc, mask) => tf.mul(acc, mask), tf.ones([this.actions.length])));\n    }\n\n    /**\n     * Trains the model on a single training story.\n     */\n    private async fitStory(story: Story): Promise<SampleData> {\n        this.resetDialog();\n\n        // 1. Prepare the input data.\n        const inputs: any[][] = [];\n        const masks: tf.Tensor1D[] = [];\n        const targets: tf.Tensor1D[] = [];\n\n        // For each story's state...\n        for (let stateIdx = 0; stateIdx < story.length; stateIdx += 1) {\n            const state = story[stateIdx];\n\n            // The query must be featurized before moving to the next state.\n            // eslint-disable-next-line no-await-in-loop\n            inputs.push(await this.handleQuery(state.query, state.data));\n\n            masks.push(this.getActionMask());\n\n            targets.push(\n                <tf.Tensor1D> tf.oneHot(\n                    this.actions.indexOf(state.action),\n                    this.outputSize\n                )\n            );\n\n            this.handleAction(state.action);\n        }\n\n        // 2. Fit the sequence.\n        let data: SampleData;\n\n        this.optimizer.minimize(() => {\n            let { c, h } = this.lstm.initLSTM(false);\n\n            // Make a prediction for each step of the input sequence.\n            const predictions = inputs.map((features, idx) => {\n                const statePred = this.lstm.predict(\n                    <tf.Tensor1D> this.getOptimizableFeatures(features),\n                    <tf.Tensor2D> c,\n                    <tf.Tensor2D> h,\n                    <tf.Tensor1D> masks[idx]\n                );\n\n                c = statePred.nc;\n                h = statePred.nh;\n\n                return statePred.y;\n            });\n\n            const targetsMatrix = tf.stack(targets);\n            const predictionsMatrix = tf.stack(predictions);\n\n            // Compare the predicted sequence with the target.\n            const lossScalar = <tf.Scalar> tf.metrics.categoricalCrossentropy(\n                targetsMatrix, predictionsMatrix\n            ).mean();\n\n            // Store the necessary data to build metrics.\n            data = {\n                targets: tf.keep(targetsMatrix.argMax(1)),\n                predictions: tf.keep(predictionsMatrix.argMax(1)),\n                loss: <number> lossScalar.arraySync(),\n                isFailing: tf.metrics\n                    .categoricalAccuracy(targetsMatrix, predictionsMatrix)\n                    .mean()\n                    .arraySync() < 0.999\n            };\n\n            // Return the loss to the optimizer to update the model.\n            return lossScalar;\n        });\n\n        // BUG: two tensors leak in the memory at each loop :/\n        tf.dispose([inputs, targets]);\n\n        return data;\n    }\n\n    /**\n     * Trains the model using the training stories.\n     *\n     * @returns Metrics collected from the last epoch (that correspond to the trained model).\n     */\n    async train({ stories, nEpochs = 12, onEpochEnd = undefined }: HCNTrainArgs): Promise<Metrics> {\n        const storiesEntries = Object.entries(stories);\n        let epochMetrics: Metrics;\n\n        // For each epoch...\n        for (let epoch = 0; epoch < nEpochs; epoch += 1) {\n            const allTargets: tf.Tensor1D[] = [];\n            const allPredictions: tf.Tensor1D[] = [];\n            const allLosses: number[] = [];\n            const failingSamples: string[] = [];\n\n            // For each training story...\n            for (let storyIdx = 0; storyIdx < storiesEntries.length; storyIdx += 1) {\n                const [storyTitle, story] = storiesEntries[storyIdx];\n\n                // (Each story must be fitted sequentially)\n                // eslint-disable-next-line no-await-in-loop\n                const storyData = await this.fitStory(story);\n\n                allTargets.push(storyData.targets);\n                allPredictions.push(storyData.predictions);\n                allLosses.push(storyData.loss);\n\n                if (storyData.isFailing) {\n                    failingSamples.push(storyTitle);\n                }\n            }\n\n            // Build the metrics.\n            const confusionMatrix = tf.tidy(() => tf.math.confusionMatrix(\n                tf.concat(allTargets),\n                tf.concat(allPredictions),\n                this.outputSize\n            ));\n\n            const truePredictions = tf.tidy(() => confusionMatrix\n                .mul(tf.eye(...confusionMatrix.shape))\n                .sum(0));\n\n            epochMetrics = {\n                epoch,\n                failingSamples,\n\n                accuracy: <number> tf.tidy(() => (\n                    truePredictions.sum().div(confusionMatrix.sum()).arraySync()\n                )),\n\n                recall: <number> tf.tidy(() => (\n                    truePredictions.div(confusionMatrix.sum(1)).mean().arraySync()\n                )),\n\n                precision: <number> tf.tidy(() => (\n                    truePredictions.div(confusionMatrix.sum(0)).mean().arraySync()\n                )),\n\n                loss: allLosses.reduce((a, b) => a + b) / allLosses.length\n            };\n\n            // Clear the tensors.\n            tf.dispose(allTargets);\n            tf.dispose(allPredictions);\n            tf.dispose([truePredictions, confusionMatrix]);\n\n            if (onEpochEnd !== undefined) {\n                onEpochEnd(epochMetrics);\n            }\n        }\n\n        this.resetDialog();\n        return epochMetrics;\n    }\n\n    /**\n     * Predict an action resulting from the given query.\n     *\n     * @param query The given query from the user.\n     * @returns The predicted action from the model and its confidence.\n     */\n    async predict(query: string):\n        Promise<{action: string, confidence: number}> {\n        // At inference, dropout is disabled\n        this.lstm.dropout = 0;\n\n        const features = this.getOptimizableFeatures(await this.handleQuery(query));\n        const masks = this.getActionMask();\n\n        const prediction = this.lstm.predict(\n            features, this.lstmC, this.lstmH, masks, this.lstmTemperature\n        );\n\n        // Update lstm internal state\n        tf.dispose([this.lstmC, this.lstmH]);\n        this.lstmC = prediction.nc.clone();\n        this.lstmH = prediction.nh.clone();\n\n        const actionIdx = <number> tf.tidy(() => prediction.y.argMax().arraySync());\n        const confidence = <number> tf.tidy(() => prediction.y.arraySync()[actionIdx]);\n\n        // Clear the memory\n        tf.dispose([features, masks]);\n        tf.dispose(prediction);\n\n        // Retablish dropout (just in case)\n        this.lstm.dropout = this.lstmDropout;\n\n        this.handleAction(this.actions[actionIdx]);\n\n        return { action: this.actions[actionIdx], confidence };\n    }\n\n    /**\n     * Evaluate the model using stories.\n     *\n     * @param stories Validation stories to evaluate the model.\n     * @returns Validation metrics based on the results from the stories.\n     */\n    async score(stories: Stories): Promise<Metrics> {\n        const storiesEntries = Object.entries(stories);\n\n        const targets: number[] = [];\n        const predictions: number[] = [];\n        const confidences: number[] = [];\n        const failingSamples: string[] = [];\n\n        // For each stories and states, make predictions.\n        for (let storyIdx = 0; storyIdx < storiesEntries.length; storyIdx += 1) {\n            const [storyTitle, story] = storiesEntries[storyIdx];\n            this.resetDialog();\n\n            for (let stateIdx = 0; stateIdx < story.length; stateIdx += 1) {\n                const state = story[stateIdx];\n\n                // eslint-disable-next-line no-await-in-loop\n                const { action, confidence } = await this.predict(state.query);\n\n                targets.push(this.actions.indexOf(state.action));\n                predictions.push(this.actions.indexOf(action));\n                confidences.push(confidence);\n\n                if (action !== state.action && !failingSamples.includes(storyTitle)) {\n                    failingSamples.push(storyTitle);\n                }\n            }\n        }\n\n        // Build a confusion matrix out of the prediction and build the metrics.\n        const confusionMatrix = tf.tidy(() => tf.math.confusionMatrix(\n            tf.tensor(targets), tf.tensor(predictions), this.outputSize\n        ));\n\n        const truePredictions = tf.tidy(() => confusionMatrix\n            .mul(tf.eye(...confusionMatrix.shape))\n            .sum(0));\n\n        const metrics: Metrics = {\n            failingSamples,\n\n            accuracy: <number> tf.tidy(() => (\n                truePredictions.sum().div(confusionMatrix.sum()).arraySync()\n            )),\n\n            recall: <number> tf.tidy(() => (\n                truePredictions.div(confusionMatrix.sum(1)).mean().arraySync()\n            )),\n\n            precision: <number> tf.tidy(() => (\n                truePredictions.div(confusionMatrix.sum(0)).mean().arraySync()\n            )),\n\n            averageConfidence: confidences.reduce((a, b) => a + b) / confidences.length\n        };\n\n        tf.dispose([truePredictions, confusionMatrix]);\n\n        this.resetDialog();\n        return metrics;\n    }\n\n    /**\n     * Load the models parameters from a JSON formatted string.\n     */\n    load(json: string) {\n        const parameters = JSON.parse(json);\n\n        this.featurizers.forEach((featurizer) => {\n            featurizer.load(parameters[featurizer.id]);\n        });\n\n        this.lstm.load(parameters.lstm);\n\n        this.resetDialog();\n    }\n\n    /**\n     * Export the models parameters in a JSON format.\n     */\n    async export(): Promise<string> {\n        const parameters = { lstm: await this.lstm.export() };\n\n        for (let idx = 0; idx < this.featurizers.length; idx += 1) {\n            const featurizer = this.featurizers[idx];\n\n            // eslint-disable-next-line no-await-in-loop\n            parameters[featurizer.id] = await featurizer.export();\n        }\n\n        return JSON.stringify(parameters);\n    }\n}\n","import { Featurizer } from '../featurizers';\n\n/**\n * An extension of featurizer that holds a value in its state.\n * @abstract\n */\nexport class Slot<Value> extends Featurizer {\n    /**\n     * The list of actions that can be taken by the model only when the slot is defined.\n     */\n    private dependantActions: string[];\n\n    /**\n     * The list of actions that can be taken by the model only when the slot is undefined.\n     */\n    private invDependantActions: string[];\n\n    /**\n     * Stores the value of the slot.\n     */\n    private value: Value;\n\n    /**\n     * @param dependantActions The list of actions that can be taken by the model\n     *                         only when the slot is defined.\n     * @param invDependantActions The list of actions that can be taken by the model\n     *                            only when the slot is undefined.\n     */\n    constructor(dependantActions: string[], invDependantActions: string[]) {\n        super();\n        this.dependantActions = dependantActions;\n        this.invDependantActions = invDependantActions;\n    }\n\n    getActionMask(): boolean[] {\n        return this.actions.map((action) => {\n            const isDefined = this.value !== undefined;\n            const isDependant = this.dependantActions.includes(action);\n            const isInvDependant = this.invDependantActions.includes(action);\n\n            return (!isDefined && (!isDependant || isInvDependant))\n                || (isDefined && !isInvDependant);\n        });\n    }\n\n    resetDialog(): void {\n        this.value = undefined;\n    }\n\n    /**\n     * Retrieves the value of the slot.\n     */\n    getValue(): Value {\n        return this.value;\n    }\n\n    /**\n     * Redefine a new value for the slot.\n     */\n    setValue(value: Value) {\n        this.value = value;\n    }\n}\n","import * as commonmark from 'commonmark';\nimport { Stories, State, ExtractedValue } from '../utils';\n\nexport interface ParsedWistyML {\n    stories: Stories,\n    extractedValues: {[title: string]: ExtractedValue[]}\n}\n\nenum StateStep {\n    query, data, action, empty\n}\n\n/**\n * Parse a source string formatted according the WistyML syntax.\n * Usually, this source string is extracted using fetch or from a file.\n */\nexport function parseWistyML(source: string): ParsedWistyML {\n    const markdownParser = new commonmark.Parser({ smart: true });\n    const parsedMarkdown = markdownParser.parse(source);\n\n    const walker = parsedMarkdown.walker();\n    let event = walker.next();\n\n    const state = {\n        // General\n        section: undefined,\n        status: undefined,\n        // Slot\n        slot: undefined,\n        sentence: '',\n        extractedValues: [],\n        // Story\n        storyName: undefined,\n        story: [],\n        currentState: undefined,\n        stateStep: StateStep.empty\n    };\n\n    const parsedSource: ParsedWistyML = { stories: {}, extractedValues: {} };\n\n    function endStory() {\n        if (state.storyName !== undefined) {\n            // Push the last step\n            state.story.push(state.currentState);\n            // Add a last LUS state\n            state.story.push({ query: '', action: 'LUS' });\n\n            // Push the story\n            parsedSource.stories[state.storyName] = state.story;\n\n            // Reset the state\n            state.storyName = undefined;\n            state.story = [];\n            state.currentState = undefined;\n            state.stateStep = StateStep.empty;\n        }\n    }\n\n    function pushSlot(sentence: string) {\n        if (parsedSource.extractedValues[state.slot] === undefined) {\n            parsedSource.extractedValues[state.slot] = [];\n        }\n\n        // Push the samples to the parsed digest\n        parsedSource.extractedValues[state.slot].push(\n            // Add the complete sentence to the extracted values.\n            ...state.extractedValues.map((extractedValues) => ({ sentence, ...extractedValues }))\n        );\n\n        state.extractedValues = [];\n        state.sentence = '';\n    }\n\n    while (event !== null) {\n        const { entering, node } = event;\n        const {\n            type,\n            literal,\n            destination,\n            info,\n            level\n        } = node;\n\n        /*\n            Handle section change\n            ## section name\n        */\n        if (entering && type === 'heading' && level === 2) {\n            state.status = 'new-section';\n            endStory();\n\n        /*\n            Get the new section name\n        */\n        } else if (type === 'text' && state.status === 'new-section') {\n            state.section = literal;\n            state.status = `${literal}.entering`;\n\n        /*\n            Slots section parsing :\n            ## wisty.slots\n        */\n        } else if (state.section === 'wisty.slots') {\n            /*\n                Entering a new slot\n\n                ## wisty.slots\n                ...\n                ### slot name\n            */\n            if (entering && type === 'heading' && level === 3) {\n                state.status = 'slots.new-slot';\n\n            /*\n                Get the slot name\n            */\n            } else if (type === 'text' && state.status === 'slots.new-slot') {\n                if (!(literal in parsedSource.extractedValues)) {\n                    parsedSource.extractedValues[literal] = [];\n                }\n\n                state.slot = literal;\n                state.status = 'slots.in-slot';\n\n            /*\n                Entering a sample\n\n                - sample\n            */\n            } else if (entering && type === 'item' && state.status === 'slots.in-slot') {\n                state.status = 'slots.in-sample';\n\n            /*\n                Getting sample text\n            */\n            } else if (type === 'text' && state.status === 'slots.in-sample') {\n                state.sentence += literal;\n\n            /*\n                Getting selected sample text\n\n                `selected text`\n            */\n            } else if (type === 'code' && state.status === 'slots.in-sample') {\n                state.extractedValues.push({\n                    extract: literal,\n                    start: state.sentence.length,\n                    end: state.sentence.length + literal.length - 1\n                });\n\n                state.sentence += literal;\n\n            /*\n                Exiting a sample\n            */\n            } else if (!entering && type === 'item' && state.status === 'slots.in-sample') {\n                pushSlot(state.sentence);\n                state.status = 'slots.in-slot';\n            }\n\n        /*\n            Stories section parsing :\n\n            ## wisty.stories\n        */\n        } else if (state.section === 'wisty.stories') {\n            /*\n                Entering a new story\n\n                ## wisty.stories\n                ...\n                ### story name\n            */\n            if (entering && type === 'heading' && level === 3) {\n                state.status = 'stories.new-story';\n\n                endStory();\n\n            /*\n                Get the new story name\n            */\n            } else if (type === 'text' && state.status === 'stories.new-story') {\n                state.storyName = literal;\n                state.status = 'stories.in-story';\n\n            /*\n                State management code\n            */\n            } else if (\n                entering\n                && ['block_quote', 'item', 'code_block'].includes(type)\n                && state.status === 'stories.in-story') {\n                // Identify the type of data in the state (the \"step\")\n                let newStep: StateStep;\n\n                switch (type) {\n                case 'block_quote':\n                    newStep = StateStep.query;\n                    break;\n\n                case 'item':\n                    newStep = StateStep.action;\n                    break;\n\n                case 'code_block':\n                    newStep = StateStep.data;\n                    break;\n\n                default: break;\n                }\n\n                /*\n                    If the new step is breaking the order of enunciation of the state step,\n                    we begin a new state.\n                */\n                if (newStep <= state.stateStep) {\n                    // Push the previous step\n                    if (state.currentState !== undefined) {\n                        state.story.push(state.currentState);\n\n                        if (newStep === StateStep.query && state.currentState.action !== 'LUS') {\n                            state.story.push(<State> { query: '', action: 'LUS' });\n                        }\n                    }\n\n                    state.currentState = { query: '', action: 'LUS' };\n                }\n\n                switch (newStep) {\n                case StateStep.query:\n                    state.status = 'stories.new-query';\n                    break;\n\n                case StateStep.action:\n                    state.status = 'stories.new-action';\n                    break;\n\n                case StateStep.data:\n                    if (info === 'json') {\n                        state.currentState.data = JSON.parse(literal);\n                    }\n                    break;\n\n                default: break;\n                }\n\n                state.stateStep = newStep;\n\n            /*\n                Get the query\n            */\n            } else if (type === 'text' && state.status === 'stories.new-query') {\n                state.currentState.query += literal;\n\n            /*\n                Slot training in a query\n            */\n            } else if (entering && type === 'link' && state.status === 'stories.new-query') {\n                state.slot = destination;\n                state.status = 'stories.new-slot';\n\n            /*\n                Get the extracted text from the slot\n            */\n            } else if (type === 'text' && state.status === 'stories.new-slot') {\n                state.extractedValues.push({\n                    extract: literal,\n                    start: state.currentState.query.length,\n                    end: state.currentState.query.length + literal.length - 1\n                });\n\n                state.currentState.query += literal;\n                state.status = 'stories.new-query';\n\n            /*\n                End the query block\n            */\n            } else if (!entering && type === 'block_quote' && state.status === 'stories.new-query') {\n                if (state.extractedValues.length > 0) pushSlot(state.currentState.query);\n\n                state.status = 'stories.in-story';\n\n            /*\n                Get the action name\n            */\n            } else if (type === 'text' && state.status === 'stories.new-action') {\n                state.currentState.action = literal;\n                state.status = 'stories.in-story';\n            }\n        }\n\n        event = walker.next();\n    }\n\n    endStory();\n\n    return parsedSource;\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Slot } from './slot';\nimport { fuzzyMatch } from '../utils';\n\ntype Categories = {[category: string]: string[]};\ntype CategoricalValue = { category: string, extract: string, score: number };\n\n/**\n * Parameters for Categorical Slot constructor.\n */\ninterface CategoricalSlotArgs {\n    /**\n     * The name of the slot.\n     */\n    name: string;\n\n    /**\n     * An object with the name of the category as a key and an array of synonyms that belong to\n     * the category as a value.\n     */\n    categories: Categories;\n\n    /**\n     * The list of actions that can be taken by the model only when the slot is defined.\n     */\n    dependantActions?: string[];\n\n    /**\n     * The list of actions that can be taken by the model only when the slot is undefined.\n     */\n    invDependantActions?: string[];\n\n    /**\n     * The minimum similarity to get selected as a value. (based on Leveinshtein Distance)\n     */\n    threshold?: number;\n}\n\n/**\n * A slot that stores a categorical value extracted using fuzzy string matching.\n */\nexport class CategoricalSlot extends Slot<CategoricalValue> {\n    readonly id: string;\n    readonly size: number;\n\n    private categoryNames: string[];\n    private categories: Categories;\n\n    private threshold: number;\n\n    constructor({\n        name,\n        categories,\n        dependantActions = [],\n        invDependantActions = [],\n        threshold = 0.75\n    }: CategoricalSlotArgs) {\n        super(dependantActions, invDependantActions);\n\n        this.categoryNames = Object.keys(categories);\n        this.categories = categories;\n        this.threshold = threshold;\n\n        this.id = `${name}#Categorical`;\n        this.size = 2 * this.categoryNames.length;\n    }\n\n    async init(actions: any[]) {\n        await super.init(actions);\n        this.resetDialog();\n    }\n\n    private oneHotValue(value: CategoricalValue): tf.Tensor1D {\n        const categoryNames = Object.keys(this.categories);\n\n        return <tf.Tensor1D> tf.oneHot(\n            categoryNames.indexOf(value.category),\n            categoryNames.length\n        );\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor1D> {\n        const previousValue = this.getValue();\n        let bestValue: CategoricalValue = { category: undefined, extract: undefined, score: 0 };\n\n        // For each category...\n        Object.entries(this.categories).forEach(([category, keywords]) => {\n            // Find the best match of the category.\n            const match = keywords\n                .map((keyword) => fuzzyMatch(query.toLowerCase(), keyword.toLowerCase()))\n\n                .filter((m) => m.score >= this.threshold)\n\n                .reduce(\n                    (hm, m) => (hm.score > m.score ? hm : m),\n                    { extract: undefined, score: 0 }\n                );\n\n            // The best match is preferably a match of a different category with the highest score.\n            if (match.extract !== undefined) {\n                const currentUneqPrevious = category !== previousValue.category;\n                const bestEqPrevious = bestValue.category === previousValue.category;\n                const betterScore = match.score > bestValue.score;\n\n                if (bestValue.category === undefined\n                    || (currentUneqPrevious && betterScore)\n                    || (bestEqPrevious && currentUneqPrevious)\n                    || (bestEqPrevious && betterScore)\n                ) {\n                    bestValue = { category, ...match };\n                }\n            }\n        });\n\n        const features = tf.tidy(() => (\n            tf.concat([\n                this.oneHotValue(bestValue),\n                this.oneHotValue(this.getValue())\n            ])\n        ));\n\n        if (bestValue.category !== undefined) {\n            this.setValue(bestValue);\n        }\n\n        return features;\n    }\n\n    getValue(): CategoricalValue {\n        if (super.getValue() === undefined) {\n            return { category: undefined, extract: undefined, score: 0 };\n        }\n\n        return super.getValue();\n    }\n}\n","import { Stories } from '../utils';\n\n/**\n * Split a set stories into random train and test subsets.\n *\n * @param stories Some stories.\n * @param testSize The proportion of stories to put in the test subset.\n */\nexport function trainTestSplit(stories: Stories, testSize: number):\n    {train: Stories, test: Stories} {\n    const trainEntries = Object.entries(stories);\n    const testEntries = [];\n\n    for (let i = 0; i / trainEntries.length < testSize; i += 1) {\n        testEntries.push(...trainEntries.splice(\n            Math.floor(Math.random() * trainEntries.length), 1\n        ));\n    }\n\n    return {\n        train: Object.fromEntries(trainEntries),\n        test: Object.fromEntries(testEntries)\n    };\n}\n","import { HCN } from '../models';\nimport { Slot } from '../slots';\n\n/**\n * Parameters for NLUFormatter.\n */\ninterface NLUFormatterArgs {\n    model: HCN;\n    slots?: Slot<any>[];\n    LUSAction?: string;\n}\n\n/**\n * An NLU digests containing the input query, the list of bot action to take,\n * the overall confidence of the turn (product of action's confidences) and slots values.\n */\ninterface NLUDigest {\n    /**\n     * The raw input query.\n     */\n    query: string;\n\n    /**\n     * An array of actions names.\n     */\n    actions: string[];\n\n    /**\n     * The overall confidence of the turn.\n     */\n    turnConfidence: number;\n\n    /**\n     * The value of each slot.\n     */\n    slots: {[slot: string]: any};\n}\n\n/**\n * An utility class using HCN methods and Slots to offer an higher level API\n * looking like NLU librairies.\n */\nexport class NLUFormatter {\n    private model: HCN;\n    private slots: Slot<any>[];\n    private LUSAction: string;\n\n    constructor({\n        model,\n        slots = [],\n        LUSAction = 'LUS'\n    }: NLUFormatterArgs) {\n        this.model = model;\n        this.slots = slots;\n        this.LUSAction = LUSAction;\n    }\n\n    /**\n     * Turn a query into a NLU digest.\n     */\n    async ask(query: string): Promise<NLUDigest> {\n        const digest: NLUDigest = {\n            query,\n            actions: [],\n            turnConfidence: 1,\n            slots: {}\n        };\n\n        let { action, confidence } = await this.model.predict(query);\n\n        while (action !== this.LUSAction) {\n            digest.actions.push(action);\n            digest.turnConfidence *= confidence;\n\n            // eslint-disable-next-line no-await-in-loop\n            ({ action, confidence } = await this.model.predict(''));\n        }\n\n        this.slots.forEach((slot) => {\n            digest.slots[slot.id] = slot.getValue();\n        });\n\n        return digest;\n    }\n\n    /**\n     * Reset the model state.\n     */\n    resetDialog() {\n        this.model.resetDialog();\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { levenshteinDistance, Trie } from '../utils';\n\n/**\n * Parameters for KeyedVectors.\n */\ninterface KeyedVectorsArgs {\n    loaderFunction(): Promise<string>;\n    size: number;\n    tokenization?: 'word' | 'byte_pair';\n    cased?: boolean;\n    maxDistance?: number;\n    unknownKey?: string;\n}\n\n/**\n * A reusable class storing words embeddings for functions and class that needs it.\n */\nexport class KeyedVectors {\n    private vectors: {[key: string]: number[]};\n\n    private loaderFunction: () => Promise<string>;\n\n    readonly size: number;\n    private tokenization: 'word' | 'byte_pair';\n    private cased: boolean;\n    private trie: Trie;\n\n    private maxDistance: number;\n    private unknownKey: string;\n\n    /**\n     * Build a KeyedVector.\n     */\n    constructor({\n        loaderFunction,\n        size,\n        tokenization = 'word',\n        cased = false,\n        maxDistance = 0.5,\n        unknownKey = undefined\n    }: KeyedVectorsArgs) {\n        if (!['word', 'byte_pair'].includes(tokenization)) {\n            throw new Error('KeyedVector tokenization setting must be \"word\" or \"byte_pair\"');\n        }\n\n        this.loaderFunction = loaderFunction;\n\n        this.size = size;\n        this.tokenization = tokenization;\n        this.cased = cased;\n\n        this.maxDistance = maxDistance;\n        this.unknownKey = unknownKey;\n    }\n\n    /**\n     * Return every keys stored as an array.\n     */\n    keys(): string[] {\n        return Object.keys(this.vectors);\n    }\n\n    /**\n     * Load the word embeddings.\n     */\n    async load() {\n        this.vectors = JSON.parse(await this.loaderFunction());\n\n        if (this.tokenization === 'byte_pair') {\n            this.trie = new Trie();\n            this.keys().forEach((key) => this.trie.add(key));\n        }\n    }\n\n    /**\n     * Check if the word embeddings were loaded.\n     */\n    isLoaded(): boolean {\n        return this.vectors !== undefined;\n    }\n\n    /**\n     * Return the vector associated with a key.\n     * If the key is not part of the vocabulary, it will use a similar key according to\n     * the leveinshtein distance.\n     * If no similar keys are below `maxDistance`, it will return the unknown key vector or\n     * undefined.\n     */\n    get(key: string): tf.Tensor1D {\n        // If the token in in the vocabulary, just use its embedding.\n        if (this.keys().includes(key)) {\n            return tf.tensor1d(this.vectors[key]);\n        }\n\n        // If the token is out of vocabulary, use the most similarly spelled token instead.\n        let bestKey: string;\n        let lowestDistance: number = Infinity;\n\n        this.keys().forEach((knownKey) => {\n            const distance = levenshteinDistance(knownKey, key);\n\n            if (distance < lowestDistance) {\n                bestKey = knownKey;\n                lowestDistance = distance;\n            }\n        });\n\n        if (lowestDistance <= this.maxDistance) {\n            return tf.tensor(this.vectors[bestKey]);\n        }\n\n        // If no tokens are enough similar, return the unknownKey vector or undefined.\n        if (this.unknownKey !== undefined) {\n            return tf.tensor(this.vectors[this.unknownKey]);\n        }\n\n        return undefined;\n    }\n\n    /**\n     * Tokenize a string at each non-word character.\n     *\n     * @param text  A non tokenized text string.\n     */\n    // eslint-disable-next-line class-methods-use-this\n    private wordTokenize(text: string): string[] {\n        return text.split(/\\W/g).filter((token) => token.length > 0);\n    }\n\n    /**\n     * Tokenize a string based on the vocabulary.\n     *\n     * @param text A non tokenized text string.\n     */\n    private bytePairTokenize(text: string): string[] {\n        const pretext = ` ${text}`.split(' ').join('â–');\n\n        return this.trie.split(pretext, this.unknownKey, ['â–']);\n    }\n\n    /**\n     * Tokenize a string based on the settings.\n     *\n     * @param text A raw text string.\n     */\n    tokenize(text: string): string[] {\n        if (this.cased) {\n            // eslint-disable-next-line no-param-reassign\n            text = text.toLowerCase();\n        }\n\n        switch (this.tokenization) {\n        case 'word':\n            return this.wordTokenize(text);\n\n        case 'byte_pair':\n            return this.bytePairTokenize(text);\n\n        default:\n            throw new Error('KeyedVector tokenization setting must be \"word\" or \"byte_pair\"');\n        }\n    }\n}\n"],"names":["Featurizer","constructor","this","[object Object]","actions","query","data","getOptimizableFeatures","undefined","tf","handleAction","action","getActionMask","map","resetDialog","load","parameters","levenshteinDistance","s1","s2","d","Array","from","length","fill","i","j","Math","min","max","fuzzyMatch","text","substring","bestMatch","extract","score","aMatch","hashcode","input","hash","charCodeAt","initializeVariable","shape","scalar","init","initializer","heNormal","zeros","randomNormal","Error","randomTensor","apply","asScalar","variable","LSTM","inputSize","hiddenSize","outputSize","dropout","lstmKernel","lstmBias","lstmForgetBias","lstmInitH","lstmInitC","denseWeights","denseBias","initLSTM","clone","c","h","predict","x","mask","temperature","nc","nh","y","matMul","add","squeeze","div","softmax","mul","weights","array","TrieNode","key","parent","childs","ending","addChild","child","setEnding","Trie","root","word","actualNode","split","unknownKey","ignoreTokens","splittedText","pushUnknown","includes","push","node","character","maskLUS","maskPreviousAction","LUSAction","super","size","embeddings","userTalked","indexOf","previousAction","indexes","toLowerCase","asType","sum","encoder","use","emptyEncoding","encodeQuery","embed","squeezedEmbed","vectors","isLoaded","tokenize","token","get","filter","v","embeddingsMatrix","mean","featurizers","optimizer","adam","lstmTemperature","lstmDropout","Promise","all","featurizer","reduce","acc","lstm","forEach","lstmC","lstmH","handleQuery","features","idx","story","inputs","masks","targets","stateIdx","state","minimize","predictions","statePred","targetsMatrix","predictionsMatrix","lossScalar","categoricalCrossentropy","argMax","loss","arraySync","isFailing","categoricalAccuracy","stories","nEpochs","onEpochEnd","storiesEntries","Object","entries","epochMetrics","epoch","allTargets","allPredictions","allLosses","failingSamples","storyIdx","storyTitle","storyData","fitStory","confusionMatrix","truePredictions","accuracy","recall","precision","a","b","prediction","actionIdx","confidence","confidences","metrics","averageConfidence","json","JSON","parse","id","export","stringify","Slot","dependantActions","invDependantActions","isDefined","value","isDependant","isInvDependant","getValue","setValue","StateStep","name","categories","threshold","categoryNames","keys","oneHotValue","category","previousValue","bestValue","keywords","match","keyword","m","hm","currentUneqPrevious","bestEqPrevious","betterScore","source","walker","commonmark","smart","event","next","section","status","slot","sentence","extractedValues","storyName","currentState","stateStep","empty","parsedSource","endStory","pushSlot","entering","type","literal","destination","info","level","start","end","newStep","testSize","trainEntries","testEntries","splice","floor","random","train","fromEntries","test","model","slots","digest","turnConfidence","loaderFunction","tokenization","cased","maxDistance","trie","bestKey","lowestDistance","Infinity","knownKey","distance","wordTokenize","bytePairTokenize","pretext","join"],"mappings":"+WASaA,EAAbC,cAeaC,UAAe,EAQxBC,WAAWC,GACPF,KAAKE,QAAUA,EAanBD,kBAAkBE,EAAeC,IAejCC,uBAAuBD,GACnB,YAAaE,IAATF,EACOG,EAAS,CAAC,IAGAH,EAOzBI,aAAaC,IAQbC,gBACI,YAAYR,QAAQS,IAAI,KAAM,GAOlCC,eAMAC,KAAKC,IAMLb,eACI,MAAO,aClGCc,EAAoBC,EAAYC,GAC5C,MAAMC,EAAIC,MAAMC,KACZD,MAAMH,EAAGK,OAAS,GAClB,IAAM,IAAIF,MAAMF,EAAGI,OAAS,GAAGC,KAAK,IAGxC,IAAK,IAAIC,EAAI,EAAGA,GAAKP,EAAGK,OAAQE,GAAK,EACjCL,EAAEK,GAAG,GAAKA,EAGd,IAAK,IAAIC,EAAI,EAAGA,GAAKP,EAAGI,OAAQG,GAAK,EACjCN,EAAE,GAAGM,GAAKA,EAGd,IAAK,IAAIA,EAAI,EAAGA,EAAIP,EAAGI,OAAQG,GAAK,EAChC,IAAK,IAAID,EAAI,EAAGA,EAAIP,EAAGK,OAAQE,GAAK,EAGhCL,EAAEK,EAAI,GAAGC,EAAI,GAAKC,KAAKC,IACnBR,EAAEK,GAAGC,EAAI,GAAK,EACdN,EAAEK,EAAI,GAAGC,GAAK,EACdN,EAAEK,GAAGC,IALiBR,EAAGO,KAAON,EAAGO,GAAM,EAAI,IAUzD,OAAON,EAAEF,EAAGK,QAAQJ,EAAGI,QAAUI,KAAKE,IAAIX,EAAGK,OAAQJ,EAAGI,OAAQ,YC1BpDO,EAAWC,EAAcC,GACrC,IAAIC,EAAmB,CAAEC,aAAS1B,EAAW2B,MAAO,GAEpD,IAAK,IAAIV,EAAI,EAAGA,EAAIM,EAAKR,OAAQE,GAAK,EAAG,CACrC,MAAMS,EAAUH,EAAKC,UAAUP,EAAGA,EAAIO,EAAUT,QAE1Ca,EAAS,CACXF,QAAAA,EACAC,MAAO,EAAIlB,EAAoBiB,EAASF,IAG5C,GAAqB,IAAjBI,EAAOD,MACP,OAAOC,EAGPA,EAAOD,MAAQF,EAAUE,QACzBF,EAAYG,GAIpB,OAAOH,WClBKI,EAASC,GACrB,IAAIC,EAAO,EAEX,IAAK,IAAId,EAAI,EAAGA,EAAIa,EAAMf,OAAQE,GAAK,EAGnCc,GAASA,GAAQ,GAAKA,EAFVD,EAAME,WAAWf,GAG7Bc,GAAQ,EAGZ,OAAOA,WCXKE,EAAmBC,EAAiBC,GAAkB,EAClEC,EAA8B,MAC9B,OAAOnC,EAAQ,KACX,IAAIoC,EAEJ,OAAQD,GACR,IAAK,KACDC,EAAcpC,EAAgBqC,SAAS,IACvC,MAEJ,IAAK,QACDD,EAAcpC,EAAgBsC,QAC9B,MAEJ,IAAK,SACDF,EAAcpC,EAAgBuC,aAAa,IAC3C,MAEJ,QACI,UAAUC,8EACkEL,OAIhF,IAAIM,EAAeL,EAAYM,MAAMT,GAGrC,OAFIC,IAAQO,EAAeA,EAAaE,YAEjCF,EAAaG,mBCxBfC,EAqBTrD,YAAYsD,EAAmBC,EAAoBC,EAAoBC,EAAkB,IACrFxD,KAAKyD,WAAalB,EAAmB,CAACc,EAAYC,EAAyB,EAAbA,IAC9DtD,KAAK0D,SAAWnB,EAAmB,CAAc,EAAbe,IAAiB,EAAO,SAC5DtD,KAAK2D,eAAiBpB,EAAmB,CAAC,IAAI,EAAM,SACpDvC,KAAK4D,UAAYrB,EAAmB,CAAC,EAAGe,IACxCtD,KAAK6D,UAAYtB,EAAmB,CAAC,EAAGe,IAExCtD,KAAK8D,aAAevB,EAAmB,CAACe,EAAYC,IACpDvD,KAAK+D,UAAYxB,EAAmB,CAACgB,IAAa,EAAO,SAEzDvD,KAAKwD,QAAUA,EAQnBQ,SAASC,GAAiB,GACtB,MAAO,CACHC,EAAkBD,EAAQjE,KAAK6D,UAAUI,QAAUjE,KAAK6D,UACxDM,EAAkBF,EAAQjE,KAAK4D,UAAUK,QAAUjE,KAAK4D,WAWhEQ,QAAQC,EAAgBH,EAAgBC,EAAgBG,EACpDC,EAAsB,GACtB,OAAOhE,EAAQ,KAEX,MAAOiE,EAAIC,GAAMlE,EACDP,KAAK2D,eACH3D,KAAKyD,WACLzD,KAAK0D,SACLnD,EAAS,CAAC8D,IACxBF,EAAGD,GAIP,IAAIQ,EAAkBnE,EACTkE,EAAIzE,KAAKwD,SACjBmB,OAAO3E,KAAK8D,cACZc,IAAI5E,KAAK+D,WACTc,UACAC,IAAIP,GACJQ,UACAC,IAAIV,GAAQ,GAKjB,OAFAI,EAAIA,EAAEI,IAAIvE,EAAOmE,IAEV,CAAEA,EAAAA,EAAGF,GAAAA,EAAIC,GAAAA,KAOxB5D,KAAKoE,GACD1E,EAAQ,KAEJP,KAAKyD,WAAalD,EAAU0E,EAAQxB,YAAYN,WAChDnD,KAAK0D,SAAWnD,EAAU0E,EAAQvB,UAAUP,WAC5CnD,KAAK2D,eAAiBpD,EAAU0E,EAAQtB,gBAAgBR,WACxDnD,KAAK4D,UAAYrD,EAAU0E,EAAQrB,WAAWT,WAC9CnD,KAAK6D,UAAYtD,EAAU0E,EAAQpB,WAAWV,WAC9CnD,KAAK8D,aAAevD,EAAU0E,EAAQnB,cAAcX,WACpDnD,KAAK+D,UAAYxD,EAAU0E,EAAQlB,WAAWZ,aAOtDlD,eAWI,MAVgB,CACZwD,sBAAuBA,WAAWyB,QAClCxB,oBAAqBA,SAASwB,QAC9BvB,0BAA2BA,eAAeuB,QAC1CtB,qBAAsBA,UAAUsB,QAChCrB,qBAAsBA,UAAUqB,QAChCpB,wBAAyBA,aAAaoB,QACtCnB,qBAAsBA,UAAUmB,gBC7G/BC,EAMTpF,YAAYqF,EAAaC,GACrBrF,KAAKoF,IAAMA,EACXpF,KAAKqF,OAASA,EAEdrF,KAAKsF,OAAS,GACdtF,KAAKuF,QAAS,EAGlBC,SAASC,GACLzF,KAAKsF,OAAOG,EAAML,KAAOK,EAG7BC,YACI1F,KAAKuF,QAAS,SAOTI,EAGT5F,cACIC,KAAK4F,KAAO,IAAIT,EAAS,GAAI,MAMjCP,IAAIiB,GACA,IAAIC,EAAa9F,KAAK4F,KAEtB,IAAK,IAAIrE,EAAI,EAAGA,EAAIsE,EAAKxE,OAAQE,GAAK,OACCjB,IAA/BwF,EAAWR,OAAOO,EAAKtE,KACvBuE,EAAWN,SAAS,IAAIL,EAASU,EAAKtE,GAAIuE,IAG9CA,EAAaA,EAAWR,OAAOO,EAAKtE,IAEhCA,IAAMsE,EAAKxE,OAAS,GACpByE,EAAWJ,YAYvBK,MAAMlE,EAAcmE,EAAgCC,EAAyB,IACzE,MAAMC,EAAe,GAErB,SAASC,EAAYN,GACbK,EAAaA,EAAa7E,OAAS,KAAO2E,GACtCC,EAAaG,SAASP,IAC1BK,EAAaG,KAAKL,GAI1B,IAAIM,EAAOtG,KAAK4F,KACZC,EAAO,GAEX,IAAK,IAAItE,EAAI,EAAGA,EAAIM,EAAKR,OAAQE,GAAK,EAAG,CACrC,MAAMgF,EAAY1E,EAAKN,QAEQjB,IAA3BgG,EAAKhB,OAAOiB,IACZV,GAAQU,EACRD,EAAOA,EAAKhB,OAAOiB,KAEfD,EAAKf,OAAQW,EAAaG,KAAKR,GAC9BM,EAAYN,QAEmBvF,IAAhCN,KAAK4F,KAAKN,OAAOiB,IACjBV,EAAOU,EACPD,EAAOtG,KAAK4F,KAAKN,OAAOiB,KAExBJ,EAAYN,GACZA,EAAO,GACPS,EAAOtG,KAAK4F,OAQxB,OAHIU,EAAKf,OAAQW,EAAaG,KAAKR,GAC9BM,EAAYN,GAEVK,yKCpEuBpG,EAalCC,aAAYyG,QACRA,GAAU,EADFC,mBAERA,GAAqB,EAFbC,UAGRA,EAAY,OACU,CAAEF,SAAS,EAAMC,oBAAoB,EAAMC,UAAW,QAC5EC,QAjBK3G,QAAK,oBAkBVA,KAAKwG,QAAUA,EACfxG,KAAKyG,mBAAqBA,EAC1BzG,KAAK0G,UAAYA,EAEjB1G,KAAKY,cAGTX,WAAWC,eACKwC,KAAKxC,GAEjBF,KAAK4G,KAAO1G,EAAQmB,OACpBrB,KAAK6G,WAAatE,EAAmB,CAACvC,KAAK4G,KAAM5G,KAAK4G,OAG1D3G,kBAAkBE,GACd,OAAOI,EAAQ,KACXP,KAAK8G,WAAuB,KAAV3G,EAGGI,EACjB,CAACP,KAAKE,QAAQ6G,QAAQ/G,KAAKgH,iBAC3BhH,KAAKE,QAAQmB,UAKzBhB,uBAAuBD,GACnB,OAAqBA,EAAKuE,OAAO3E,KAAK6G,YAAYhC,UAGtDrE,aAAaC,GAETT,KAAKgH,eAAiBvG,IAAWT,KAAK0G,UAAYjG,EAAST,KAAKgH,eAGpEtG,gBACI,MAAM4D,EAAOqC,MAAMjG,gBAYnB,OATIV,KAAKwG,SAAWxG,KAAK8G,aACrBxC,EAAKtE,KAAKE,QAAQ6G,QAAQ/G,KAAK0G,aAAc,GAI7C1G,KAAKyG,oBAAsBzG,KAAKE,QAAQkG,SAASpG,KAAKgH,kBACtD1C,EAAKtE,KAAKE,QAAQ6G,QAAQ/G,KAAKgH,kBAAmB,GAG/C1C,EAGX1D,cACIZ,KAAK8G,YAAa,EAClB9G,KAAKgH,oBAAiB1G,EAG1BO,KAAKC,GACDd,KAAK6G,WAAatG,EAAQ,IAAMA,EAAUO,EAAW+F,YAAY1D,YAGrElD,eACI,MAAO,CACH4G,sBAAoCA,WAAW3B,6BC3GlCpF,EAOrBC,YAAY6G,GACRD,QAPK3G,QAAK,eAQVA,KAAK4G,KAAOA,EAGhB3G,kBAAkBE,GACd,OAAOI,EAAQ,KACX,MAAM0G,EAAU9G,EAAM+G,cACjBnB,MAAM,OACNpF,IAAKkF,GAAS1D,EAAS0D,GAAQ7F,KAAK4G,MAEzC,OAAqBrG,EAAU0G,EAASjH,KAAK4G,MAAMO,OAAO,WAAWC,IAAI,yBCrB5DtH,EAAzBC,kCACaC,QAAK,6BAKLA,UAAO,IAEhBC,WAAWC,eACKwC,KAAKxC,GACjBF,KAAKqH,cAAgBC,IAGrBtH,KAAKuH,yBAA2BC,YAAY,IAMxCvH,kBAAkBE,GACtB,MAAMsH,aAAmBJ,QAAQI,MAAM,CAACtH,IAClCuH,EAA8BD,EAAM5C,UAG1C,OAFAtE,EAAWkH,GAEJC,EAGXzH,kBAAkBE,GAEd,OAAKA,OAIOqH,YAAYrH,QAHRoH,cAActD,sCCzBHnE,EAS/BC,YAAY4H,GACRhB,QATK3G,QAAK,iBAUVA,KAAK4G,KAAO,EAAIe,EAAQf,KACxB5G,KAAK2H,QAAUA,EAGnB1H,WAAWC,eACKwC,KAAKxC,GAEZF,KAAK2H,QAAQC,uBAAuBD,QAAQ9G,OAGrDZ,kBAAkBE,GACd,OAAqBI,EAAQ,KACzB,MACMsG,EADS7G,KAAK2H,QAAQE,SAAS1H,GAEhCQ,IAAKmH,GAAU9H,KAAK2H,QAAQI,IAAID,IAChCE,OAAQC,QAAY3H,IAAN2H,GAGnB,GAA0B,IAAtBpB,EAAWxF,OACX,OAAOd,EAAS,CAACP,KAAK4G,OAG1B,MAAMsB,EAAmB3H,EAASsG,GAElC,OAAOtG,EAAU,CAAC2H,EAAiBC,KAAK,GAAID,EAAiBvG,IAAI,sCCgEzE5B,aAAYG,QACRA,EADQkI,YAERA,EAFQ9E,WAGRA,EAAa,GAHL+E,UAIRA,EAAY9H,EAAS+H,KAAK,KAJlB/D,YAKRA,EAAc,EALNf,QAMRA,EAAU,IAEVxD,KAAKE,QAAUA,EACfF,KAAKoI,YAAcA,EACnBpI,KAAKqI,UAAYA,EAEjBrI,KAAKsD,WAAaA,EAClBtD,KAAKuD,WAAarD,EAAQmB,OAE1BrB,KAAKuI,gBAAkBhE,EACvBvE,KAAKwI,YAAchF,EAMvBvD,mBAEUwI,QAAQC,IACV1I,KAAKoI,YAAYzH,IAAKgI,GAAeA,EAAWjG,KAAK1C,KAAKE,WAI9DF,KAAKqD,UAAYrD,KAAKoI,YACjBzH,IAAKgI,GAAeA,EAAW/B,MAC/BgC,OAAO,CAACC,EAAKjC,IAASiC,EAAMjC,EAAM,GAEvC5G,KAAK8I,KAAO,IAAI1F,EAAKpD,KAAKqD,UAAWrD,KAAKsD,WAAYtD,KAAKuD,WAAYvD,KAAKwI,aAE5ExI,KAAKY,cAMTA,cACIZ,KAAKoI,YAAYW,QAASJ,GAAeA,EAAW/H,iBACjDsD,EAAGlE,KAAKgJ,MAAO7E,EAAGnE,KAAKiJ,OAAUjJ,KAAK8I,KAAK9E,YAM1C/D,kBAAkBE,EAAeC,GACrC,OAAOqI,QAAQC,IACX1I,KAAKoI,YAAYzH,IAAKgI,GAAeA,EAAWO,YAAY/I,EAAOC,KAOnEC,uBAAuB8I,GAC3B,OAAO5I,EAAQ,KACX,MAAMsG,EAAa7G,KAAKoI,YAAYzH,IAChC,CAACgI,EAAYS,IAAQT,EAAWtI,uBAAuB8I,EAASC,KAMpE,OAFAvC,EAAWR,KAAK9F,EAAS,CAAC,KAELA,EAAUsG,KAO/BrG,aAAaC,GACjBT,KAAKoI,YAAYzH,IAAKgI,GAAeA,EAAWnI,aAAaC,IAMzDC,gBACJ,OAAOH,EAAQ,IAAMP,KAAKoI,YAErBzH,IAAKgI,GAA6BpI,EAC/BoI,EAAWjI,qBACXJ,EAAW,YAGdsI,OAAO,CAACC,EAAKvE,IAAS/D,EAAOsI,EAAKvE,GAAO/D,EAAQ,CAACP,KAAKE,QAAQmB,WAMhEpB,eAAeoJ,GACnBrJ,KAAKY,cAGL,MAAM0I,EAAkB,GAClBC,EAAuB,GACvBC,EAAyB,GAG/B,IAAK,IAAIC,EAAW,EAAGA,EAAWJ,EAAMhI,OAAQoI,GAAY,EAAG,CAC3D,MAAMC,EAAQL,EAAMI,GAIpBH,EAAOjD,gBAAgB6C,YAAYQ,EAAMvJ,MAAOuJ,EAAMtJ,OAEtDmJ,EAAMlD,KAAKrG,KAAKU,iBAEhB8I,EAAQnD,KACU9F,EACVP,KAAKE,QAAQ6G,QAAQ2C,EAAMjJ,QAC3BT,KAAKuD,aAIbvD,KAAKQ,aAAakJ,EAAMjJ,QAI5B,IAAIL,EA8CJ,OA5CAJ,KAAKqI,UAAUsB,SAAS,KACpB,IAAIzF,EAAEA,EAAFC,EAAKA,GAAMnE,KAAK8I,KAAK9E,UAAS,GAGlC,MAAM4F,EAAcN,EAAO3I,IAAI,CAACwI,EAAUC,KACtC,MAAMS,EAAY7J,KAAK8I,KAAK1E,QACVpE,KAAKK,uBAAuB8I,GAC5BjF,EACAC,EACAoF,EAAMH,IAMxB,OAHAlF,EAAI2F,EAAUrF,GACdL,EAAI0F,EAAUpF,GAEPoF,EAAUnF,IAGfoF,EAAgBvJ,EAASiJ,GACzBO,EAAoBxJ,EAASqJ,GAG7BI,EAAyBzJ,EAAW0J,wBACtCH,EAAeC,GACjB5B,OAcF,OAXA/H,EAAO,CACHoJ,QAASjJ,EAAQuJ,EAAcI,OAAO,IACtCN,YAAarJ,EAAQwJ,EAAkBG,OAAO,IAC9CC,KAAeH,EAAWI,YAC1BC,UAAW9J,EACN+J,oBAAoBR,EAAeC,GACnC5B,OACAiC,YAAc,MAIhBJ,IAIXzJ,EAAW,CAAC+I,EAAQE,IAEbpJ,EAQXH,aAAYsK,QAAEA,EAAFC,QAAWA,EAAU,GAArBC,WAAyBA,IACjC,MAAMC,EAAiBC,OAAOC,QAAQL,GACtC,IAAIM,EAGJ,IAAK,IAAIC,EAAQ,EAAGA,EAAQN,EAASM,GAAS,EAAG,CAC7C,MAAMC,EAA4B,GAC5BC,EAAgC,GAChCC,EAAsB,GACtBC,EAA2B,GAGjC,IAAK,IAAIC,EAAW,EAAGA,EAAWT,EAAerJ,OAAQ8J,GAAY,EAAG,CACpE,MAAOC,EAAY/B,GAASqB,EAAeS,GAIrCE,aAAuBC,SAASjC,GAEtC0B,EAAW1E,KAAKgF,EAAU7B,SAC1BwB,EAAe3E,KAAKgF,EAAUzB,aAC9BqB,EAAU5E,KAAKgF,EAAUlB,MAErBkB,EAAUhB,WACVa,EAAe7E,KAAK+E,GAK5B,MAAMG,EAAkBhL,EAAQ,IAAMA,EAAQgL,gBAC1ChL,EAAUwK,GACVxK,EAAUyK,GACVhL,KAAKuD,aAGHiI,EAAkBjL,EAAQ,IAAMgL,EACjCvG,IAAIzE,KAAUgL,EAAgB/I,QAC9B4E,IAAI,IAETyD,EAAe,CACXC,MAAAA,EACAI,eAAAA,EAEAO,SAAmBlL,EAAQ,IACvBiL,EAAgBpE,MAAMtC,IAAIyG,EAAgBnE,OAAOgD,aAGrDsB,OAAiBnL,EAAQ,IACrBiL,EAAgB1G,IAAIyG,EAAgBnE,IAAI,IAAIe,OAAOiC,aAGvDuB,UAAoBpL,EAAQ,IACxBiL,EAAgB1G,IAAIyG,EAAgBnE,IAAI,IAAIe,OAAOiC,aAGvDD,KAAMc,EAAUrC,OAAO,CAACgD,EAAGC,IAAMD,EAAIC,GAAKZ,EAAU5J,QAIxDd,EAAWwK,GACXxK,EAAWyK,GACXzK,EAAW,CAACiL,EAAiBD,SAEVjL,IAAfmK,GACAA,EAAWI,GAKnB,OADA7K,KAAKY,cACEiK,EASX5K,cAAcE,GAGVH,KAAK8I,KAAKtF,QAAU,EAEpB,MAAM2F,EAAWnJ,KAAKK,kCAAkC6I,YAAY/I,IAC9DoJ,EAAQvJ,KAAKU,gBAEboL,EAAa9L,KAAK8I,KAAK1E,QACzB+E,EAAUnJ,KAAKgJ,MAAOhJ,KAAKiJ,MAAOM,EAAOvJ,KAAKuI,iBAIlDhI,EAAW,CAACP,KAAKgJ,MAAOhJ,KAAKiJ,QAC7BjJ,KAAKgJ,MAAQ8C,EAAWtH,GAAGP,QAC3BjE,KAAKiJ,MAAQ6C,EAAWrH,GAAGR,QAE3B,MAAM8H,EAAqBxL,EAAQ,IAAMuL,EAAWpH,EAAEwF,SAASE,aACzD4B,EAAsBzL,EAAQ,IAAMuL,EAAWpH,EAAE0F,YAAY2B,IAWnE,OARAxL,EAAW,CAAC4I,EAAUI,IACtBhJ,EAAWuL,GAGX9L,KAAK8I,KAAKtF,QAAUxD,KAAKwI,YAEzBxI,KAAKQ,aAAaR,KAAKE,QAAQ6L,IAExB,CAAEtL,OAAQT,KAAKE,QAAQ6L,GAAYC,WAAAA,GAS9C/L,YAAYsK,GACR,MAAMG,EAAiBC,OAAOC,QAAQL,GAEhCf,EAAoB,GACpBI,EAAwB,GACxBqC,EAAwB,GACxBf,EAA2B,GAGjC,IAAK,IAAIC,EAAW,EAAGA,EAAWT,EAAerJ,OAAQ8J,GAAY,EAAG,CACpE,MAAOC,EAAY/B,GAASqB,EAAeS,GAC3CnL,KAAKY,cAEL,IAAK,IAAI6I,EAAW,EAAGA,EAAWJ,EAAMhI,OAAQoI,GAAY,EAAG,CAC3D,MAAMC,EAAQL,EAAMI,IAGdhJ,OAAEA,EAAFuL,WAAUA,cAA0B5H,QAAQsF,EAAMvJ,OAExDqJ,EAAQnD,KAAKrG,KAAKE,QAAQ6G,QAAQ2C,EAAMjJ,SACxCmJ,EAAYvD,KAAKrG,KAAKE,QAAQ6G,QAAQtG,IACtCwL,EAAY5F,KAAK2F,GAEbvL,IAAWiJ,EAAMjJ,QAAWyK,EAAe9E,SAASgF,IACpDF,EAAe7E,KAAK+E,IAMhC,MAAMG,EAAkBhL,EAAQ,IAAMA,EAAQgL,gBAC1ChL,EAAUiJ,GAAUjJ,EAAUqJ,GAAc5J,KAAKuD,aAG/CiI,EAAkBjL,EAAQ,IAAMgL,EACjCvG,IAAIzE,KAAUgL,EAAgB/I,QAC9B4E,IAAI,IAEH8E,EAAmB,CACrBhB,eAAAA,EAEAO,SAAmBlL,EAAQ,IACvBiL,EAAgBpE,MAAMtC,IAAIyG,EAAgBnE,OAAOgD,aAGrDsB,OAAiBnL,EAAQ,IACrBiL,EAAgB1G,IAAIyG,EAAgBnE,IAAI,IAAIe,OAAOiC,aAGvDuB,UAAoBpL,EAAQ,IACxBiL,EAAgB1G,IAAIyG,EAAgBnE,IAAI,IAAIe,OAAOiC,aAGvD+B,kBAAmBF,EAAYrD,OAAO,CAACgD,EAAGC,IAAMD,EAAIC,GAAKI,EAAY5K,QAMzE,OAHAd,EAAW,CAACiL,EAAiBD,IAE7BvL,KAAKY,cACEsL,EAMXrL,KAAKuL,GACD,MAAMtL,EAAauL,KAAKC,MAAMF,GAE9BpM,KAAKoI,YAAYW,QAASJ,IACtBA,EAAW9H,KAAKC,EAAW6H,EAAW4D,OAG1CvM,KAAK8I,KAAKjI,KAAKC,EAAWgI,MAE1B9I,KAAKY,cAMTX,eACI,MAAMa,EAAa,CAAEgI,gBAAiBA,KAAK0D,UAE3C,IAAK,IAAIpD,EAAM,EAAGA,EAAMpJ,KAAKoI,YAAY/G,OAAQ+H,GAAO,EAAG,CACvD,MAAMT,EAAa3I,KAAKoI,YAAYgB,GAGpCtI,EAAW6H,EAAW4D,UAAY5D,EAAW6D,SAGjD,OAAOH,KAAKI,UAAU3L,YCzejB4L,UAAoB5M,EAsB7BC,YAAY4M,EAA4BC,GACpCjG,QACA3G,KAAK2M,iBAAmBA,EACxB3M,KAAK4M,oBAAsBA,EAG/BlM,gBACI,YAAYR,QAAQS,IAAKF,IACrB,MAAMoM,OAA2BvM,IAAfN,KAAK8M,MACjBC,EAAc/M,KAAK2M,iBAAiBvG,SAAS3F,GAC7CuM,EAAiBhN,KAAK4M,oBAAoBxG,SAAS3F,GAEzD,OAASoM,KAAeE,GAAeC,IAC/BH,IAAcG,IAI9BpM,cACIZ,KAAK8M,WAAQxM,EAMjB2M,WACI,YAAYH,MAMhBI,SAASJ,GACL9M,KAAK8M,MAAQA,OCpDhBK,yDCiCgCT,EASjC3M,aAAYqN,KACRA,EADQC,WAERA,EAFQV,iBAGRA,EAAmB,GAHXC,oBAIRA,EAAsB,GAJdU,UAKRA,EAAY,MAEZ3G,MAAMgG,EAAkBC,GAExB5M,KAAKuN,cAAgB5C,OAAO6C,KAAKH,GACjCrN,KAAKqN,WAAaA,EAClBrN,KAAKsN,UAAYA,EAEjBtN,KAAKuM,GAAQa,iBACbpN,KAAK4G,KAAO,EAAI5G,KAAKuN,cAAclM,OAGvCpB,WAAWC,eACKwC,KAAKxC,GACjBF,KAAKY,cAGD6M,YAAYX,GAChB,MAAMS,EAAgB5C,OAAO6C,KAAKxN,KAAKqN,YAEvC,OAAqB9M,EACjBgN,EAAcxG,QAAQ+F,EAAMY,UAC5BH,EAAclM,QAItBpB,kBAAkBE,GACd,MAAMwN,EAAgB3N,KAAKiN,WAC3B,IAAIW,EAA8B,CAAEF,cAAUpN,EAAW0B,aAAS1B,EAAW2B,MAAO,GAGpF0I,OAAOC,QAAQ5K,KAAKqN,YAAYtE,QAAQ,EAAE2E,EAAUG,MAEhD,MAAMC,EAAQD,EACTlN,IAAKoN,GAAYnM,EAAWzB,EAAM+G,cAAe6G,EAAQ7G,gBAEzDc,OAAQgG,GAAMA,EAAE/L,OAASjC,KAAKsN,WAE9B1E,OACG,CAACqF,EAAID,IAAOC,EAAGhM,MAAQ+L,EAAE/L,MAAQgM,EAAKD,EACtC,CAAEhM,aAAS1B,EAAW2B,MAAO,IAIrC,QAAsB3B,IAAlBwN,EAAM9L,QAAuB,CAC7B,MAAMkM,EAAsBR,IAAaC,EAAcD,SACjDS,EAAiBP,EAAUF,WAAaC,EAAcD,SACtDU,EAAcN,EAAM7L,MAAQ2L,EAAU3L,YAEjB3B,IAAvBsN,EAAUF,UACNQ,GAAuBE,GACvBD,GAAkBD,GAClBC,GAAkBC,KAEtBR,EAAY,CAAEF,SAAAA,KAAaI,OAKvC,MAAM3E,EAAW5I,EAAQ,IACrBA,EAAU,CACNP,KAAKyN,YAAYG,GACjB5N,KAAKyN,YAAYzN,KAAKiN,eAQ9B,YAJ2B3M,IAAvBsN,EAAUF,UACV1N,KAAKkN,SAASU,GAGXzE,EAGX8D,WACI,YAAyB3M,IAArBqG,MAAMsG,WACC,CAAES,cAAUpN,EAAW0B,aAAS1B,EAAW2B,MAAO,SAGhDgL,eD7HrB,SAAKE,GACDA,qBAAOA,mBAAMA,uBAAQA,qBADzB,CAAKA,IAAAA,mDAQwBkB,GACzB,MAGMC,EAHiB,IAAIC,EAAkB,CAAEC,OAAO,IAChBlC,MAAM+B,GAEdC,SAC9B,IAAIG,EAAQH,EAAOI,OAEnB,MAAMhF,EAAQ,CAEViF,aAASrO,EACTsO,YAAQtO,EAERuO,UAAMvO,EACNwO,SAAU,GACVC,gBAAiB,GAEjBC,eAAW1O,EACX+I,MAAO,GACP4F,kBAAc3O,EACd4O,UAAW/B,EAAUgC,OAGnBC,EAA8B,CAAE7E,QAAS,GAAIwE,gBAAiB,IAEpE,SAASM,SACmB/O,IAApBoJ,EAAMsF,YAENtF,EAAML,MAAMhD,KAAKqD,EAAMuF,cAEvBvF,EAAML,MAAMhD,KAAK,CAAElG,MAAO,GAAIM,OAAQ,QAGtC2O,EAAa7E,QAAQb,EAAMsF,WAAatF,EAAML,MAG9CK,EAAMsF,eAAY1O,EAClBoJ,EAAML,MAAQ,GACdK,EAAMuF,kBAAe3O,EACrBoJ,EAAMwF,UAAY/B,EAAUgC,OAIpC,SAASG,EAASR,QACmCxO,IAA7C8O,EAAaL,gBAAgBrF,EAAMmF,QACnCO,EAAaL,gBAAgBrF,EAAMmF,MAAQ,IAI/CO,EAAaL,gBAAgBrF,EAAMmF,MAAMxI,QAElCqD,EAAMqF,gBAAgBpO,IAAKoO,KAAuBD,SAAAA,KAAaC,MAGtErF,EAAMqF,gBAAkB,GACxBrF,EAAMoF,SAAW,GAGrB,KAAiB,OAAVL,GAAgB,CACnB,MAAMc,SAAEA,EAAFjJ,KAAYA,GAASmI,GACrBe,KACFA,EADEC,QAEFA,EAFEC,YAGFA,EAHEC,KAIFA,EAJEC,MAKFA,GACAtJ,EAMJ,GAAIiJ,GAAqB,YAATC,GAAgC,IAAVI,EAClClG,EAAMkF,OAAS,cACfS,YAKgB,SAATG,GAAoC,gBAAjB9F,EAAMkF,OAChClF,EAAMiF,QAAUc,EAChB/F,EAAMkF,OAAYa,sBAMO,gBAAlB/F,EAAMiF,QAQTY,GAAqB,YAATC,GAAgC,IAAVI,EAClClG,EAAMkF,OAAS,iBAKC,SAATY,GAAoC,mBAAjB9F,EAAMkF,QAC1Ba,KAAWL,EAAaL,kBAC1BK,EAAaL,gBAAgBU,GAAW,IAG5C/F,EAAMmF,KAAOY,EACb/F,EAAMkF,OAAS,iBAORW,GAAqB,SAATC,GAAoC,kBAAjB9F,EAAMkF,OAC5ClF,EAAMkF,OAAS,kBAKC,SAATY,GAAoC,oBAAjB9F,EAAMkF,OAChClF,EAAMoF,UAAYW,EAOF,SAATD,GAAoC,oBAAjB9F,EAAMkF,QAChClF,EAAMqF,gBAAgB1I,KAAK,CACvBrE,QAASyN,EACTI,MAAOnG,EAAMoF,SAASzN,OACtByO,IAAKpG,EAAMoF,SAASzN,OAASoO,EAAQpO,OAAS,IAGlDqI,EAAMoF,UAAYW,GAKVF,GAAqB,SAATC,GAAoC,oBAAjB9F,EAAMkF,SAC7CU,EAAS5F,EAAMoF,UACfpF,EAAMkF,OAAS,yBAQM,kBAAlBlF,EAAMiF,QAQb,GAAIY,GAAqB,YAATC,GAAgC,IAAVI,EAClClG,EAAMkF,OAAS,oBAEfS,YAKgB,SAATG,GAAoC,sBAAjB9F,EAAMkF,OAChClF,EAAMsF,UAAYS,EAClB/F,EAAMkF,OAAS,2BAMfW,GACG,CAAC,cAAe,OAAQ,cAAcnJ,SAASoJ,IAC9B,qBAAjB9F,EAAMkF,OAA+B,CAExC,IAAImB,EAEJ,OAAQP,GACR,IAAK,cACDO,EAAU5C,EAAUhN,MACpB,MAEJ,IAAK,OACD4P,EAAU5C,EAAU1M,OACpB,MAEJ,IAAK,aACDsP,EAAU5C,EAAU/M,KAuBxB,OAbI2P,GAAWrG,EAAMwF,iBAEU5O,IAAvBoJ,EAAMuF,eACNvF,EAAML,MAAMhD,KAAKqD,EAAMuF,cAEnBc,IAAY5C,EAAUhN,OAAuC,QAA9BuJ,EAAMuF,aAAaxO,QAClDiJ,EAAML,MAAMhD,KAAa,CAAElG,MAAO,GAAIM,OAAQ,SAItDiJ,EAAMuF,aAAe,CAAE9O,MAAO,GAAIM,OAAQ,QAGtCsP,GACR,KAAK5C,EAAUhN,MACXuJ,EAAMkF,OAAS,oBACf,MAEJ,KAAKzB,EAAU1M,OACXiJ,EAAMkF,OAAS,qBACf,MAEJ,KAAKzB,EAAU/M,KACE,SAATuP,IACAjG,EAAMuF,aAAa7O,KAAOiM,KAAKC,MAAMmD,IAO7C/F,EAAMwF,UAAYa,MAKF,SAATP,GAAoC,sBAAjB9F,EAAMkF,OAChClF,EAAMuF,aAAa9O,OAASsP,EAKrBF,GAAqB,SAATC,GAAoC,sBAAjB9F,EAAMkF,QAC5ClF,EAAMmF,KAAOa,EACbhG,EAAMkF,OAAS,oBAKC,SAATY,GAAoC,qBAAjB9F,EAAMkF,QAChClF,EAAMqF,gBAAgB1I,KAAK,CACvBrE,QAASyN,EACTI,MAAOnG,EAAMuF,aAAa9O,MAAMkB,OAChCyO,IAAKpG,EAAMuF,aAAa9O,MAAMkB,OAASoO,EAAQpO,OAAS,IAG5DqI,EAAMuF,aAAa9O,OAASsP,EAC5B/F,EAAMkF,OAAS,qBAKPW,GAAqB,gBAATC,GAA2C,sBAAjB9F,EAAMkF,OAQpC,SAATY,GAAoC,uBAAjB9F,EAAMkF,SAChClF,EAAMuF,aAAaxO,OAASgP,EAC5B/F,EAAMkF,OAAS,qBATXlF,EAAMqF,gBAAgB1N,OAAS,GAAGiO,EAAS5F,EAAMuF,aAAa9O,OAElEuJ,EAAMkF,OAAS,oBAWvBH,EAAQH,EAAOI,OAKnB,OAFAW,IAEOD,2BEhSoB7E,EAAkByF,GAE7C,MAAMC,EAAetF,OAAOC,QAAQL,GAC9B2F,EAAc,GAEpB,IAAK,IAAI3O,EAAI,EAAGA,EAAI0O,EAAa5O,OAAS2O,EAAUzO,GAAK,EACrD2O,EAAY7J,QAAQ4J,EAAaE,OAC7B1O,KAAK2O,MAAM3O,KAAK4O,SAAWJ,EAAa5O,QAAS,IAIzD,MAAO,CACHiP,MAAO3F,OAAO4F,YAAYN,GAC1BO,KAAM7F,OAAO4F,YAAYL,wBC0B7BnQ,aAAY0Q,MACRA,EADQC,MAERA,EAAQ,GAFAhK,UAGRA,EAAY,QAEZ1G,KAAKyQ,MAAQA,EACbzQ,KAAK0Q,MAAQA,EACb1Q,KAAK0G,UAAYA,EAMrBzG,UAAUE,GACN,MAAMwQ,EAAoB,CACtBxQ,MAAAA,EACAD,QAAS,GACT0Q,eAAgB,EAChBF,MAAO,IAGX,IAAIjQ,OAAEA,EAAFuL,WAAUA,cAA0ByE,MAAMrM,QAAQjE,GAEtD,KAAOM,IAAWT,KAAK0G,WACnBiK,EAAOzQ,QAAQmG,KAAK5F,GACpBkQ,EAAOC,gBAAkB5E,IAGtBvL,OAAAA,EAAQuL,WAAAA,cAA0ByE,MAAMrM,QAAQ,KAOvD,OAJApE,KAAK0Q,MAAM3H,QAAS8F,IAChB8B,EAAOD,MAAM7B,EAAKtC,IAAMsC,EAAK5B,aAG1B0D,EAMX/P,cACIZ,KAAKyQ,MAAM7P,mCCvDfb,aAAY8Q,eACRA,EADQjK,KAERA,EAFQkK,aAGRA,EAAe,OAHPC,MAIRA,GAAQ,EAJAC,YAKRA,EAAc,GALNhL,WAMRA,IAEA,IAAK,CAAC,OAAQ,aAAaI,SAAS0K,GAChC,UAAU/N,MAAM,kEAGpB/C,KAAK6Q,eAAiBA,EAEtB7Q,KAAK4G,KAAOA,EACZ5G,KAAK8Q,aAAeA,EACpB9Q,KAAK+Q,MAAQA,EAEb/Q,KAAKgR,YAAcA,EACnBhR,KAAKgG,WAAaA,EAMtBwH,OACI,OAAO7C,OAAO6C,KAAKxN,KAAK2H,SAM5B1H,aACID,KAAK2H,QAAU0E,KAAKC,iBAAiBuE,kBAEX,cAAtB7Q,KAAK8Q,eACL9Q,KAAKiR,KAAO,IAAItL,EAChB3F,KAAKwN,OAAOzE,QAAS3D,GAAQpF,KAAKiR,KAAKrM,IAAIQ,KAOnDwC,WACI,YAAwBtH,SAAZqH,QAUhBI,IAAI3C,GAEA,GAAIpF,KAAKwN,OAAOpH,SAAShB,GACrB,OAAO7E,EAAYP,KAAK2H,QAAQvC,IAIpC,IAAI8L,EACAC,EAAyBC,SAW7B,OATApR,KAAKwN,OAAOzE,QAASsI,IACjB,MAAMC,EAAWvQ,EAAoBsQ,EAAUjM,GAE3CkM,EAAWH,IACXD,EAAUG,EACVF,EAAiBG,KAIrBH,GAAkBnR,KAAKgR,YAChBzQ,EAAUP,KAAK2H,QAAQuJ,SAIV5Q,IAApBN,KAAKgG,WACEzF,EAAUP,KAAK2H,QAAQ3H,KAAKgG,kBADvC,EAaIuL,aAAa1P,GACjB,OAAOA,EAAKkE,MAAM,OAAOiC,OAAQF,GAAUA,EAAMzG,OAAS,GAQtDmQ,iBAAiB3P,GACrB,MAAM4P,OAAc5P,GAAOkE,MAAM,KAAK2L,KAAK,KAE3C,YAAYT,KAAKlL,MAAM0L,EAASzR,KAAKgG,WAAY,CAAC,MAQtD6B,SAAShG,GAML,OALI7B,KAAK+Q,QAELlP,EAAOA,EAAKqF,eAGRlH,KAAK8Q,cACb,IAAK,OACD,YAAYS,aAAa1P,GAE7B,IAAK,YACD,YAAY2P,iBAAiB3P,GAEjC,QACI,UAAUkB,MAAM"}