{"version":3,"file":"index.modern.js","sources":["../src/featurizers/featurizer.ts","../src/utils/levenshtein_distance.ts","../src/utils/fuzzy_match.ts","../src/utils/hashcode.ts","../src/utils/initialize_variable.ts","../src/utils/lstm.ts","../src/utils/trie.ts","../src/featurizers/action_featurizer.ts","../src/featurizers/bow.ts","../src/featurizers/use.ts","../src/featurizers/word_embedding.ts","../src/models/hcn.ts","../src/slots/slot.ts","../src/slots/categorical_slot.ts","../src/tools/parse_stories.ts","../src/tools/train_test_split.ts","../src/tools/nlu_formatter.ts","../src/tools/keyed_vectors.ts"],"sourcesContent":["import * as tf from '@tensorflow/tfjs';\n\ntype JSONSerializable = {[key: string]: any};\n\n/**\n * A stateful featurizer that turns queries into numerical representations.\n *\n * @abstract\n */\nexport class Featurizer {\n    /**\n     * An ID used by models for exportations.\n     */\n    readonly id: string;\n\n    /**\n     * The list of every action the model can take.\n     */\n    protected actions: any[];\n\n    /**\n     * The size of the vector returned by the featurizer.\n     * By default it's set to 1 which is the default for a featurizer that returns no features.\n     */\n    readonly size: number = 1;\n\n    /**\n     * Initialize the model, can be asynchronous async code.\n     *\n     * This method is executed by the model during it's initialization,\n     * it will also set the actions attribute.\n     */\n    async init(actions: any[]) {\n        this.actions = actions;\n    }\n\n    /**\n     * Featurizes and handle a text query.\n     *\n     * @remarks\n     * This method can directly return a 1D tensor to provide features to the model.\n     * Alternatively, it can returns data of any type if the Featurizer implement a custom\n     * getOptimizableFeatures method to handle those data.\n     * If this method doesn't return something, no features will be passed to the model.\n     */\n    // eslint-disable-next-line no-unused-vars, class-methods-use-this, no-empty-function\n    async handleQuery(query: string): Promise<any> {}\n\n    /**\n     * Turn the data returned by handleQuery into an embedding vector.\n     * This function is used to expose featurizer variables to the model optimizer for training.\n     *\n     * Reimplementing this method is not necessary if your featurizer is not meant to be optimizable\n     * through gradient descent.\n     * In this case, just return the feature vector directly using the handleQuery method.\n     *\n     * @remarks\n     * It's important to keep this function stateless, it should only depend of its tensor argument\n     * and of featurizer's variables.\n     */\n    // eslint-disable-next-line class-methods-use-this\n    getOptimizableFeatures(data: any): tf.Tensor1D {\n        if (data === undefined) {\n            return tf.zeros([1]);\n        }\n\n        return <tf.Tensor1D> data;\n    }\n\n    /**\n     * Let the featurizer know what action the model has taken.\n     */\n    // eslint-disable-next-line no-unused-vars, class-methods-use-this\n    handleAction(action: any): void {}\n\n    /**\n     * Produce an action mask according to featurizer state.\n     * (Generally, this method is reimplemented in stateful featurizers)\n     *\n     * @returns An array of boolean mapping every actions availability.\n     */\n    getActionMask(): boolean[] {\n        return this.actions.map(() => true);\n    }\n\n    /**\n     * Resets the state of the featurizer (if the stateful feature is used).\n     */\n    // eslint-disable-next-line class-methods-use-this\n    resetDialog(): void {}\n\n    /**\n     * Load parameters extracted from a JSON-like document.\n     */\n    // eslint-disable-next-line no-unused-vars, class-methods-use-this\n    load(parameters: JSONSerializable) {}\n\n    /**\n     * Export the featurizer's internal parameters to be serialized along the model.\n     */\n    // eslint-disable-next-line class-methods-use-this\n    async export(): Promise<JSONSerializable> {\n        return {};\n    }\n}\n","/**\n * Compute the Levenshtein distance between two strings using the\n * [Wagner-Fisher algorithm](https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm).\n */\nexport function levenshteinDistance(s1: string, s2: string): number {\n    const d = Array.from(\n        Array(s1.length + 1),\n        () => new Array(s2.length + 1).fill(0)\n    );\n\n    for (let i = 1; i <= s1.length; i += 1) {\n        d[i][0] = i;\n    }\n\n    for (let j = 1; j <= s2.length; j += 1) {\n        d[0][j] = j;\n    }\n\n    for (let j = 0; j < s2.length; j += 1) {\n        for (let i = 0; i < s1.length; i += 1) {\n            const substitutionCost = (s1[i] !== s2[j]) ? 1 : 0;\n\n            d[i + 1][j + 1] = Math.min(\n                d[i][j + 1] + 1,\n                d[i + 1][j] + 1,\n                d[i][j] + substitutionCost\n            );\n        }\n    }\n\n    return d[s1.length][s2.length] / Math.max(s1.length, s2.length, 1);\n}\n","import { levenshteinDistance } from './levenshtein_distance';\n\ntype Match = { extract: string, score: number };\n\nexport function fuzzyMatch(text: string, substring: string): Match {\n    let bestMatch: Match = { extract: undefined, score: 0 };\n\n    for (let i = 0; i < text.length; i += 1) {\n        const extract = text.substring(i, i + substring.length);\n\n        const aMatch = {\n            extract,\n            score: 1 - levenshteinDistance(extract, substring)\n        };\n\n        if (aMatch.score === 1) {\n            return aMatch;\n        }\n\n        if (aMatch.score > bestMatch.score) {\n            bestMatch = aMatch;\n        }\n    }\n\n    return bestMatch;\n}\n","/* eslint-disable no-bitwise */\n\n/**\n * Hash a string.\n * Based on https://stackoverflow.com/a/7616484\n */\nexport function hashcode(input: string) {\n    let hash = 0;\n\n    for (let i = 0; i < input.length; i += 1) {\n        const chr = input.charCodeAt(i);\n\n        hash = ((hash << 5) - hash) + chr;\n        hash |= 0; // Convert to 32bit integer\n    }\n\n    return hash;\n}\n","import * as tf from '@tensorflow/tfjs';\n\n/**\n * Initialize a variable of a given shape.\n */\nexport function initializeVariable(shape: number[], scalar: boolean = false,\n    init: 'he'|'zeros'|'normal' = 'he'): tf.Variable {\n    return tf.tidy(() => {\n        let initializer;\n\n        switch (init) {\n        case 'he':\n            initializer = tf.initializers.heNormal({});\n            break;\n\n        case 'zeros':\n            initializer = tf.initializers.zeros();\n            break;\n\n        case 'normal':\n            initializer = tf.initializers.randomNormal({});\n            break;\n\n        default:\n            throw new Error(\n                `Expected parameter init to take value 'he', 'zeros' or 'normal' not '${init}'.`\n            );\n        }\n\n        let randomTensor = initializer.apply(shape);\n        if (scalar) randomTensor = randomTensor.asScalar();\n\n        return randomTensor.variable();\n    });\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { initializeVariable } from './initialize_variable';\n\ntype LSTMPrediction = {y: tf.Tensor1D, nc: tf.Tensor2D, nh: tf.Tensor2D};\n\n/**\n * An LSTM cell with a dense layer on its top.\n */\nexport class LSTM {\n    // LSTM parameters :\n    private lstmKernel: tf.Tensor;\n    private lstmBias: tf.Tensor;\n    private lstmForgetBias: tf.Tensor;\n    private lstmInitH: tf.Tensor;\n    private lstmInitC: tf.Tensor;\n\n    // Dense layer parameters :\n    private denseWeights: tf.Tensor;\n    private denseBias: tf.Tensor;\n\n    // Let dropout be public to allow to change its value when training/inference.\n    public dropout: number;\n\n    /**\n     * @param inputSize The dimension of the input data.\n     * @param hiddenSize The dimension of the output of the LSTM, passed to the dense layer.\n     * @param outputSize The dimension of the output data.\n     * @param dropout The dropout rate between the LSTM cell and the dense layer.\n     */\n    constructor(inputSize: number, hiddenSize: number, outputSize: number, dropout: number = 0.2) {\n        this.lstmKernel = initializeVariable([inputSize + hiddenSize, hiddenSize * 4]);\n        this.lstmBias = initializeVariable([hiddenSize * 4], false, 'zeros');\n        this.lstmForgetBias = initializeVariable([1], true, 'zeros'); // (scalar)\n        this.lstmInitH = initializeVariable([1, hiddenSize]);\n        this.lstmInitC = initializeVariable([1, hiddenSize]);\n\n        this.denseWeights = initializeVariable([hiddenSize, outputSize]);\n        this.denseBias = initializeVariable([outputSize], false, 'zeros');\n\n        this.dropout = dropout;\n    }\n\n    /**\n     * Gives the initial state values of the LSTM (c and h).\n     *\n     * @param clone If it is necessary to clone states variable or no.\n     */\n    initLSTM(clone: boolean = true): {c: tf.Tensor2D, h: tf.Tensor2D} {\n        return {\n            c: <tf.Tensor2D> (clone ? this.lstmInitC.clone() : this.lstmInitC),\n            h: <tf.Tensor2D> (clone ? this.lstmInitH.clone() : this.lstmInitH)\n        };\n    }\n\n    /**\n     * Make a prediction given an input and state values (c and h).\n     * @param x A vector of shape [inputSize].\n     * @param c LSTM's state value.\n     * @param h LSTM's last output value.\n     * @param mask A vector of ones and zeros of shape [outputSize].\n     */\n    predict(x: tf.Tensor1D, c: tf.Tensor2D, h: tf.Tensor2D, mask?: tf.Tensor1D,\n        temperature: number = 1): LSTMPrediction {\n        return tf.tidy(() => {\n            // Execute the LSTM cell.\n            const [nc, nh] = tf.basicLSTMCell(\n                <tf.Scalar> this.lstmForgetBias,\n                <tf.Tensor2D> this.lstmKernel,\n                <tf.Tensor1D> this.lstmBias,\n                <tf.Tensor2D> tf.stack([x]),\n                h, c\n            );\n\n            // Execute the dense layer on top of the LSTM cell.\n            let y = <tf.Tensor1D> tf\n                .dropout(nh, this.dropout)\n                .matMul(this.denseWeights)\n                .add(this.denseBias)\n                .squeeze()\n                .div(temperature)\n                .softmax()\n                .mul(mask ?? 1);\n\n            // Apply normalization after the mask to get probabilities.\n            y = y.div(tf.sum(y));\n\n            return { y, nc, nh };\n        });\n    }\n\n    /**\n     * Update the given model parameters.\n     */\n    load(weights: {[key: string]: any}) {\n        tf.tidy(() => {\n            // Convert every parameter to a tf variable tensor.\n            this.lstmKernel = tf.tensor(weights.lstmKernel).variable();\n            this.lstmBias = tf.tensor(weights.lstmBias).variable();\n            this.lstmForgetBias = tf.tensor(weights.lstmForgetBias).variable();\n            this.lstmInitH = tf.tensor(weights.lstmInitH).variable();\n            this.lstmInitC = tf.tensor(weights.lstmInitC).variable();\n            this.denseWeights = tf.tensor(weights.denseWeights).variable();\n            this.denseBias = tf.tensor(weights.denseBias).variable();\n        });\n    }\n\n    /**\n     * Return all the LSTM model parameters.\n     */\n    async export(): Promise<{[key: string]: any}> {\n        const exports = {\n            lstmKernel: await this.lstmKernel.array(),\n            lstmBias: await this.lstmBias.array(),\n            lstmForgetBias: await this.lstmForgetBias.array(),\n            lstmInitH: await this.lstmInitH.array(),\n            lstmInitC: await this.lstmInitC.array(),\n            denseWeights: await this.denseWeights.array(),\n            denseBias: await this.denseBias.array()\n        };\n\n        return exports;\n    }\n}\n","/**\n * Inspired by Trie.js\n * https://gist.github.com/tpae/72e1c54471e88b689f85ad2b3940a8f0\n */\n\nexport class TrieNode {\n    readonly parent: TrieNode;\n    readonly key: string;\n    readonly childs: {[key: string]: TrieNode};\n    ending: boolean;\n\n    constructor(key: string, parent: TrieNode) {\n        this.key = key;\n        this.parent = parent;\n\n        this.childs = {};\n        this.ending = false;\n    }\n\n    addChild(child: TrieNode) {\n        this.childs[child.key] = child;\n    }\n\n    setEnding() {\n        this.ending = true;\n    }\n}\n\nexport class Trie {\n    root: TrieNode;\n\n    constructor() {\n        this.root = new TrieNode('', null);\n    }\n\n    add(word: string) {\n        let actualNode = this.root;\n\n        for (let i = 0; i < word.length; i += 1) {\n            if (actualNode.childs[word[i]] === undefined) {\n                actualNode.addChild(new TrieNode(word[i], actualNode));\n            }\n\n            actualNode = actualNode.childs[word[i]];\n\n            if (i === word.length - 1) {\n                actualNode.setEnding();\n            }\n        }\n    }\n\n    split(text: string, unknownKey: string = undefined, ignoreTokens: string[] = []): string[] {\n        const splittedText = [];\n\n        function pushUnknown(word: string) {\n            if (splittedText[splittedText.length - 1] !== unknownKey\n                && !ignoreTokens.includes(word)) {\n                splittedText.push(unknownKey);\n            }\n        }\n\n        let node = this.root;\n        let word = '';\n\n        for (let i = 0; i < text.length; i += 1) {\n            const character = text[i];\n\n            if (node.childs[character] !== undefined) {\n                word += character;\n                node = node.childs[character];\n            } else {\n                if (node.ending) splittedText.push(word);\n                else pushUnknown(word);\n\n                if (this.root.childs[character] !== undefined) {\n                    word = character;\n                    node = this.root.childs[character];\n                } else {\n                    pushUnknown(word);\n                    word = '';\n                    node = this.root;\n                }\n            }\n        }\n\n        if (node.ending) splittedText.push(word);\n        else pushUnknown(word);\n\n        return splittedText;\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Featurizer } from './featurizer';\nimport { initializeVariable } from '../utils';\n\n/**\n * Parameters for ActionFeaturizer constructor.\n */\ninterface ActionFeaturizerArgs {\n    /**\n     * Enable the masking of LUS when the user has just talked.\n     * Enabled by default.\n     */\n    maskLUS?: boolean;\n\n    /**\n     * Enable the masking of the previous action.\n     * Enabled by default.\n     */\n    maskPreviousAction?: boolean;\n\n    /**\n     * The action the bot takes to let the user talk.\n     * Default to 'LUS' (acronym for Let User Speak).\n     */\n    LUSAction?: string;\n}\n\n/**\n * Rule-based featurizer improving model robustness.\n *\n * - Featurize the previous action the model has taken.\n * - Mask the LUS action when the user has just talked.\n *   (Force the model to reply at least once)\n * - Mask the previous action.\n *   (Prevent looping : the model can't take two times in a row the same action)\n */\nexport class ActionFeaturizer extends Featurizer {\n    readonly id = 'Action Featurizer';\n    size: number;\n\n    private LUSAction: any;\n    private maskLUS: boolean;\n    private maskPreviousAction: boolean;\n\n    private userTalked: boolean;\n    private previousAction: any;\n\n    private embeddings: tf.Tensor;\n\n    constructor({\n        maskLUS = true,\n        maskPreviousAction = true,\n        LUSAction = 'LUS'\n    }: ActionFeaturizerArgs = { maskLUS: true, maskPreviousAction: true, LUSAction: 'LUS' }) {\n        super();\n        this.maskLUS = maskLUS;\n        this.maskPreviousAction = maskPreviousAction;\n        this.LUSAction = LUSAction;\n\n        this.resetDialog();\n    }\n\n    async init(actions: any[]) {\n        await super.init(actions);\n\n        this.size = actions.length;\n        this.embeddings = initializeVariable([this.size, this.size]);\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor2D> {\n        return tf.tidy(() => {\n            this.userTalked = query !== '';\n\n            // One-hot encode the previous action.\n            return <tf.Tensor2D> tf.oneHot(\n                [this.actions.indexOf(this.previousAction)],\n                this.actions.length\n            );\n        });\n    }\n\n    getOptimizableFeatures(data: tf.Tensor2D): tf.Tensor1D {\n        return <tf.Tensor1D> data.matMul(this.embeddings).squeeze();\n    }\n\n    handleAction(action: any) {\n        // Store the new action if it's not the LUS action.\n        this.previousAction = action !== this.LUSAction ? action : this.previousAction;\n    }\n\n    getActionMask(): boolean[] {\n        const mask = super.getActionMask();\n\n        // Mask LUS when the user talk and the option is enabled.\n        if (this.maskLUS && this.userTalked) {\n            mask[this.actions.indexOf(this.LUSAction)] = false;\n        }\n\n        // Mask the previous action when the option is enabled and if applicable.\n        if (this.maskPreviousAction && this.actions.includes(this.previousAction)) {\n            mask[this.actions.indexOf(this.previousAction)] = false;\n        }\n\n        return mask;\n    }\n\n    resetDialog() {\n        this.userTalked = false;\n        this.previousAction = undefined;\n    }\n\n    load(parameters: {embeddings: number[][]}) {\n        this.embeddings = tf.tidy(() => tf.tensor(parameters.embeddings).variable());\n    }\n\n    async export(): Promise<{embeddings: number[][]}> {\n        return {\n            embeddings: <number[][]> await this.embeddings.array()\n        };\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Featurizer } from './featurizer';\nimport { hashcode } from '../utils';\n\n/**\n * Featurizes queries as bag of words.\n *\n * The algorithm uses the [hashing trick](https://en.wikipedia.org/wiki/Feature_hashing) to avoid\n * having to store a vocabulary in the memory.\n */\nexport class BOW extends Featurizer {\n    readonly id = 'Bag-of-Words';\n    readonly size: number;\n\n    /**\n     * @param size The vocabulary size you allow to the featurizer.\n     */\n    constructor(size: number) {\n        super();\n        this.size = size;\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor1D> {\n        return tf.tidy(() => {\n            const indexes = query.toLowerCase()\n                .split(/\\W/g)\n                .map((word) => hashcode(word) % this.size);\n\n            return <tf.Tensor1D> tf.oneHot(indexes, this.size).asType('float32').sum(0);\n        });\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport * as use from '@tensorflow-models/universal-sentence-encoder';\nimport { Featurizer } from './featurizer';\n\n/**\n * Featurizes queries using the Universal Sentence Encoder model.\n */\nexport class USE extends Featurizer {\n    readonly id = 'Universal Sentence Encoder';\n\n    private encoder: use.UniversalSentenceEncoder;\n    private emptyEncoding: tf.Tensor1D;\n\n    readonly size = 512;\n\n    async init(actions: any[]) {\n        await super.init(actions);\n        this.encoder = await use.load();\n\n        // Cache the empty string embed (for optimization purpose).\n        this.emptyEncoding = await this.encodeQuery('');\n    }\n\n    /**\n     * Encodes a query using the model.\n     */\n    private async encodeQuery(query: string): Promise<tf.Tensor1D> {\n        const embed = await this.encoder.embed([query]);\n        const squeezedEmbed = <tf.Tensor1D> embed.squeeze();\n        tf.dispose(embed);\n\n        return squeezedEmbed;\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor1D> {\n        // When the query is empty, return the cached empty query encoding.\n        if (!query) {\n            return this.emptyEncoding.clone();\n        }\n\n        return this.encodeQuery(query);\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Featurizer } from './featurizer';\nimport { KeyedVectors } from '../tools';\n\n/**\n * Featurize queries by pooling words embedding using SWEM-concat(*).\n *\n * (*): Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, Qinliang Su, Yizhe Zhang,\n *      Chunyuan Li, Ricardo Henao, Lawrence Carin- 2018.\n *      Baseline Needs More Love: On Simple Word-Embedding-Based Models and\n *      Associated Pooling Mechanisms.\n */\nexport class WordEmbedding extends Featurizer {\n    readonly id = 'Word Embedding';\n    readonly size: number;\n\n    private vectors: KeyedVectors;\n\n    /**\n     * @param vectors The keyed vectors storing the embeddings.\n     */\n    constructor(vectors: KeyedVectors) {\n        super();\n        this.size = 2 * vectors.size;\n        this.vectors = vectors;\n    }\n\n    async init(actions: any[]) {\n        await super.init(actions);\n\n        if (!this.vectors.isLoaded()) await this.vectors.load();\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor1D> {\n        return <tf.Tensor1D> tf.tidy(() => {\n            const tokens = this.vectors.tokenize(query);\n            const embeddings = tokens\n                .map((token) => this.vectors.get(token))\n                .filter((v) => v !== undefined);\n\n            // When there is no embeddable tokens, return a zeros vector.\n            if (embeddings.length === 0) {\n                return tf.zeros([this.size]);\n            }\n\n            const embeddingsMatrix = tf.stack(embeddings);\n\n            return tf.concat([embeddingsMatrix.mean(0), embeddingsMatrix.max(0)]);\n        });\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Featurizer } from '../featurizers';\nimport { LSTM, Story, Metrics } from '../utils';\n\ninterface SampleData {\n    targets: tf.Tensor1D,\n    predictions: tf.Tensor1D,\n    loss: number,\n    isFailing: boolean\n}\n\n/**\n * @callback\n */\ntype TrainingCallback = (metrics: Metrics) => void;\n\n/**\n * Parameters for HCN constructor.\n */\ninterface HCNConstructorArgs {\n    /**\n     * The list of actions the model can take.\n     * (keeping the order the same is important for pretrained models)\n     */\n    actions: string[];\n\n    /**\n     * The list of featurizers the model uses.\n     * (keeping the order the same is important for pretrained models)\n     */\n    featurizers: Featurizer[];\n\n    /**\n     * The output size of the LSTM cell.\n     * Default is set to 32 units.\n     */\n    hiddenSize?: number;\n\n    /**\n     * The optimization algorithm used for training.\n     * By default, Adam with a learning rate of 0.01 is used.\n     */\n    optimizer?: tf.Optimizer;\n\n    /**\n     * Temperature of the model softmax, used to calibrate confidence estimation.\n     * By default, the temperature is 1 but you usually want it higher to make less overconfident.\n     */\n    temperature?: number;\n\n    /**\n     * The percentage of units to dropout between the LSTM cell layer and the dense.\n     * Useful for regularizing the model. It's disabled by default (value = 0).\n     */\n    dropout?: number;\n}\n\n/**\n * Parameters for HCN train method.\n */\ninterface HCNTrainArgs {\n    /**\n     * Training stories to learn from.\n     */\n    stories: Story[];\n\n    /**\n     * Number of times the model will be passed the whole set of training stories during training.\n     * Default is set to 12 epochs.\n     */\n    nEpochs?: number;\n\n    /**\n     * After each epoch, this callback function will be executed with the metrics collected\n     * during the epoch.\n     */\n    onEpochEnd?: TrainingCallback;\n}\n\n/**\n * An implementation of Hybrid Code Networks(*) dialog manager.\n *\n * (*): Williams, Asadi, Zweig - 2017.\n *      Hybrid Code Networks: practical and efﬁcient end-to-end dialog control with supervised\n *      and reinforcement learning.\n */\nexport class HCN {\n    private actions: string[];\n    private featurizers: Featurizer[];\n    private optimizer: tf.Optimizer;\n\n    private inputSize: number;\n    private hiddenSize: number;\n    private outputSize: number;\n\n    private lstm: LSTM;\n    private lstmH: tf.Tensor2D;\n    private lstmC: tf.Tensor2D;\n    private lstmTemperature: number;\n    private lstmDropout: number;\n\n    /**\n     * Defines the model.\n     *\n     * To fully initialize the model, run the async *init()* method.\n     */\n    constructor({\n        actions,\n        featurizers,\n        hiddenSize = 32,\n        optimizer = tf.train.adam(0.01),\n        temperature = 1,\n        dropout = 0\n    }: HCNConstructorArgs) {\n        this.actions = actions;\n        this.featurizers = featurizers;\n        this.optimizer = optimizer;\n\n        this.hiddenSize = hiddenSize;\n        this.outputSize = actions.length;\n\n        this.lstmTemperature = temperature;\n        this.lstmDropout = dropout;\n    }\n\n    /**\n     * Initialize the model and its featurizers.\n     */\n    async init() {\n        // Initialize asynchronously all featurizers.\n        await Promise.all(\n            this.featurizers.map((featurizer) => featurizer.init(this.actions))\n        );\n\n        // The model input size is the sum of the sizes of features vectors.\n        this.inputSize = this.featurizers\n            .map((featurizer) => featurizer.size)\n            .reduce((acc, size) => acc + size, 1);\n\n        this.lstm = new LSTM(this.inputSize, this.hiddenSize, this.outputSize, this.lstmDropout);\n\n        this.resetDialog();\n    }\n\n    /**\n     * Resets the state of the model and its featurizers.\n     */\n    resetDialog() {\n        this.featurizers.forEach((featurizer) => featurizer.resetDialog());\n        ({ c: this.lstmC, h: this.lstmH } = this.lstm.initLSTM());\n    }\n\n    /**\n     * Get the data returned from every featurizer's handleQuery method.\n     */\n    private async handleQuery(query: string): Promise<any[]> {\n        return Promise.all(\n            this.featurizers.map((featurizer) => featurizer.handleQuery(query))\n        );\n    }\n\n    /**\n     * Get the embedding vector resulted from every featurizers.\n     */\n    private getOptimizableFeatures(features: tf.Tensor[]): tf.Tensor1D {\n        return tf.tidy(() => {\n            const embeddings = this.featurizers.map(\n                (featurizer, idx) => featurizer.getOptimizableFeatures(features[idx])\n            );\n\n            // Add a zero to make tf.concat work consistently even with only one featurizer.\n            embeddings.push(tf.zeros([1]));\n\n            return <tf.Tensor1D> tf.concat(embeddings);\n        });\n    }\n\n    /**\n     * Inform every featurizers of the taken action.\n     */\n    private handleAction(action: string) {\n        this.featurizers.map((featurizer) => featurizer.handleAction(action));\n    }\n\n    /**\n     * Get the final action mask resulted from every featurizers.\n     */\n    private getActionMask(): tf.Tensor1D {\n        return tf.tidy(() => this.featurizers\n            // Get action mask and convert them to tensors.\n            .map((featurizer) => <tf.Tensor1D> tf.tensor(\n                featurizer.getActionMask(),\n                undefined, 'float32'\n            ))\n            // Compute the product of every masks.\n            .reduce((acc, mask) => tf.mul(acc, mask), tf.ones([this.actions.length])));\n    }\n\n    /**\n     * Trains the model on a single training story.\n     */\n    private async fitStory(story: Story): Promise<SampleData> {\n        this.resetDialog();\n\n        // 1. Prepare the input data.\n        const inputs: any[][] = [];\n        const masks: tf.Tensor1D[] = [];\n        const targets: tf.Tensor1D[] = [];\n\n        // For each story's state...\n        for (let stateIdx = 0; stateIdx < story.length; stateIdx += 1) {\n            const state = story[stateIdx];\n\n            // The query must be featurized before moving to the next state.\n            // eslint-disable-next-line no-await-in-loop\n            inputs.push(await this.handleQuery(state.query));\n\n            masks.push(this.getActionMask());\n\n            targets.push(\n                <tf.Tensor1D> tf.oneHot(\n                    this.actions.indexOf(state.action),\n                    this.outputSize\n                )\n            );\n\n            this.handleAction(state.action);\n        }\n\n        // 2. Fit the sequence.\n        let data: SampleData;\n\n        this.optimizer.minimize(() => {\n            let { c, h } = this.lstm.initLSTM(false);\n\n            // Make a prediction for each step of the input sequence.\n            const predictions = inputs.map((features, idx) => {\n                const statePred = this.lstm.predict(\n                    <tf.Tensor1D> this.getOptimizableFeatures(features),\n                    <tf.Tensor2D> c,\n                    <tf.Tensor2D> h,\n                    <tf.Tensor1D> masks[idx]\n                );\n\n                c = statePred.nc;\n                h = statePred.nh;\n\n                return statePred.y;\n            });\n\n            const targetsMatrix = tf.stack(targets);\n            const predictionsMatrix = tf.stack(predictions);\n\n            // Compare the predicted sequence with the target.\n            const lossScalar = <tf.Scalar> tf.metrics.categoricalCrossentropy(\n                targetsMatrix, predictionsMatrix\n            ).mean();\n\n            // Store the necessary data to build metrics.\n            data = {\n                targets: tf.keep(targetsMatrix.argMax(1)),\n                predictions: tf.keep(predictionsMatrix.argMax(1)),\n                loss: <number> lossScalar.arraySync(),\n                isFailing: tf.metrics\n                    .categoricalAccuracy(targetsMatrix, predictionsMatrix)\n                    .mean()\n                    .arraySync() < 0.999\n            };\n\n            // Return the loss to the optimizer to update the model.\n            return lossScalar;\n        });\n\n        // BUG: two tensors leak in the memory at each loop :/\n        tf.dispose([inputs, targets]);\n\n        return data;\n    }\n\n    /**\n     * Trains the model using the training stories.\n     *\n     * @returns Metrics collected from the last epoch (that correspond to the trained model).\n     */\n    async train({ stories, nEpochs = 12, onEpochEnd = undefined }: HCNTrainArgs): Promise<Metrics> {\n        let epochMetrics: Metrics;\n\n        // For each epoch...\n        for (let epoch = 0; epoch < nEpochs; epoch += 1) {\n            const allTargets: tf.Tensor1D[] = [];\n            const allPredictions: tf.Tensor1D[] = [];\n            const allLosses: number[] = [];\n            const failingSamples: number[] = [];\n\n            // For each training story...\n            for (let storyIdx = 0; storyIdx < stories.length; storyIdx += 1) {\n                // (Each story must be fitted sequentially)\n                // eslint-disable-next-line no-await-in-loop\n                const storyData = await this.fitStory(stories[storyIdx]);\n\n                allTargets.push(storyData.targets);\n                allPredictions.push(storyData.predictions);\n                allLosses.push(storyData.loss);\n\n                if (storyData.isFailing) {\n                    failingSamples.push(storyIdx);\n                }\n            }\n\n            // Build the metrics.\n            const confusionMatrix = tf.tidy(() => tf.math.confusionMatrix(\n                tf.concat(allTargets),\n                tf.concat(allPredictions),\n                this.outputSize\n            ));\n\n            const truePredictions = tf.tidy(() => confusionMatrix\n                .mul(tf.eye(...confusionMatrix.shape))\n                .sum(0));\n\n            epochMetrics = {\n                epoch,\n                failingSamples,\n\n                accuracy: <number> tf.tidy(() => (\n                    truePredictions.sum().div(confusionMatrix.sum()).arraySync()\n                )),\n\n                recall: <number> tf.tidy(() => (\n                    truePredictions.div(confusionMatrix.sum(1)).mean().arraySync()\n                )),\n\n                precision: <number> tf.tidy(() => (\n                    truePredictions.div(confusionMatrix.sum(0)).mean().arraySync()\n                )),\n\n                loss: allLosses.reduce((a, b) => a + b) / allLosses.length\n            };\n\n            // Clear the tensors.\n            tf.dispose(allTargets);\n            tf.dispose(allPredictions);\n            tf.dispose([truePredictions, confusionMatrix]);\n\n            if (onEpochEnd !== undefined) {\n                onEpochEnd(epochMetrics);\n            }\n        }\n\n        this.resetDialog();\n        return epochMetrics;\n    }\n\n    /**\n     * Predict an action resulting from the given query.\n     *\n     * @param query The given query from the user.\n     * @returns The predicted action from the model and its confidence.\n     */\n    async predict(query: string):\n        Promise<{action: string, confidence: number}> {\n        // At inference, dropout is disabled\n        this.lstm.dropout = 0;\n\n        const features = this.getOptimizableFeatures(await this.handleQuery(query));\n        const masks = this.getActionMask();\n\n        const prediction = this.lstm.predict(\n            features, this.lstmC, this.lstmH, masks, this.lstmTemperature\n        );\n\n        // Update lstm internal state\n        tf.dispose([this.lstmC, this.lstmH]);\n        this.lstmC = prediction.nc.clone();\n        this.lstmH = prediction.nh.clone();\n\n        const actionIdx = <number> tf.tidy(() => prediction.y.argMax().arraySync());\n        const confidence = <number> tf.tidy(() => prediction.y.arraySync()[actionIdx]);\n\n        // Clear the memory\n        tf.dispose([features, masks]);\n        tf.dispose(prediction);\n\n        // Retablish dropout (just in case)\n        this.lstm.dropout = this.lstmDropout;\n\n        this.handleAction(this.actions[actionIdx]);\n\n        return { action: this.actions[actionIdx], confidence };\n    }\n\n    /**\n     * Evaluate the model using stories.\n     *\n     * @param stories Validation stories to evaluate the model.\n     * @returns Validation metrics based on the results from the stories.\n     */\n    async score(stories: Story[]): Promise<Metrics> {\n        const targets: number[] = [];\n        const predictions: number[] = [];\n        const confidences: number[] = [];\n        const failingSamples: number[] = [];\n\n        // For each stories and states, make predictions.\n        for (let storyIdx = 0; storyIdx < stories.length; storyIdx += 1) {\n            this.resetDialog();\n\n            for (let stateIdx = 0; stateIdx < stories[storyIdx].length; stateIdx += 1) {\n                const state = stories[storyIdx][stateIdx];\n\n                // eslint-disable-next-line no-await-in-loop\n                const { action, confidence } = await this.predict(state.query);\n\n                targets.push(this.actions.indexOf(state.action));\n                predictions.push(this.actions.indexOf(action));\n                confidences.push(confidence);\n\n                if (action !== state.action && !failingSamples.includes(storyIdx)) {\n                    failingSamples.push(storyIdx);\n                }\n            }\n        }\n\n        // Build a confusion matrix out of the prediction and build the metrics.\n        const confusionMatrix = tf.tidy(() => tf.math.confusionMatrix(\n            tf.tensor(targets), tf.tensor(predictions), this.outputSize\n        ));\n\n        const truePredictions = tf.tidy(() => confusionMatrix\n            .mul(tf.eye(...confusionMatrix.shape))\n            .sum(0));\n\n        const metrics: Metrics = {\n            failingSamples,\n\n            accuracy: <number> tf.tidy(() => (\n                truePredictions.sum().div(confusionMatrix.sum()).arraySync()\n            )),\n\n            recall: <number> tf.tidy(() => (\n                truePredictions.div(confusionMatrix.sum(1)).mean().arraySync()\n            )),\n\n            precision: <number> tf.tidy(() => (\n                truePredictions.div(confusionMatrix.sum(0)).mean().arraySync()\n            )),\n\n            averageConfidence: confidences.reduce((a, b) => a + b) / confidences.length\n        };\n\n        tf.dispose([truePredictions, confusionMatrix]);\n\n        this.resetDialog();\n        return metrics;\n    }\n\n    /**\n     * Load the models parameters from a JSON formatted string.\n     */\n    load(json: string) {\n        const parameters = JSON.parse(json);\n\n        this.featurizers.forEach((featurizer) => {\n            featurizer.load(parameters[featurizer.id]);\n        });\n\n        this.lstm.load(parameters.lstm);\n\n        this.resetDialog();\n    }\n\n    /**\n     * Export the models parameters in a JSON format.\n     */\n    async export(): Promise<string> {\n        const parameters = { lstm: await this.lstm.export() };\n\n        for (let idx = 0; idx < this.featurizers.length; idx += 1) {\n            const featurizer = this.featurizers[idx];\n\n            // eslint-disable-next-line no-await-in-loop\n            parameters[featurizer.id] = await featurizer.export();\n        }\n\n        return JSON.stringify(parameters);\n    }\n}\n","import { Featurizer } from '../featurizers';\n\n/**\n * An extension of featurizer that holds a value in its state.\n * @abstract\n */\nexport class Slot<Value> extends Featurizer {\n    /**\n     * The list of actions that can be taken by the model only when the slot is defined.\n     */\n    private dependantActions: string[];\n\n    /**\n     * The list of actions that can be taken by the model only when the slot is undefined.\n     */\n    private invDependantActions: string[];\n\n    /**\n     * Stores the value of the slot.\n     */\n    private value: Value;\n\n    /**\n     * @param dependantActions The list of actions that can be taken by the model\n     *                         only when the slot is defined.\n     * @param invDependantActions The list of actions that can be taken by the model\n     *                            only when the slot is undefined.\n     */\n    constructor(dependantActions: string[], invDependantActions: string[]) {\n        super();\n        this.dependantActions = dependantActions;\n        this.invDependantActions = invDependantActions;\n    }\n\n    getActionMask(): boolean[] {\n        return this.actions.map((action) => {\n            const isDefined = this.value !== undefined;\n            const isDependant = this.dependantActions.includes(action);\n            const isInvDependant = this.invDependantActions.includes(action);\n\n            return (!isDefined && (!isDependant || isInvDependant))\n                || (isDefined && !isInvDependant);\n        });\n    }\n\n    resetDialog(): void {\n        this.value = undefined;\n    }\n\n    /**\n     * Retrieves the value of the slot.\n     */\n    getValue(): Value {\n        return this.value;\n    }\n\n    /**\n     * Redefine a new value for the slot.\n     */\n    setValue(value: Value) {\n        this.value = value;\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Slot } from './slot';\nimport { fuzzyMatch } from '../utils';\n\ntype Categories = {[category: string]: string[]};\ntype CategoricalValue = { category: string, extract: string, score: number };\n\n/**\n * Parameters for Categorical Slot constructor.\n */\ninterface CategoricalSlotArgs {\n    /**\n     * The name of the slot.\n     */\n    name: string;\n\n    /**\n     * An object with the name of the category as a key and an array of synonyms that belong to\n     * the category as a value.\n     */\n    categories: Categories;\n\n    /**\n     * The list of actions that can be taken by the model only when the slot is defined.\n     */\n    dependantActions?: string[];\n\n    /**\n     * The list of actions that can be taken by the model only when the slot is undefined.\n     */\n    invDependantActions?: string[];\n\n    /**\n     * The minimum similarity to get selected as a value. (based on Leveinshtein Distance)\n     */\n    threshold?: number;\n}\n\n/**\n * A slot that stores a categorical value extracted using fuzzy string matching.\n */\nexport class CategoricalSlot extends Slot<CategoricalValue> {\n    readonly id: string;\n    readonly size: number;\n\n    private categoryNames: string[];\n    private categories: Categories;\n\n    private threshold: number;\n\n    constructor({\n        name,\n        categories,\n        dependantActions = [],\n        invDependantActions = [],\n        threshold = 0.75\n    }: CategoricalSlotArgs) {\n        super(dependantActions, invDependantActions);\n\n        this.categoryNames = Object.keys(categories);\n        this.categories = categories;\n        this.threshold = threshold;\n\n        this.id = `${name}#Categorical`;\n        this.size = 2 * this.categoryNames.length;\n    }\n\n    async init(actions: any[]) {\n        await super.init(actions);\n        this.resetDialog();\n    }\n\n    private oneHotValue(value: CategoricalValue): tf.Tensor1D {\n        const categoryNames = Object.keys(this.categories);\n\n        return <tf.Tensor1D> tf.oneHot(\n            categoryNames.indexOf(value.category),\n            categoryNames.length\n        );\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor1D> {\n        const previousValue = this.getValue();\n        let bestValue: CategoricalValue = { category: undefined, extract: undefined, score: 0 };\n\n        // For each category...\n        Object.entries(this.categories).forEach(([category, keywords]) => {\n            // Find the best match of the category.\n            const match = keywords\n                .map((keyword) => fuzzyMatch(query.toLowerCase(), keyword.toLowerCase()))\n\n                .filter((m) => m.score >= this.threshold)\n\n                .reduce(\n                    (hm, m) => (hm.score > m.score ? hm : m),\n                    { extract: undefined, score: 0 }\n                );\n\n            // The best match is preferably a match of a different category with the highest score.\n            if (match.extract !== undefined) {\n                const currentUneqPrevious = category !== previousValue.category;\n                const bestEqPrevious = bestValue.category === previousValue.category;\n                const betterScore = match.score > bestValue.score;\n\n                if (bestValue.category === undefined\n                    || (currentUneqPrevious && betterScore)\n                    || (bestEqPrevious && currentUneqPrevious)\n                    || (bestEqPrevious && betterScore)\n                ) {\n                    bestValue = { category, ...match };\n                }\n            }\n        });\n\n        const features = tf.tidy(() => (\n            tf.concat([\n                this.oneHotValue(bestValue),\n                this.oneHotValue(this.getValue())\n            ])\n        ));\n\n        if (bestValue.category !== undefined) {\n            this.setValue(bestValue);\n        }\n\n        return features;\n    }\n\n    getValue(): CategoricalValue {\n        if (super.getValue() === undefined) {\n            return { category: undefined, extract: undefined, score: 0 };\n        }\n\n        return super.getValue();\n    }\n}\n","import { Story, State } from '../utils';\n\n/**\n * Parse a source string formatted according the Wisty Training Story syntax.\n * Usually, this source string is extracted using fetch or from a file.\n */\nexport function parseStories(source: string): Story[] {\n    const stories: Story[] = [];\n    let story: Story = [];\n    let inputAnswered = true;\n\n    source.split('\\n').forEach((line) => {\n        const newStory = /^## *([^#].*)?$/gm.exec(line);\n        const newInput = /^> *(.*)$/gm.exec(line);\n        const newAction = /^- *(\\w*)$/gm.exec(line);\n\n        /*\n            New story\n            ## Story name\n        */\n        if (newStory != null && story.length > 0) {\n            story.push(<State> { query: '', action: 'LUS' });\n            stories.push(story); // Push previous story.\n            story = [];\n\n        /*\n            New user input\n            > user input\n        */\n        } else if (newInput != null) {\n            // Add a LUS action to mark the end of the previous turn.\n            if (story.length > 0) {\n                if (story[story.length - 1].action === undefined) {\n                    story[story.length - 1].action = 'LUS';\n                } else {\n                    story.push(<State> { query: '', action: 'LUS' });\n                }\n            }\n\n            story.push(<State> { query: newInput[1], action: undefined });\n            inputAnswered = false;\n\n        /*\n            New bot action\n            (> user input)\n            (- action_name)\n            - action_name\n        */\n        } else if (newAction != null && inputAnswered) {\n            story.push(<State> { query: '', action: newAction[1] });\n\n        /*\n            New bot action (first answer)\n            (> user input)\n            - action_name\n        */\n        } else if (newAction != null && !inputAnswered) {\n            // eslint-disable-next-line prefer-destructuring\n            story[story.length - 1].action = newAction[1];\n            inputAnswered = true;\n        }\n    });\n\n    // Finalize the last story\n    if (story.length > 0) {\n        story.push(<State> { query: '', action: 'LUS' });\n        stories.push(story);\n    }\n\n    return stories;\n}\n","import { Story } from '../utils';\n\n/**\n * Split a list of stories into random train and test subsets.\n *\n * @param stories A list of stories.\n * @param testSize The proportion of stories to put in the test subset.\n */\nexport function trainTestSplit(stories: Story[], testSize: number):\n    {train: Story[], test: Story[]} {\n    const test: Story[] = [];\n\n    for (let i = 0; i / stories.length < testSize; i += 1) {\n        test.push(...stories.splice(\n            Math.floor(Math.random() * stories.length), 1\n        ));\n    }\n\n    return { train: stories, test };\n}\n","import { HCN } from '../models';\nimport { Slot } from '../slots';\n\n/**\n * Parameters for NLUFormatter.\n */\ninterface NLUFormatterArgs {\n    model: HCN;\n    slots?: Slot<any>[];\n    LUSAction?: string;\n}\n\n/**\n * An NLU digests containing the input query, the list of bot action to take,\n * the overall confidence of the turn (product of action's confidences) and slots values.\n */\ninterface NLUDigest {\n    /**\n     * The raw input query.\n     */\n    query: string;\n\n    /**\n     * An array of actions names.\n     */\n    actions: string[];\n\n    /**\n     * The overall confidence of the turn.\n     */\n    turnConfidence: number;\n\n    /**\n     * The value of each slot.\n     */\n    slots: {[slot: string]: any};\n}\n\n/**\n * An utility class using HCN methods and Slots to offer an higher level API\n * looking like NLU librairies.\n */\nexport class NLUFormatter {\n    private model: HCN;\n    private slots: Slot<any>[];\n    private LUSAction: string;\n\n    constructor({\n        model,\n        slots = [],\n        LUSAction = 'LUS'\n    }: NLUFormatterArgs) {\n        this.model = model;\n        this.slots = slots;\n        this.LUSAction = LUSAction;\n    }\n\n    /**\n     * Turn a query into a NLU digest.\n     */\n    async ask(query: string): Promise<NLUDigest> {\n        const digest: NLUDigest = {\n            query,\n            actions: [],\n            turnConfidence: 1,\n            slots: {}\n        };\n\n        let { action, confidence } = await this.model.predict(query);\n\n        while (action !== this.LUSAction) {\n            digest.actions.push(action);\n            digest.turnConfidence *= confidence;\n\n            // eslint-disable-next-line no-await-in-loop\n            ({ action, confidence } = await this.model.predict(''));\n        }\n\n        this.slots.forEach((slot) => {\n            digest.slots[slot.id] = slot.getValue();\n        });\n\n        return digest;\n    }\n\n    /**\n     * Reset the model state.\n     */\n    resetDialog() {\n        this.model.resetDialog();\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { levenshteinDistance, Trie } from '../utils';\n\n/**\n * Parameters for KeyedVectors.\n */\ninterface KeyedVectorsArgs {\n    loaderFunction(): Promise<string>;\n    size: number;\n    tokenization?: 'word' | 'byte_pair';\n    cased?: boolean;\n    maxDistance?: number;\n    unknownKey?: string;\n}\n\n/**\n * A reusable class storing words embeddings for functions and class that needs it.\n */\nexport class KeyedVectors {\n    private vectors: {[key: string]: number[]};\n\n    private loaderFunction: () => Promise<string>;\n\n    readonly size: number;\n    private tokenization: 'word' | 'byte_pair';\n    private cased: boolean;\n    private trie: Trie;\n\n    private maxDistance: number;\n    private unknownKey: string;\n\n    /**\n     * Build a KeyedVector.\n     */\n    constructor({\n        loaderFunction,\n        size,\n        tokenization = 'word',\n        cased = false,\n        maxDistance = 0.5,\n        unknownKey = undefined\n    }: KeyedVectorsArgs) {\n        if (!['word', 'byte_pair'].includes(tokenization)) {\n            throw new Error('KeyedVector tokenization setting must be \"word\" or \"byte_pair\"');\n        }\n\n        this.loaderFunction = loaderFunction;\n\n        this.size = size;\n        this.tokenization = tokenization;\n        this.cased = cased;\n\n        this.maxDistance = maxDistance;\n        this.unknownKey = unknownKey;\n    }\n\n    /**\n     * Return every keys stored as an array.\n     */\n    keys(): string[] {\n        return Object.keys(this.vectors);\n    }\n\n    /**\n     * Load the word embeddings.\n     */\n    async load() {\n        this.vectors = JSON.parse(await this.loaderFunction());\n\n        if (this.tokenization === 'byte_pair') {\n            this.trie = new Trie();\n            this.keys().forEach((key) => this.trie.add(key));\n        }\n    }\n\n    /**\n     * Check if the word embeddings were loaded.\n     */\n    isLoaded(): boolean {\n        return this.vectors !== undefined;\n    }\n\n    /**\n     * Return the vector associated with a key.\n     * If the key is not part of the vocabulary, it will use a similar key according to\n     * the leveinshtein distance.\n     * If no similar keys are below `maxDistance`, it will return the unknown key vector or\n     * undefined.\n     */\n    get(key: string): tf.Tensor1D {\n        // If the token in in the vocabulary, just use its embedding.\n        if (this.keys().includes(key)) {\n            return tf.tensor1d(this.vectors[key]);\n        }\n\n        // If the token is out of vocabulary, use the most similarly spelled token instead.\n        let bestKey: string;\n        let lowestDistance: number = Infinity;\n\n        this.keys().forEach((knownKey) => {\n            const distance = levenshteinDistance(knownKey, key);\n\n            if (distance < lowestDistance) {\n                bestKey = knownKey;\n                lowestDistance = distance;\n            }\n        });\n\n        if (lowestDistance <= this.maxDistance) {\n            return tf.tensor(this.vectors[bestKey]);\n        }\n\n        // If no tokens are enough similar, return the unknownKey vector or undefined.\n        if (this.unknownKey !== undefined) {\n            return tf.tensor(this.vectors[this.unknownKey]);\n        }\n\n        return undefined;\n    }\n\n    /**\n     * Tokenize a string at each non-word character.\n     *\n     * @param text  A non tokenized text string.\n     */\n    // eslint-disable-next-line class-methods-use-this\n    private wordTokenize(text: string): string[] {\n        return text.split(/\\W/g).filter((token) => token.length > 0);\n    }\n\n    /**\n     * Tokenize a string based on the vocabulary.\n     *\n     * @param text A non tokenized text string.\n     */\n    private bytePairTokenize(text: string): string[] {\n        const pretext = ` ${text}`.split(' ').join('▁');\n\n        return this.trie.split(pretext, this.unknownKey, ['▁']);\n    }\n\n    /**\n     * Tokenize a string based on the settings.\n     *\n     * @param text A raw text string.\n     */\n    tokenize(text: string): string[] {\n        if (this.cased) {\n            // eslint-disable-next-line no-param-reassign\n            text = text.toLowerCase();\n        }\n\n        switch (this.tokenization) {\n        case 'word':\n            return this.wordTokenize(text);\n\n        case 'byte_pair':\n            return this.bytePairTokenize(text);\n\n        default:\n            throw new Error('KeyedVector tokenization setting must be \"word\" or \"byte_pair\"');\n        }\n    }\n}\n"],"names":["Featurizer","constructor","this","[object Object]","actions","query","getOptimizableFeatures","data","undefined","tf","handleAction","action","getActionMask","map","resetDialog","load","parameters","levenshteinDistance","s1","s2","d","Array","from","length","fill","i","j","Math","min","max","fuzzyMatch","text","substring","bestMatch","extract","score","aMatch","hashcode","input","hash","charCodeAt","initializeVariable","shape","scalar","init","initializer","heNormal","zeros","randomNormal","Error","randomTensor","apply","asScalar","variable","LSTM","inputSize","hiddenSize","outputSize","dropout","lstmKernel","lstmBias","lstmForgetBias","lstmInitH","lstmInitC","denseWeights","denseBias","initLSTM","clone","c","h","predict","x","mask","temperature","nc","nh","y","matMul","add","squeeze","div","softmax","mul","weights","array","TrieNode","key","parent","childs","ending","addChild","child","setEnding","Trie","root","word","actualNode","split","unknownKey","ignoreTokens","splittedText","pushUnknown","includes","push","node","character","maskLUS","maskPreviousAction","LUSAction","super","size","embeddings","userTalked","indexOf","previousAction","indexes","toLowerCase","asType","sum","encoder","use","emptyEncoding","encodeQuery","embed","squeezedEmbed","vectors","isLoaded","tokenize","token","get","filter","v","embeddingsMatrix","mean","featurizers","optimizer","adam","lstmTemperature","lstmDropout","Promise","all","featurizer","reduce","acc","lstm","forEach","lstmC","lstmH","handleQuery","features","idx","story","inputs","masks","targets","stateIdx","state","minimize","predictions","statePred","targetsMatrix","predictionsMatrix","lossScalar","categoricalCrossentropy","argMax","loss","arraySync","isFailing","categoricalAccuracy","stories","nEpochs","onEpochEnd","epochMetrics","epoch","allTargets","allPredictions","allLosses","failingSamples","storyIdx","storyData","fitStory","confusionMatrix","truePredictions","accuracy","recall","precision","a","b","prediction","actionIdx","confidence","confidences","metrics","averageConfidence","json","JSON","parse","id","export","stringify","Slot","dependantActions","invDependantActions","isDefined","value","isDependant","isInvDependant","getValue","setValue","name","categories","threshold","categoryNames","Object","keys","oneHotValue","category","previousValue","bestValue","entries","keywords","match","keyword","m","hm","currentUneqPrevious","bestEqPrevious","betterScore","source","inputAnswered","line","newStory","exec","newInput","newAction","testSize","test","splice","floor","random","train","model","slots","digest","turnConfidence","slot","loaderFunction","tokenization","cased","maxDistance","trie","bestKey","lowestDistance","Infinity","knownKey","distance","wordTokenize","bytePairTokenize","pretext","join"],"mappings":"2UASaA,EAAbC,cAeaC,UAAe,EAQxBC,WAAWC,GACPF,KAAKE,QAAUA,EAanBD,kBAAkBE,IAelBC,uBAAuBC,GACnB,YAAaC,IAATD,EACOE,EAAS,CAAC,IAGAF,EAOzBG,aAAaC,IAQbC,gBACI,YAAYR,QAAQS,IAAI,KAAM,GAOlCC,eAMAC,KAAKC,IAMLb,eACI,MAAO,aClGCc,EAAoBC,EAAYC,GAC5C,MAAMC,EAAIC,MAAMC,KACZD,MAAMH,EAAGK,OAAS,GAClB,IAAM,IAAIF,MAAMF,EAAGI,OAAS,GAAGC,KAAK,IAGxC,IAAK,IAAIC,EAAI,EAAGA,GAAKP,EAAGK,OAAQE,GAAK,EACjCL,EAAEK,GAAG,GAAKA,EAGd,IAAK,IAAIC,EAAI,EAAGA,GAAKP,EAAGI,OAAQG,GAAK,EACjCN,EAAE,GAAGM,GAAKA,EAGd,IAAK,IAAIA,EAAI,EAAGA,EAAIP,EAAGI,OAAQG,GAAK,EAChC,IAAK,IAAID,EAAI,EAAGA,EAAIP,EAAGK,OAAQE,GAAK,EAGhCL,EAAEK,EAAI,GAAGC,EAAI,GAAKC,KAAKC,IACnBR,EAAEK,GAAGC,EAAI,GAAK,EACdN,EAAEK,EAAI,GAAGC,GAAK,EACdN,EAAEK,GAAGC,IALiBR,EAAGO,KAAON,EAAGO,GAAM,EAAI,IAUzD,OAAON,EAAEF,EAAGK,QAAQJ,EAAGI,QAAUI,KAAKE,IAAIX,EAAGK,OAAQJ,EAAGI,OAAQ,YC1BpDO,EAAWC,EAAcC,GACrC,IAAIC,EAAmB,CAAEC,aAAS1B,EAAW2B,MAAO,GAEpD,IAAK,IAAIV,EAAI,EAAGA,EAAIM,EAAKR,OAAQE,GAAK,EAAG,CACrC,MAAMS,EAAUH,EAAKC,UAAUP,EAAGA,EAAIO,EAAUT,QAE1Ca,EAAS,CACXF,QAAAA,EACAC,MAAO,EAAIlB,EAAoBiB,EAASF,IAG5C,GAAqB,IAAjBI,EAAOD,MACP,OAAOC,EAGPA,EAAOD,MAAQF,EAAUE,QACzBF,EAAYG,GAIpB,OAAOH,WClBKI,EAASC,GACrB,IAAIC,EAAO,EAEX,IAAK,IAAId,EAAI,EAAGA,EAAIa,EAAMf,OAAQE,GAAK,EAGnCc,GAASA,GAAQ,GAAKA,EAFVD,EAAME,WAAWf,GAG7Bc,GAAQ,EAGZ,OAAOA,WCXKE,EAAmBC,EAAiBC,GAAkB,EAClEC,EAA8B,MAC9B,OAAOnC,EAAQ,KACX,IAAIoC,EAEJ,OAAQD,GACR,IAAK,KACDC,EAAcpC,EAAgBqC,SAAS,IACvC,MAEJ,IAAK,QACDD,EAAcpC,EAAgBsC,QAC9B,MAEJ,IAAK,SACDF,EAAcpC,EAAgBuC,aAAa,IAC3C,MAEJ,QACI,UAAUC,8EACkEL,OAIhF,IAAIM,EAAeL,EAAYM,MAAMT,GAGrC,OAFIC,IAAQO,EAAeA,EAAaE,YAEjCF,EAAaG,mBCxBfC,EAqBTrD,YAAYsD,EAAmBC,EAAoBC,EAAoBC,EAAkB,IACrFxD,KAAKyD,WAAalB,EAAmB,CAACc,EAAYC,EAAyB,EAAbA,IAC9DtD,KAAK0D,SAAWnB,EAAmB,CAAc,EAAbe,IAAiB,EAAO,SAC5DtD,KAAK2D,eAAiBpB,EAAmB,CAAC,IAAI,EAAM,SACpDvC,KAAK4D,UAAYrB,EAAmB,CAAC,EAAGe,IACxCtD,KAAK6D,UAAYtB,EAAmB,CAAC,EAAGe,IAExCtD,KAAK8D,aAAevB,EAAmB,CAACe,EAAYC,IACpDvD,KAAK+D,UAAYxB,EAAmB,CAACgB,IAAa,EAAO,SAEzDvD,KAAKwD,QAAUA,EAQnBQ,SAASC,GAAiB,GACtB,MAAO,CACHC,EAAkBD,EAAQjE,KAAK6D,UAAUI,QAAUjE,KAAK6D,UACxDM,EAAkBF,EAAQjE,KAAK4D,UAAUK,QAAUjE,KAAK4D,WAWhEQ,QAAQC,EAAgBH,EAAgBC,EAAgBG,EACpDC,EAAsB,GACtB,OAAOhE,EAAQ,KAEX,MAAOiE,EAAIC,GAAMlE,EACDP,KAAK2D,eACH3D,KAAKyD,WACLzD,KAAK0D,SACLnD,EAAS,CAAC8D,IACxBF,EAAGD,GAIP,IAAIQ,EAAkBnE,EACTkE,EAAIzE,KAAKwD,SACjBmB,OAAO3E,KAAK8D,cACZc,IAAI5E,KAAK+D,WACTc,UACAC,IAAIP,GACJQ,UACAC,IAAIV,GAAQ,GAKjB,OAFAI,EAAIA,EAAEI,IAAIvE,EAAOmE,IAEV,CAAEA,EAAAA,EAAGF,GAAAA,EAAIC,GAAAA,KAOxB5D,KAAKoE,GACD1E,EAAQ,KAEJP,KAAKyD,WAAalD,EAAU0E,EAAQxB,YAAYN,WAChDnD,KAAK0D,SAAWnD,EAAU0E,EAAQvB,UAAUP,WAC5CnD,KAAK2D,eAAiBpD,EAAU0E,EAAQtB,gBAAgBR,WACxDnD,KAAK4D,UAAYrD,EAAU0E,EAAQrB,WAAWT,WAC9CnD,KAAK6D,UAAYtD,EAAU0E,EAAQpB,WAAWV,WAC9CnD,KAAK8D,aAAevD,EAAU0E,EAAQnB,cAAcX,WACpDnD,KAAK+D,UAAYxD,EAAU0E,EAAQlB,WAAWZ,aAOtDlD,eAWI,MAVgB,CACZwD,sBAAuBA,WAAWyB,QAClCxB,oBAAqBA,SAASwB,QAC9BvB,0BAA2BA,eAAeuB,QAC1CtB,qBAAsBA,UAAUsB,QAChCrB,qBAAsBA,UAAUqB,QAChCpB,wBAAyBA,aAAaoB,QACtCnB,qBAAsBA,UAAUmB,gBChH/BC,EAMTpF,YAAYqF,EAAaC,GACrBrF,KAAKoF,IAAMA,EACXpF,KAAKqF,OAASA,EAEdrF,KAAKsF,OAAS,GACdtF,KAAKuF,QAAS,EAGlBC,SAASC,GACLzF,KAAKsF,OAAOG,EAAML,KAAOK,EAG7BC,YACI1F,KAAKuF,QAAS,SAITI,EAGT5F,cACIC,KAAK4F,KAAO,IAAIT,EAAS,GAAI,MAGjCP,IAAIiB,GACA,IAAIC,EAAa9F,KAAK4F,KAEtB,IAAK,IAAIrE,EAAI,EAAGA,EAAIsE,EAAKxE,OAAQE,GAAK,OACCjB,IAA/BwF,EAAWR,OAAOO,EAAKtE,KACvBuE,EAAWN,SAAS,IAAIL,EAASU,EAAKtE,GAAIuE,IAG9CA,EAAaA,EAAWR,OAAOO,EAAKtE,IAEhCA,IAAMsE,EAAKxE,OAAS,GACpByE,EAAWJ,YAKvBK,MAAMlE,EAAcmE,EAAgCC,EAAyB,IACzE,MAAMC,EAAe,GAErB,SAASC,EAAYN,GACbK,EAAaA,EAAa7E,OAAS,KAAO2E,GACtCC,EAAaG,SAASP,IAC1BK,EAAaG,KAAKL,GAI1B,IAAIM,EAAOtG,KAAK4F,KACZC,EAAO,GAEX,IAAK,IAAItE,EAAI,EAAGA,EAAIM,EAAKR,OAAQE,GAAK,EAAG,CACrC,MAAMgF,EAAY1E,EAAKN,QAEQjB,IAA3BgG,EAAKhB,OAAOiB,IACZV,GAAQU,EACRD,EAAOA,EAAKhB,OAAOiB,KAEfD,EAAKf,OAAQW,EAAaG,KAAKR,GAC9BM,EAAYN,QAEmBvF,IAAhCN,KAAK4F,KAAKN,OAAOiB,IACjBV,EAAOU,EACPD,EAAOtG,KAAK4F,KAAKN,OAAOiB,KAExBJ,EAAYN,GACZA,EAAO,GACPS,EAAOtG,KAAK4F,OAQxB,OAHIU,EAAKf,OAAQW,EAAaG,KAAKR,GAC9BM,EAAYN,GAEVK,yKCpDuBpG,EAalCC,aAAYyG,QACRA,GAAU,EADFC,mBAERA,GAAqB,EAFbC,UAGRA,EAAY,OACU,CAAEF,SAAS,EAAMC,oBAAoB,EAAMC,UAAW,QAC5EC,QAjBK3G,QAAK,oBAkBVA,KAAKwG,QAAUA,EACfxG,KAAKyG,mBAAqBA,EAC1BzG,KAAK0G,UAAYA,EAEjB1G,KAAKY,cAGTX,WAAWC,eACKwC,KAAKxC,GAEjBF,KAAK4G,KAAO1G,EAAQmB,OACpBrB,KAAK6G,WAAatE,EAAmB,CAACvC,KAAK4G,KAAM5G,KAAK4G,OAG1D3G,kBAAkBE,GACd,OAAOI,EAAQ,KACXP,KAAK8G,WAAuB,KAAV3G,EAGGI,EACjB,CAACP,KAAKE,QAAQ6G,QAAQ/G,KAAKgH,iBAC3BhH,KAAKE,QAAQmB,UAKzBjB,uBAAuBC,GACnB,OAAqBA,EAAKsE,OAAO3E,KAAK6G,YAAYhC,UAGtDrE,aAAaC,GAETT,KAAKgH,eAAiBvG,IAAWT,KAAK0G,UAAYjG,EAAST,KAAKgH,eAGpEtG,gBACI,MAAM4D,EAAOqC,MAAMjG,gBAYnB,OATIV,KAAKwG,SAAWxG,KAAK8G,aACrBxC,EAAKtE,KAAKE,QAAQ6G,QAAQ/G,KAAK0G,aAAc,GAI7C1G,KAAKyG,oBAAsBzG,KAAKE,QAAQkG,SAASpG,KAAKgH,kBACtD1C,EAAKtE,KAAKE,QAAQ6G,QAAQ/G,KAAKgH,kBAAmB,GAG/C1C,EAGX1D,cACIZ,KAAK8G,YAAa,EAClB9G,KAAKgH,oBAAiB1G,EAG1BO,KAAKC,GACDd,KAAK6G,WAAatG,EAAQ,IAAMA,EAAUO,EAAW+F,YAAY1D,YAGrElD,eACI,MAAO,CACH4G,sBAAoCA,WAAW3B,6BC3GlCpF,EAOrBC,YAAY6G,GACRD,QAPK3G,QAAK,eAQVA,KAAK4G,KAAOA,EAGhB3G,kBAAkBE,GACd,OAAOI,EAAQ,KACX,MAAM0G,EAAU9G,EAAM+G,cACjBnB,MAAM,OACNpF,IAAKkF,GAAS1D,EAAS0D,GAAQ7F,KAAK4G,MAEzC,OAAqBrG,EAAU0G,EAASjH,KAAK4G,MAAMO,OAAO,WAAWC,IAAI,yBCrB5DtH,EAAzBC,kCACaC,QAAK,6BAKLA,UAAO,IAEhBC,WAAWC,eACKwC,KAAKxC,GACjBF,KAAKqH,cAAgBC,IAGrBtH,KAAKuH,yBAA2BC,YAAY,IAMxCvH,kBAAkBE,GACtB,MAAMsH,aAAmBJ,QAAQI,MAAM,CAACtH,IAClCuH,EAA8BD,EAAM5C,UAG1C,OAFAtE,EAAWkH,GAEJC,EAGXzH,kBAAkBE,GAEd,OAAKA,OAIOqH,YAAYrH,QAHRoH,cAActD,sCCzBHnE,EAS/BC,YAAY4H,GACRhB,QATK3G,QAAK,iBAUVA,KAAK4G,KAAO,EAAIe,EAAQf,KACxB5G,KAAK2H,QAAUA,EAGnB1H,WAAWC,eACKwC,KAAKxC,GAEZF,KAAK2H,QAAQC,uBAAuBD,QAAQ9G,OAGrDZ,kBAAkBE,GACd,OAAqBI,EAAQ,KACzB,MACMsG,EADS7G,KAAK2H,QAAQE,SAAS1H,GAEhCQ,IAAKmH,GAAU9H,KAAK2H,QAAQI,IAAID,IAChCE,OAAQC,QAAY3H,IAAN2H,GAGnB,GAA0B,IAAtBpB,EAAWxF,OACX,OAAOd,EAAS,CAACP,KAAK4G,OAG1B,MAAMsB,EAAmB3H,EAASsG,GAElC,OAAOtG,EAAU,CAAC2H,EAAiBC,KAAK,GAAID,EAAiBvG,IAAI,sCC2DzE5B,aAAYG,QACRA,EADQkI,YAERA,EAFQ9E,WAGRA,EAAa,GAHL+E,UAIRA,EAAY9H,EAAS+H,KAAK,KAJlB/D,YAKRA,EAAc,EALNf,QAMRA,EAAU,IAEVxD,KAAKE,QAAUA,EACfF,KAAKoI,YAAcA,EACnBpI,KAAKqI,UAAYA,EAEjBrI,KAAKsD,WAAaA,EAClBtD,KAAKuD,WAAarD,EAAQmB,OAE1BrB,KAAKuI,gBAAkBhE,EACvBvE,KAAKwI,YAAchF,EAMvBvD,mBAEUwI,QAAQC,IACV1I,KAAKoI,YAAYzH,IAAKgI,GAAeA,EAAWjG,KAAK1C,KAAKE,WAI9DF,KAAKqD,UAAYrD,KAAKoI,YACjBzH,IAAKgI,GAAeA,EAAW/B,MAC/BgC,OAAO,CAACC,EAAKjC,IAASiC,EAAMjC,EAAM,GAEvC5G,KAAK8I,KAAO,IAAI1F,EAAKpD,KAAKqD,UAAWrD,KAAKsD,WAAYtD,KAAKuD,WAAYvD,KAAKwI,aAE5ExI,KAAKY,cAMTA,cACIZ,KAAKoI,YAAYW,QAASJ,GAAeA,EAAW/H,iBACjDsD,EAAGlE,KAAKgJ,MAAO7E,EAAGnE,KAAKiJ,OAAUjJ,KAAK8I,KAAK9E,YAM1C/D,kBAAkBE,GACtB,OAAOsI,QAAQC,IACX1I,KAAKoI,YAAYzH,IAAKgI,GAAeA,EAAWO,YAAY/I,KAO5DC,uBAAuB+I,GAC3B,OAAO5I,EAAQ,KACX,MAAMsG,EAAa7G,KAAKoI,YAAYzH,IAChC,CAACgI,EAAYS,IAAQT,EAAWvI,uBAAuB+I,EAASC,KAMpE,OAFAvC,EAAWR,KAAK9F,EAAS,CAAC,KAELA,EAAUsG,KAO/BrG,aAAaC,GACjBT,KAAKoI,YAAYzH,IAAKgI,GAAeA,EAAWnI,aAAaC,IAMzDC,gBACJ,OAAOH,EAAQ,IAAMP,KAAKoI,YAErBzH,IAAKgI,GAA6BpI,EAC/BoI,EAAWjI,qBACXJ,EAAW,YAGdsI,OAAO,CAACC,EAAKvE,IAAS/D,EAAOsI,EAAKvE,GAAO/D,EAAQ,CAACP,KAAKE,QAAQmB,WAMhEpB,eAAeoJ,GACnBrJ,KAAKY,cAGL,MAAM0I,EAAkB,GAClBC,EAAuB,GACvBC,EAAyB,GAG/B,IAAK,IAAIC,EAAW,EAAGA,EAAWJ,EAAMhI,OAAQoI,GAAY,EAAG,CAC3D,MAAMC,EAAQL,EAAMI,GAIpBH,EAAOjD,gBAAgB6C,YAAYQ,EAAMvJ,QAEzCoJ,EAAMlD,KAAKrG,KAAKU,iBAEhB8I,EAAQnD,KACU9F,EACVP,KAAKE,QAAQ6G,QAAQ2C,EAAMjJ,QAC3BT,KAAKuD,aAIbvD,KAAKQ,aAAakJ,EAAMjJ,QAI5B,IAAIJ,EA8CJ,OA5CAL,KAAKqI,UAAUsB,SAAS,KACpB,IAAIzF,EAAEA,EAAFC,EAAKA,GAAMnE,KAAK8I,KAAK9E,UAAS,GAGlC,MAAM4F,EAAcN,EAAO3I,IAAI,CAACwI,EAAUC,KACtC,MAAMS,EAAY7J,KAAK8I,KAAK1E,QACVpE,KAAKI,uBAAuB+I,GAC5BjF,EACAC,EACAoF,EAAMH,IAMxB,OAHAlF,EAAI2F,EAAUrF,GACdL,EAAI0F,EAAUpF,GAEPoF,EAAUnF,IAGfoF,EAAgBvJ,EAASiJ,GACzBO,EAAoBxJ,EAASqJ,GAG7BI,EAAyBzJ,EAAW0J,wBACtCH,EAAeC,GACjB5B,OAcF,OAXA9H,EAAO,CACHmJ,QAASjJ,EAAQuJ,EAAcI,OAAO,IACtCN,YAAarJ,EAAQwJ,EAAkBG,OAAO,IAC9CC,KAAeH,EAAWI,YAC1BC,UAAW9J,EACN+J,oBAAoBR,EAAeC,GACnC5B,OACAiC,YAAc,MAIhBJ,IAIXzJ,EAAW,CAAC+I,EAAQE,IAEbnJ,EAQXJ,aAAYsK,QAAEA,EAAFC,QAAWA,EAAU,GAArBC,WAAyBA,IACjC,IAAIC,EAGJ,IAAK,IAAIC,EAAQ,EAAGA,EAAQH,EAASG,GAAS,EAAG,CAC7C,MAAMC,EAA4B,GAC5BC,EAAgC,GAChCC,EAAsB,GACtBC,EAA2B,GAGjC,IAAK,IAAIC,EAAW,EAAGA,EAAWT,EAAQlJ,OAAQ2J,GAAY,EAAG,CAG7D,MAAMC,aAAuBC,SAASX,EAAQS,IAE9CJ,EAAWvE,KAAK4E,EAAUzB,SAC1BqB,EAAexE,KAAK4E,EAAUrB,aAC9BkB,EAAUzE,KAAK4E,EAAUd,MAErBc,EAAUZ,WACVU,EAAe1E,KAAK2E,GAK5B,MAAMG,EAAkB5K,EAAQ,IAAMA,EAAQ4K,gBAC1C5K,EAAUqK,GACVrK,EAAUsK,GACV7K,KAAKuD,aAGH6H,EAAkB7K,EAAQ,IAAM4K,EACjCnG,IAAIzE,KAAU4K,EAAgB3I,QAC9B4E,IAAI,IAETsD,EAAe,CACXC,MAAAA,EACAI,eAAAA,EAEAM,SAAmB9K,EAAQ,IACvB6K,EAAgBhE,MAAMtC,IAAIqG,EAAgB/D,OAAOgD,aAGrDkB,OAAiB/K,EAAQ,IACrB6K,EAAgBtG,IAAIqG,EAAgB/D,IAAI,IAAIe,OAAOiC,aAGvDmB,UAAoBhL,EAAQ,IACxB6K,EAAgBtG,IAAIqG,EAAgB/D,IAAI,IAAIe,OAAOiC,aAGvDD,KAAMW,EAAUlC,OAAO,CAAC4C,EAAGC,IAAMD,EAAIC,GAAKX,EAAUzJ,QAIxDd,EAAWqK,GACXrK,EAAWsK,GACXtK,EAAW,CAAC6K,EAAiBD,SAEV7K,IAAfmK,GACAA,EAAWC,GAKnB,OADA1K,KAAKY,cACE8J,EASXzK,cAAcE,GAGVH,KAAK8I,KAAKtF,QAAU,EAEpB,MAAM2F,EAAWnJ,KAAKI,kCAAkC8I,YAAY/I,IAC9DoJ,EAAQvJ,KAAKU,gBAEbgL,EAAa1L,KAAK8I,KAAK1E,QACzB+E,EAAUnJ,KAAKgJ,MAAOhJ,KAAKiJ,MAAOM,EAAOvJ,KAAKuI,iBAIlDhI,EAAW,CAACP,KAAKgJ,MAAOhJ,KAAKiJ,QAC7BjJ,KAAKgJ,MAAQ0C,EAAWlH,GAAGP,QAC3BjE,KAAKiJ,MAAQyC,EAAWjH,GAAGR,QAE3B,MAAM0H,EAAqBpL,EAAQ,IAAMmL,EAAWhH,EAAEwF,SAASE,aACzDwB,EAAsBrL,EAAQ,IAAMmL,EAAWhH,EAAE0F,YAAYuB,IAWnE,OARApL,EAAW,CAAC4I,EAAUI,IACtBhJ,EAAWmL,GAGX1L,KAAK8I,KAAKtF,QAAUxD,KAAKwI,YAEzBxI,KAAKQ,aAAaR,KAAKE,QAAQyL,IAExB,CAAElL,OAAQT,KAAKE,QAAQyL,GAAYC,WAAAA,GAS9C3L,YAAYsK,GACR,MAAMf,EAAoB,GACpBI,EAAwB,GACxBiC,EAAwB,GACxBd,EAA2B,GAGjC,IAAK,IAAIC,EAAW,EAAGA,EAAWT,EAAQlJ,OAAQ2J,GAAY,EAAG,CAC7DhL,KAAKY,cAEL,IAAK,IAAI6I,EAAW,EAAGA,EAAWc,EAAQS,GAAU3J,OAAQoI,GAAY,EAAG,CACvE,MAAMC,EAAQa,EAAQS,GAAUvB,IAG1BhJ,OAAEA,EAAFmL,WAAUA,cAA0BxH,QAAQsF,EAAMvJ,OAExDqJ,EAAQnD,KAAKrG,KAAKE,QAAQ6G,QAAQ2C,EAAMjJ,SACxCmJ,EAAYvD,KAAKrG,KAAKE,QAAQ6G,QAAQtG,IACtCoL,EAAYxF,KAAKuF,GAEbnL,IAAWiJ,EAAMjJ,QAAWsK,EAAe3E,SAAS4E,IACpDD,EAAe1E,KAAK2E,IAMhC,MAAMG,EAAkB5K,EAAQ,IAAMA,EAAQ4K,gBAC1C5K,EAAUiJ,GAAUjJ,EAAUqJ,GAAc5J,KAAKuD,aAG/C6H,EAAkB7K,EAAQ,IAAM4K,EACjCnG,IAAIzE,KAAU4K,EAAgB3I,QAC9B4E,IAAI,IAEH0E,EAAmB,CACrBf,eAAAA,EAEAM,SAAmB9K,EAAQ,IACvB6K,EAAgBhE,MAAMtC,IAAIqG,EAAgB/D,OAAOgD,aAGrDkB,OAAiB/K,EAAQ,IACrB6K,EAAgBtG,IAAIqG,EAAgB/D,IAAI,IAAIe,OAAOiC,aAGvDmB,UAAoBhL,EAAQ,IACxB6K,EAAgBtG,IAAIqG,EAAgB/D,IAAI,IAAIe,OAAOiC,aAGvD2B,kBAAmBF,EAAYjD,OAAO,CAAC4C,EAAGC,IAAMD,EAAIC,GAAKI,EAAYxK,QAMzE,OAHAd,EAAW,CAAC6K,EAAiBD,IAE7BnL,KAAKY,cACEkL,EAMXjL,KAAKmL,GACD,MAAMlL,EAAamL,KAAKC,MAAMF,GAE9BhM,KAAKoI,YAAYW,QAASJ,IACtBA,EAAW9H,KAAKC,EAAW6H,EAAWwD,OAG1CnM,KAAK8I,KAAKjI,KAAKC,EAAWgI,MAE1B9I,KAAKY,cAMTX,eACI,MAAMa,EAAa,CAAEgI,gBAAiBA,KAAKsD,UAE3C,IAAK,IAAIhD,EAAM,EAAGA,EAAMpJ,KAAKoI,YAAY/G,OAAQ+H,GAAO,EAAG,CACvD,MAAMT,EAAa3I,KAAKoI,YAAYgB,GAGpCtI,EAAW6H,EAAWwD,UAAYxD,EAAWyD,SAGjD,OAAOH,KAAKI,UAAUvL,YC9djBwL,UAAoBxM,EAsB7BC,YAAYwM,EAA4BC,GACpC7F,QACA3G,KAAKuM,iBAAmBA,EACxBvM,KAAKwM,oBAAsBA,EAG/B9L,gBACI,YAAYR,QAAQS,IAAKF,IACrB,MAAMgM,OAA2BnM,IAAfN,KAAK0M,MACjBC,EAAc3M,KAAKuM,iBAAiBnG,SAAS3F,GAC7CmM,EAAiB5M,KAAKwM,oBAAoBpG,SAAS3F,GAEzD,OAASgM,KAAeE,GAAeC,IAC/BH,IAAcG,IAI9BhM,cACIZ,KAAK0M,WAAQpM,EAMjBuM,WACI,YAAYH,MAMhBI,SAASJ,GACL1M,KAAK0M,MAAQA,8DCnBgBJ,EASjCvM,aAAYgN,KACRA,EADQC,WAERA,EAFQT,iBAGRA,EAAmB,GAHXC,oBAIRA,EAAsB,GAJdS,UAKRA,EAAY,MAEZtG,MAAM4F,EAAkBC,GAExBxM,KAAKkN,cAAgBC,OAAOC,KAAKJ,GACjChN,KAAKgN,WAAaA,EAClBhN,KAAKiN,UAAYA,EAEjBjN,KAAKmM,GAAQY,iBACb/M,KAAK4G,KAAO,EAAI5G,KAAKkN,cAAc7L,OAGvCpB,WAAWC,eACKwC,KAAKxC,GACjBF,KAAKY,cAGDyM,YAAYX,GAChB,MAAMQ,EAAgBC,OAAOC,KAAKpN,KAAKgN,YAEvC,OAAqBzM,EACjB2M,EAAcnG,QAAQ2F,EAAMY,UAC5BJ,EAAc7L,QAItBpB,kBAAkBE,GACd,MAAMoN,EAAgBvN,KAAK6M,WAC3B,IAAIW,EAA8B,CAAEF,cAAUhN,EAAW0B,aAAS1B,EAAW2B,MAAO,GAGpFkL,OAAOM,QAAQzN,KAAKgN,YAAYjE,QAAQ,EAAEuE,EAAUI,MAEhD,MAAMC,EAAQD,EACT/M,IAAKiN,GAAYhM,EAAWzB,EAAM+G,cAAe0G,EAAQ1G,gBAEzDc,OAAQ6F,GAAMA,EAAE5L,OAASjC,KAAKiN,WAE9BrE,OACG,CAACkF,EAAID,IAAOC,EAAG7L,MAAQ4L,EAAE5L,MAAQ6L,EAAKD,EACtC,CAAE7L,aAAS1B,EAAW2B,MAAO,IAIrC,QAAsB3B,IAAlBqN,EAAM3L,QAAuB,CAC7B,MAAM+L,EAAsBT,IAAaC,EAAcD,SACjDU,EAAiBR,EAAUF,WAAaC,EAAcD,SACtDW,EAAcN,EAAM1L,MAAQuL,EAAUvL,YAEjB3B,IAAvBkN,EAAUF,UACNS,GAAuBE,GACvBD,GAAkBD,GAClBC,GAAkBC,KAEtBT,EAAY,CAAEF,SAAAA,KAAaK,OAKvC,MAAMxE,EAAW5I,EAAQ,IACrBA,EAAU,CACNP,KAAKqN,YAAYG,GACjBxN,KAAKqN,YAAYrN,KAAK6M,eAQ9B,YAJ2BvM,IAAvBkN,EAAUF,UACVtN,KAAK8M,SAASU,GAGXrE,EAGX0D,WACI,YAAyBvM,IAArBqG,MAAMkG,WACC,CAAES,cAAUhN,EAAW0B,aAAS1B,EAAW2B,MAAO,SAGhD4K,sDC/HQqB,GACzB,MAAM3D,EAAmB,GACzB,IAAIlB,EAAe,GACf8E,GAAgB,EA4DpB,OA1DAD,EAAOnI,MAAM,MAAMgD,QAASqF,IACxB,MAAMC,EAAW,oBAAoBC,KAAKF,GACpCG,EAAW,cAAcD,KAAKF,GAC9BI,EAAY,eAAeF,KAAKF,GAMtB,MAAZC,GAAoBhF,EAAMhI,OAAS,GACnCgI,EAAMhD,KAAa,CAAElG,MAAO,GAAIM,OAAQ,QACxC8J,EAAQlE,KAAKgD,GACbA,EAAQ,IAMW,MAAZkF,GAEHlF,EAAMhI,OAAS,SACwBf,IAAnC+I,EAAMA,EAAMhI,OAAS,GAAGZ,OACxB4I,EAAMA,EAAMhI,OAAS,GAAGZ,OAAS,MAEjC4I,EAAMhD,KAAa,CAAElG,MAAO,GAAIM,OAAQ,SAIhD4I,EAAMhD,KAAa,CAAElG,MAAOoO,EAAS,GAAI9N,YAAQH,IACjD6N,GAAgB,GAQI,MAAbK,GAAqBL,EAC5B9E,EAAMhD,KAAa,CAAElG,MAAO,GAAIM,OAAQ+N,EAAU,KAO9B,MAAbA,GAAsBL,IAE7B9E,EAAMA,EAAMhI,OAAS,GAAGZ,OAAS+N,EAAU,GAC3CL,GAAgB,KAKpB9E,EAAMhI,OAAS,IACfgI,EAAMhD,KAAa,CAAElG,MAAO,GAAIM,OAAQ,QACxC8J,EAAQlE,KAAKgD,IAGVkB,2BC7DoBA,EAAkBkE,GAE7C,MAAMC,EAAgB,GAEtB,IAAK,IAAInN,EAAI,EAAGA,EAAIgJ,EAAQlJ,OAASoN,EAAUlN,GAAK,EAChDmN,EAAKrI,QAAQkE,EAAQoE,OACjBlN,KAAKmN,MAAMnN,KAAKoN,SAAWtE,EAAQlJ,QAAS,IAIpD,MAAO,CAAEyN,MAAOvE,EAASmE,KAAAA,uBC6BzB3O,aAAYgP,MACRA,EADQC,MAERA,EAAQ,GAFAtI,UAGRA,EAAY,QAEZ1G,KAAK+O,MAAQA,EACb/O,KAAKgP,MAAQA,EACbhP,KAAK0G,UAAYA,EAMrBzG,UAAUE,GACN,MAAM8O,EAAoB,CACtB9O,MAAAA,EACAD,QAAS,GACTgP,eAAgB,EAChBF,MAAO,IAGX,IAAIvO,OAAEA,EAAFmL,WAAUA,cAA0BmD,MAAM3K,QAAQjE,GAEtD,KAAOM,IAAWT,KAAK0G,WACnBuI,EAAO/O,QAAQmG,KAAK5F,GACpBwO,EAAOC,gBAAkBtD,IAGtBnL,OAAAA,EAAQmL,WAAAA,cAA0BmD,MAAM3K,QAAQ,KAOvD,OAJApE,KAAKgP,MAAMjG,QAASoG,IAChBF,EAAOD,MAAMG,EAAKhD,IAAMgD,EAAKtC,aAG1BoC,EAMXrO,cACIZ,KAAK+O,MAAMnO,mCCvDfb,aAAYqP,eACRA,EADQxI,KAERA,EAFQyI,aAGRA,EAAe,OAHPC,MAIRA,GAAQ,EAJAC,YAKRA,EAAc,GALNvJ,WAMRA,IAEA,IAAK,CAAC,OAAQ,aAAaI,SAASiJ,GAChC,UAAUtM,MAAM,kEAGpB/C,KAAKoP,eAAiBA,EAEtBpP,KAAK4G,KAAOA,EACZ5G,KAAKqP,aAAeA,EACpBrP,KAAKsP,MAAQA,EAEbtP,KAAKuP,YAAcA,EACnBvP,KAAKgG,WAAaA,EAMtBoH,OACI,OAAOD,OAAOC,KAAKpN,KAAK2H,SAM5B1H,aACID,KAAK2H,QAAUsE,KAAKC,iBAAiBkD,kBAEX,cAAtBpP,KAAKqP,eACLrP,KAAKwP,KAAO,IAAI7J,EAChB3F,KAAKoN,OAAOrE,QAAS3D,GAAQpF,KAAKwP,KAAK5K,IAAIQ,KAOnDwC,WACI,YAAwBtH,SAAZqH,QAUhBI,IAAI3C,GAEA,GAAIpF,KAAKoN,OAAOhH,SAAShB,GACrB,OAAO7E,EAAYP,KAAK2H,QAAQvC,IAIpC,IAAIqK,EACAC,EAAyBC,SAW7B,OATA3P,KAAKoN,OAAOrE,QAAS6G,IACjB,MAAMC,EAAW9O,EAAoB6O,EAAUxK,GAE3CyK,EAAWH,IACXD,EAAUG,EACVF,EAAiBG,KAIrBH,GAAkB1P,KAAKuP,YAChBhP,EAAUP,KAAK2H,QAAQ8H,SAIVnP,IAApBN,KAAKgG,WACEzF,EAAUP,KAAK2H,QAAQ3H,KAAKgG,kBADvC,EAaI8J,aAAajO,GACjB,OAAOA,EAAKkE,MAAM,OAAOiC,OAAQF,GAAUA,EAAMzG,OAAS,GAQtD0O,iBAAiBlO,GACrB,MAAMmO,OAAcnO,GAAOkE,MAAM,KAAKkK,KAAK,KAE3C,YAAYT,KAAKzJ,MAAMiK,EAAShQ,KAAKgG,WAAY,CAAC,MAQtD6B,SAAShG,GAML,OALI7B,KAAKsP,QAELzN,EAAOA,EAAKqF,eAGRlH,KAAKqP,cACb,IAAK,OACD,YAAYS,aAAajO,GAE7B,IAAK,YACD,YAAYkO,iBAAiBlO,GAEjC,QACI,UAAUkB,MAAM"}