{"version":3,"file":"index.js","sources":["../src/featurizers/featurizer.ts","../src/utils/levenshtein_distance.ts","../src/utils/fuzzy_match.ts","../src/utils/hashcode.ts","../src/utils/initialize_variable.ts","../src/utils/lstm.ts","../src/featurizers/action_featurizer.ts","../src/featurizers/bow.ts","../src/featurizers/use.ts","../src/featurizers/word_embedding.ts","../src/models/hcn.ts","../src/slots/slot.ts","../src/slots/categorical_slot.ts","../src/tools/nlu_formatter.ts","../src/tools/parse_stories.ts","../src/tools/train_test_split.ts","../src/tools/keyed_vectors.ts"],"sourcesContent":["import * as tf from '@tensorflow/tfjs';\n\ntype JSONSerializable = {[key: string]: any};\n\n/**\n * A stateful featurizer that turns queries into numerical representations.\n *\n * @abstract\n */\nexport class Featurizer {\n    /**\n     * An ID used by models for exportations.\n     */\n    readonly id: string;\n\n    /**\n     * The list of every action the model can take.\n     */\n    protected actions: any[];\n\n    /**\n     * The size of the vector returned by the featurizer.\n     * By default it's set to 1 which is the default for a featurizer that returns no features.\n     */\n    readonly size: number = 1;\n\n    /**\n     * Initialize the model, can be asynchronous async code.\n     *\n     * This method is executed by the model during it's initialization,\n     * it will also set the actions attribute.\n     */\n    async init(actions: any[]) {\n        this.actions = actions;\n    }\n\n    /**\n     * Featurizes and handle a text query.\n     *\n     * @remarks\n     * This method can directly return a 1D tensor to provide features to the model.\n     * Alternatively, it can returns data of any type if the Featurizer implement a custom\n     * getOptimizableFeatures method to handle those data.\n     * If this method doesn't return something, no features will be passed to the model.\n     */\n    // eslint-disable-next-line no-unused-vars, class-methods-use-this, no-empty-function\n    async handleQuery(query: string): Promise<any> {}\n\n    /**\n     * Turn the data returned by handleQuery into an embedding vector.\n     * This function is used to expose featurizer variables to the model optimizer for training.\n     *\n     * Reimplementing this method is not necessary if your featurizer is not meant to be optimizable\n     * through gradient descent.\n     * In this case, just return the feature vector directly using the handleQuery method.\n     *\n     * @remarks\n     * It's important to keep this function stateless, it should only depend of its tensor argument\n     * and of featurizer's variables.\n     */\n    // eslint-disable-next-line class-methods-use-this\n    getOptimizableFeatures(data: any): tf.Tensor1D {\n        if (data === undefined) {\n            return tf.zeros([1]);\n        }\n\n        return <tf.Tensor1D> data;\n    }\n\n    /**\n     * Let the featurizer know what action the model has taken.\n     */\n    // eslint-disable-next-line no-unused-vars, class-methods-use-this\n    handleAction(action: any): void {}\n\n    /**\n     * Produce an action mask according to featurizer state.\n     * (Generally, this method is reimplemented in stateful featurizers)\n     *\n     * @returns An array of boolean mapping every actions availability.\n     */\n    getActionMask(): boolean[] {\n        return this.actions.map(() => true);\n    }\n\n    /**\n     * Resets the state of the featurizer (if the stateful feature is used).\n     */\n    // eslint-disable-next-line class-methods-use-this\n    resetDialog(): void {}\n\n    /**\n     * Load parameters extracted from a JSON-like document.\n     */\n    // eslint-disable-next-line no-unused-vars, class-methods-use-this\n    load(parameters: JSONSerializable) {}\n\n    /**\n     * Export the featurizer's internal parameters to be serialized along the model.\n     */\n    // eslint-disable-next-line class-methods-use-this\n    async export(): Promise<JSONSerializable> {\n        return {};\n    }\n}\n","/**\n * Compute the Levenshtein distance between two strings using the\n * [Wagner-Fisher algorithm](https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm).\n */\nexport function levenshteinDistance(s1: string, s2: string): number {\n    const d = Array.from(\n        Array(s1.length + 1),\n        () => new Array(s2.length + 1).fill(0)\n    );\n\n    for (let i = 1; i <= s1.length; i += 1) {\n        d[i][0] = i;\n    }\n\n    for (let j = 1; j <= s2.length; j += 1) {\n        d[0][j] = j;\n    }\n\n    for (let j = 0; j < s2.length; j += 1) {\n        for (let i = 0; i < s1.length; i += 1) {\n            const substitutionCost = (s1[i] !== s2[j]) ? 1 : 0;\n\n            d[i + 1][j + 1] = Math.min(\n                d[i][j + 1] + 1,\n                d[i + 1][j] + 1,\n                d[i][j] + substitutionCost\n            );\n        }\n    }\n\n    return d[s1.length][s2.length] / Math.max(s1.length, s2.length, 1);\n}\n","import { levenshteinDistance } from './levenshtein_distance';\n\ntype Match = { extract: string, score: number };\n\nexport function fuzzyMatch(text: string, substring: string): Match {\n    let bestMatch: Match = { extract: undefined, score: 0 };\n\n    for (let i = 0; i < text.length; i += 1) {\n        const extract = text.substring(i, i + substring.length);\n\n        const aMatch = {\n            extract,\n            score: 1 - levenshteinDistance(extract, substring)\n        };\n\n        if (aMatch.score === 1) {\n            return aMatch;\n        }\n\n        if (aMatch.score > bestMatch.score) {\n            bestMatch = aMatch;\n        }\n    }\n\n    return bestMatch;\n}\n","/* eslint-disable no-bitwise */\n\n/**\n * Hash a string.\n * Based on https://stackoverflow.com/a/7616484\n */\nexport function hashcode(input: string) {\n    let hash = 0;\n\n    for (let i = 0; i < input.length; i += 1) {\n        const chr = input.charCodeAt(i);\n\n        hash = ((hash << 5) - hash) + chr;\n        hash |= 0; // Convert to 32bit integer\n    }\n\n    return hash;\n}\n","import * as tf from '@tensorflow/tfjs';\n\n/**\n * Initialize a variable of a given shape.\n */\nexport function initializeVariable(shape: number[], scalar: boolean = false,\n    init: 'he'|'zeros'|'normal' = 'he'): tf.Variable {\n    return tf.tidy(() => {\n        let initializer;\n\n        switch (init) {\n        case 'he':\n            initializer = tf.initializers.heNormal({});\n            break;\n\n        case 'zeros':\n            initializer = tf.initializers.zeros();\n            break;\n\n        case 'normal':\n            initializer = tf.initializers.randomNormal({});\n            break;\n\n        default:\n            throw new Error(\n                `Expected parameter init to take value 'he', 'zeros' or 'normal' not '${init}'.`\n            );\n        }\n\n        let randomTensor = initializer.apply(shape);\n        if (scalar) randomTensor = randomTensor.asScalar();\n\n        return randomTensor.variable();\n    });\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { initializeVariable } from './initialize_variable';\n\ntype LSTMPrediction = {y: tf.Tensor1D, nc: tf.Tensor2D, nh: tf.Tensor2D};\n\n/**\n * An LSTM cell with a dense layer on its top.\n */\nexport class LSTM {\n    // LSTM parameters :\n    private lstmKernel: tf.Tensor;\n    private lstmBias: tf.Tensor;\n    private lstmForgetBias: tf.Tensor;\n    private lstmInitH: tf.Tensor;\n    private lstmInitC: tf.Tensor;\n\n    // Dense layer parameters :\n    private denseWeights: tf.Tensor;\n    private denseBias: tf.Tensor;\n\n    // Let dropout be public to allow to change its value when training/inference.\n    public dropout: number;\n\n    /**\n     * @param inputSize The dimension of the input data.\n     * @param hiddenSize The dimension of the output of the LSTM, passed to the dense layer.\n     * @param outputSize The dimension of the output data.\n     * @param dropout The dropout rate between the LSTM cell and the dense layer.\n     */\n    constructor(inputSize: number, hiddenSize: number, outputSize: number, dropout: number = 0.2) {\n        this.lstmKernel = initializeVariable([inputSize + hiddenSize, hiddenSize * 4]);\n        this.lstmBias = initializeVariable([hiddenSize * 4], false, 'zeros');\n        this.lstmForgetBias = initializeVariable([1], true, 'zeros'); // (scalar)\n        this.lstmInitH = initializeVariable([1, hiddenSize]);\n        this.lstmInitC = initializeVariable([1, hiddenSize]);\n\n        this.denseWeights = initializeVariable([hiddenSize, outputSize]);\n        this.denseBias = initializeVariable([outputSize], false, 'zeros');\n\n        this.dropout = dropout;\n    }\n\n    /**\n     * Gives the initial state values of the LSTM (c and h).\n     *\n     * @param clone If it is necessary to clone states variable or no.\n     */\n    initLSTM(clone: boolean = true): {c: tf.Tensor2D, h: tf.Tensor2D} {\n        return {\n            c: <tf.Tensor2D> (clone ? this.lstmInitC.clone() : this.lstmInitC),\n            h: <tf.Tensor2D> (clone ? this.lstmInitH.clone() : this.lstmInitH)\n        };\n    }\n\n    /**\n     * Make a prediction given an input and state values (c and h).\n     * @param x A vector of shape [inputSize].\n     * @param c LSTM's state value.\n     * @param h LSTM's last output value.\n     * @param mask A vector of ones and zeros of shape [outputSize].\n     */\n    predict(x: tf.Tensor1D, c: tf.Tensor2D, h: tf.Tensor2D, mask?: tf.Tensor1D,\n        temperature: number = 1): LSTMPrediction {\n        return tf.tidy(() => {\n            // Execute the LSTM cell.\n            const [nc, nh] = tf.basicLSTMCell(\n                <tf.Scalar> this.lstmForgetBias,\n                <tf.Tensor2D> this.lstmKernel,\n                <tf.Tensor1D> this.lstmBias,\n                <tf.Tensor2D> tf.stack([x]),\n                h, c\n            );\n\n            // Execute the dense layer on top of the LSTM cell.\n            let y = <tf.Tensor1D> tf\n                .dropout(nh, this.dropout)\n                .matMul(this.denseWeights)\n                .add(this.denseBias)\n                .squeeze()\n                .div(temperature)\n                .softmax()\n                .mul(mask ?? 1);\n\n            // Apply normalization after the mask to get probabilities.\n            y = y.div(tf.sum(y));\n\n            return { y, nc, nh };\n        });\n    }\n\n    /**\n     * Update the given model parameters.\n     */\n    load(weights: {[key: string]: any}) {\n        tf.tidy(() => {\n            // Convert every parameter to a tf variable tensor.\n            this.lstmKernel = tf.tensor(weights.lstmKernel).variable();\n            this.lstmBias = tf.tensor(weights.lstmBias).variable();\n            this.lstmForgetBias = tf.tensor(weights.lstmForgetBias).variable();\n            this.lstmInitH = tf.tensor(weights.lstmInitH).variable();\n            this.lstmInitC = tf.tensor(weights.lstmInitC).variable();\n            this.denseWeights = tf.tensor(weights.denseWeights).variable();\n            this.denseBias = tf.tensor(weights.denseBias).variable();\n        });\n    }\n\n    /**\n     * Return all the LSTM model parameters.\n     */\n    async export(): Promise<{[key: string]: any}> {\n        const exports = {\n            lstmKernel: await this.lstmKernel.array(),\n            lstmBias: await this.lstmBias.array(),\n            lstmForgetBias: await this.lstmForgetBias.array(),\n            lstmInitH: await this.lstmInitH.array(),\n            lstmInitC: await this.lstmInitC.array(),\n            denseWeights: await this.denseWeights.array(),\n            denseBias: await this.denseBias.array()\n        };\n\n        return exports;\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Featurizer } from './featurizer';\nimport { initializeVariable } from '../utils';\n\n/**\n * Parameters for ActionFeaturizer constructor.\n */\ninterface ActionFeaturizerArgs {\n    /**\n     * Enable the masking of LUS when the user has just talked.\n     * Enabled by default.\n     */\n    maskLUS?: boolean;\n\n    /**\n     * Enable the masking of the previous action.\n     * Enabled by default.\n     */\n    maskPreviousAction?: boolean;\n\n    /**\n     * The action the bot takes to let the user talk.\n     * Default to 'LUS' (acronym for Let User Speak).\n     */\n    LUSAction?: string;\n}\n\n/**\n * Rule-based featurizer improving model robustness.\n *\n * - Featurize the previous action the model has taken.\n * - Mask the LUS action when the user has just talked.\n *   (Force the model to reply at least once)\n * - Mask the previous action.\n *   (Prevent looping : the model can't take two times in a row the same action)\n */\nexport class ActionFeaturizer extends Featurizer {\n    readonly id = 'Action Featurizer';\n    size: number;\n\n    private LUSAction: any;\n    private maskLUS: boolean;\n    private maskPreviousAction: boolean;\n\n    private userTalked: boolean;\n    private previousAction: any;\n\n    private embeddings: tf.Tensor;\n\n    constructor({\n        maskLUS = true,\n        maskPreviousAction = true,\n        LUSAction = 'LUS'\n    }: ActionFeaturizerArgs = { maskLUS: true, maskPreviousAction: true, LUSAction: 'LUS' }) {\n        super();\n        this.maskLUS = maskLUS;\n        this.maskPreviousAction = maskPreviousAction;\n        this.LUSAction = LUSAction;\n\n        this.resetDialog();\n    }\n\n    async init(actions: any[]) {\n        await super.init(actions);\n\n        this.size = actions.length;\n        this.embeddings = initializeVariable([this.size, this.size]);\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor2D> {\n        return tf.tidy(() => {\n            this.userTalked = query !== '';\n\n            // One-hot encode the previous action.\n            return <tf.Tensor2D> tf.oneHot(\n                [this.actions.indexOf(this.previousAction)],\n                this.actions.length\n            );\n        });\n    }\n\n    getOptimizableFeatures(data: tf.Tensor2D): tf.Tensor1D {\n        return <tf.Tensor1D> data.matMul(this.embeddings).squeeze();\n    }\n\n    handleAction(action: any) {\n        // Store the new action if it's not the LUS action.\n        this.previousAction = action !== this.LUSAction ? action : this.previousAction;\n    }\n\n    getActionMask(): boolean[] {\n        const mask = super.getActionMask();\n\n        // Mask LUS when the user talk and the option is enabled.\n        if (this.maskLUS && this.userTalked) {\n            mask[this.actions.indexOf(this.LUSAction)] = false;\n        }\n\n        // Mask the previous action when the option is enabled and if applicable.\n        if (this.maskPreviousAction && this.actions.includes(this.previousAction)) {\n            mask[this.actions.indexOf(this.previousAction)] = false;\n        }\n\n        return mask;\n    }\n\n    resetDialog() {\n        this.userTalked = false;\n        this.previousAction = undefined;\n    }\n\n    load(parameters: {embeddings: number[][]}) {\n        this.embeddings = tf.tidy(() => tf.tensor(parameters.embeddings).variable());\n    }\n\n    async export(): Promise<{embeddings: number[][]}> {\n        return {\n            embeddings: <number[][]> await this.embeddings.array()\n        };\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Featurizer } from './featurizer';\nimport { hashcode } from '../utils';\n\n/**\n * Featurizes queries as bag of words.\n *\n * The algorithm uses the [hashing trick](https://en.wikipedia.org/wiki/Feature_hashing) to avoid\n * having to store a vocabulary in the memory.\n */\nexport class BOW extends Featurizer {\n    readonly id = 'Bag-of-Words';\n    readonly size: number;\n\n    /**\n     * @param size The vocabulary size you allow to the featurizer.\n     */\n    constructor(size: number) {\n        super();\n        this.size = size;\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor1D> {\n        return tf.tidy(() => {\n            const indexes = query.toLowerCase()\n                .split(/\\W/g)\n                .map((word) => hashcode(word) % this.size);\n\n            return <tf.Tensor1D> tf.oneHot(indexes, this.size).asType('float32').sum(0);\n        });\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport * as use from '@tensorflow-models/universal-sentence-encoder';\nimport { Featurizer } from './featurizer';\n\n/**\n * Featurizes queries using the Universal Sentence Encoder model.\n */\nexport class USE extends Featurizer {\n    readonly id = 'Universal Sentence Encoder';\n\n    private encoder: use.UniversalSentenceEncoder;\n    private emptyEncoding: tf.Tensor1D;\n\n    readonly size = 512;\n\n    async init(actions: any[]) {\n        await super.init(actions);\n        this.encoder = await use.load();\n\n        // Cache the empty string embed (for optimization purpose).\n        this.emptyEncoding = await this.encodeQuery('');\n    }\n\n    /**\n     * Encodes a query using the model.\n     */\n    private async encodeQuery(query: string): Promise<tf.Tensor1D> {\n        const embed = await this.encoder.embed([query]);\n        const squeezedEmbed = <tf.Tensor1D> embed.squeeze();\n        tf.dispose(embed);\n\n        return squeezedEmbed;\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor1D> {\n        // When the query is empty, return the cached empty query encoding.\n        if (!query) {\n            return this.emptyEncoding.clone();\n        }\n\n        return this.encodeQuery(query);\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Featurizer } from './featurizer';\nimport { KeyedVectors } from '../tools';\n\n/**\n * Featurize queries by pooling words embedding using SWEM-concat(*).\n *\n * (*): Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, Qinliang Su, Yizhe Zhang,\n *      Chunyuan Li, Ricardo Henao, Lawrence Carin- 2018.\n *      Baseline Needs More Love: On Simple Word-Embedding-Based Models and\n *      Associated Pooling Mechanisms.\n */\nexport class WordEmbedding extends Featurizer {\n    readonly id = 'Word Embedding';\n    readonly size: number;\n\n    private vectors: KeyedVectors;\n\n    /**\n     * @param vectors The keyed vectors storing the embeddings.\n     */\n    constructor(vectors: KeyedVectors) {\n        super();\n        this.size = 2 * vectors.size;\n        this.vectors = vectors;\n    }\n\n    async init(actions: any[]) {\n        await super.init(actions);\n\n        if (!this.vectors.isLoaded()) await this.vectors.load();\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor1D> {\n        return <tf.Tensor1D> tf.tidy(() => {\n            const tokens = this.vectors.tokenize(query);\n            const embeddings = tokens\n                .map((token) => this.vectors.get(token))\n                .filter((v) => v !== undefined);\n\n            // When there is no embeddable tokens, return a zeros vector.\n            if (embeddings.length === 0) {\n                return tf.zeros([this.size]);\n            }\n\n            const embeddingsMatrix = tf.stack(embeddings);\n\n            return tf.concat([embeddingsMatrix.mean(0), embeddingsMatrix.max(0)]);\n        });\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Featurizer } from '../featurizers';\nimport { LSTM, Story, Metrics } from '../utils';\n\ninterface SampleData {\n    targets: tf.Tensor1D,\n    predictions: tf.Tensor1D,\n    loss: number,\n    isFailing: boolean\n}\n\n/**\n * @callback\n */\ntype TrainingCallback = (metrics: Metrics) => void;\n\n/**\n * Parameters for HCN constructor.\n */\ninterface HCNConstructorArgs {\n    /**\n     * The list of actions the model can take.\n     * (keeping the order the same is important for pretrained models)\n     */\n    actions: string[];\n\n    /**\n     * The list of featurizers the model uses.\n     * (keeping the order the same is important for pretrained models)\n     */\n    featurizers: Featurizer[];\n\n    /**\n     * The output size of the LSTM cell.\n     * Default is set to 32 units.\n     */\n    hiddenSize?: number;\n\n    /**\n     * The optimization algorithm used for training.\n     * By default, Adam with a learning rate of 0.01 is used.\n     */\n    optimizer?: tf.Optimizer;\n\n    /**\n     * Temperature of the model softmax, used to calibrate confidence estimation.\n     * By default, the temperature is 1 but you usually want it higher to make less overconfident.\n     */\n    temperature?: number;\n\n    /**\n     * The percentage of units to dropout between the LSTM cell layer and the dense.\n     * Useful for regularizing the model. It's disabled by default (value = 0).\n     */\n    dropout?: number;\n}\n\n/**\n * Parameters for HCN train method.\n */\ninterface HCNTrainArgs {\n    /**\n     * Training stories to learn from.\n     */\n    stories: Story[];\n\n    /**\n     * Number of times the model will be passed the whole set of training stories during training.\n     * Default is set to 12 epochs.\n     */\n    nEpochs?: number;\n\n    /**\n     * After each epoch, this callback function will be executed with the metrics collected\n     * during the epoch.\n     */\n    onEpochEnd?: TrainingCallback;\n}\n\n/**\n * An implementation of Hybrid Code Networks(*) dialog manager.\n *\n * (*): Williams, Asadi, Zweig - 2017.\n *      Hybrid Code Networks: practical and efﬁcient end-to-end dialog control with supervised\n *      and reinforcement learning.\n */\nexport class HCN {\n    private actions: string[];\n    private featurizers: Featurizer[];\n    private optimizer: tf.Optimizer;\n\n    private inputSize: number;\n    private hiddenSize: number;\n    private outputSize: number;\n\n    private lstm: LSTM;\n    private lstmH: tf.Tensor2D;\n    private lstmC: tf.Tensor2D;\n    private lstmTemperature: number;\n    private lstmDropout: number;\n\n    /**\n     * Defines the model.\n     *\n     * To fully initialize the model, run the async *init()* method.\n     */\n    constructor({\n        actions,\n        featurizers,\n        hiddenSize = 32,\n        optimizer = tf.train.adam(0.01),\n        temperature = 1,\n        dropout = 0\n    }: HCNConstructorArgs) {\n        this.actions = actions;\n        this.featurizers = featurizers;\n        this.optimizer = optimizer;\n\n        this.hiddenSize = hiddenSize;\n        this.outputSize = actions.length;\n\n        this.lstmTemperature = temperature;\n        this.lstmDropout = dropout;\n    }\n\n    /**\n     * Initialize the model and its featurizers.\n     */\n    async init() {\n        // Initialize asynchronously all featurizers.\n        await Promise.all(\n            this.featurizers.map((featurizer) => featurizer.init(this.actions))\n        );\n\n        // The model input size is the sum of the sizes of features vectors.\n        this.inputSize = this.featurizers\n            .map((featurizer) => featurizer.size)\n            .reduce((acc, size) => acc + size, 1);\n\n        this.lstm = new LSTM(this.inputSize, this.hiddenSize, this.outputSize, this.lstmDropout);\n\n        this.resetDialog();\n    }\n\n    /**\n     * Resets the state of the model and its featurizers.\n     */\n    resetDialog() {\n        this.featurizers.forEach((featurizer) => featurizer.resetDialog());\n        ({ c: this.lstmC, h: this.lstmH } = this.lstm.initLSTM());\n    }\n\n    /**\n     * Get the data returned from every featurizer's handleQuery method.\n     */\n    private async handleQuery(query: string): Promise<any[]> {\n        return Promise.all(\n            this.featurizers.map((featurizer) => featurizer.handleQuery(query))\n        );\n    }\n\n    /**\n     * Get the embedding vector resulted from every featurizers.\n     */\n    private getOptimizableFeatures(features: tf.Tensor[]): tf.Tensor1D {\n        return tf.tidy(() => {\n            const embeddings = this.featurizers.map(\n                (featurizer, idx) => featurizer.getOptimizableFeatures(features[idx])\n            );\n\n            // Add a zero to make tf.concat work consistently even with only one featurizer.\n            embeddings.push(tf.zeros([1]));\n\n            return <tf.Tensor1D> tf.concat(embeddings);\n        });\n    }\n\n    /**\n     * Inform every featurizers of the taken action.\n     */\n    private handleAction(action: string) {\n        this.featurizers.map((featurizer) => featurizer.handleAction(action));\n    }\n\n    /**\n     * Get the final action mask resulted from every featurizers.\n     */\n    private getActionMask(): tf.Tensor1D {\n        return tf.tidy(() => this.featurizers\n            // Get action mask and convert them to tensors.\n            .map((featurizer) => <tf.Tensor1D> tf.tensor(\n                featurizer.getActionMask(),\n                undefined, 'float32'\n            ))\n            // Compute the product of every masks.\n            .reduce((acc, mask) => tf.mul(acc, mask), tf.ones([this.actions.length])));\n    }\n\n    /**\n     * Trains the model on a single training story.\n     */\n    private async fitStory(story: Story): Promise<SampleData> {\n        this.resetDialog();\n\n        // 1. Prepare the input data.\n        const inputs: any[][] = [];\n        const masks: tf.Tensor1D[] = [];\n        const targets: tf.Tensor1D[] = [];\n\n        // For each story's state...\n        for (let stateIdx = 0; stateIdx < story.length; stateIdx += 1) {\n            const state = story[stateIdx];\n\n            // The query must be featurized before moving to the next state.\n            // eslint-disable-next-line no-await-in-loop\n            inputs.push(await this.handleQuery(state.query));\n\n            masks.push(this.getActionMask());\n\n            targets.push(\n                <tf.Tensor1D> tf.oneHot(\n                    this.actions.indexOf(state.action),\n                    this.outputSize\n                )\n            );\n\n            this.handleAction(state.action);\n        }\n\n        // 2. Fit the sequence.\n        let data: SampleData;\n\n        this.optimizer.minimize(() => {\n            let { c, h } = this.lstm.initLSTM(false);\n\n            // Make a prediction for each step of the input sequence.\n            const predictions = inputs.map((features, idx) => {\n                const statePred = this.lstm.predict(\n                    <tf.Tensor1D> this.getOptimizableFeatures(features),\n                    <tf.Tensor2D> c,\n                    <tf.Tensor2D> h,\n                    <tf.Tensor1D> masks[idx]\n                );\n\n                c = statePred.nc;\n                h = statePred.nh;\n\n                return statePred.y;\n            });\n\n            const targetsMatrix = tf.stack(targets);\n            const predictionsMatrix = tf.stack(predictions);\n\n            // Compare the predicted sequence with the target.\n            const lossScalar = <tf.Scalar> tf.metrics.categoricalCrossentropy(\n                targetsMatrix, predictionsMatrix\n            ).mean();\n\n            // Store the necessary data to build metrics.\n            data = {\n                targets: tf.keep(targetsMatrix.argMax(1)),\n                predictions: tf.keep(predictionsMatrix.argMax(1)),\n                loss: <number> lossScalar.arraySync(),\n                isFailing: tf.metrics\n                    .categoricalAccuracy(targetsMatrix, predictionsMatrix)\n                    .mean()\n                    .arraySync() < 0.999\n            };\n\n            // Return the loss to the optimizer to update the model.\n            return lossScalar;\n        });\n\n        // BUG: two tensors leak in the memory at each loop :/\n        tf.dispose([inputs, targets]);\n\n        return data;\n    }\n\n    /**\n     * Trains the model using the training stories.\n     *\n     * @returns Metrics collected from the last epoch (that correspond to the trained model).\n     */\n    async train({ stories, nEpochs = 12, onEpochEnd = undefined }: HCNTrainArgs): Promise<Metrics> {\n        let epochMetrics: Metrics;\n\n        // For each epoch...\n        for (let epoch = 0; epoch < nEpochs; epoch += 1) {\n            const allTargets: tf.Tensor1D[] = [];\n            const allPredictions: tf.Tensor1D[] = [];\n            const allLosses: number[] = [];\n            const failingSamples: number[] = [];\n\n            // For each training story...\n            for (let storyIdx = 0; storyIdx < stories.length; storyIdx += 1) {\n                // (Each story must be fitted sequentially)\n                // eslint-disable-next-line no-await-in-loop\n                const storyData = await this.fitStory(stories[storyIdx]);\n\n                allTargets.push(storyData.targets);\n                allPredictions.push(storyData.predictions);\n                allLosses.push(storyData.loss);\n\n                if (storyData.isFailing) {\n                    failingSamples.push(storyIdx);\n                }\n            }\n\n            // Build the metrics.\n            const confusionMatrix = tf.tidy(() => tf.math.confusionMatrix(\n                tf.concat(allTargets),\n                tf.concat(allPredictions),\n                this.outputSize\n            ));\n\n            const truePredictions = tf.tidy(() => confusionMatrix\n                .mul(tf.eye(...confusionMatrix.shape))\n                .sum(0));\n\n            epochMetrics = {\n                epoch,\n                failingSamples,\n\n                accuracy: <number> tf.tidy(() => (\n                    truePredictions.sum().div(confusionMatrix.sum()).arraySync()\n                )),\n\n                recall: <number> tf.tidy(() => (\n                    truePredictions.div(confusionMatrix.sum(1)).mean().arraySync()\n                )),\n\n                precision: <number> tf.tidy(() => (\n                    truePredictions.div(confusionMatrix.sum(0)).mean().arraySync()\n                )),\n\n                loss: allLosses.reduce((a, b) => a + b) / allLosses.length\n            };\n\n            // Clear the tensors.\n            tf.dispose(allTargets);\n            tf.dispose(allPredictions);\n            tf.dispose([truePredictions, confusionMatrix]);\n\n            if (onEpochEnd !== undefined) {\n                onEpochEnd(epochMetrics);\n            }\n        }\n\n        this.resetDialog();\n        return epochMetrics;\n    }\n\n    /**\n     * Predict an action resulting from the given query.\n     *\n     * @param query The given query from the user.\n     * @returns The predicted action from the model and its confidence.\n     */\n    async predict(query: string):\n        Promise<{action: string, confidence: number}> {\n        // At inference, dropout is disabled\n        this.lstm.dropout = 0;\n\n        const features = this.getOptimizableFeatures(await this.handleQuery(query));\n        const masks = this.getActionMask();\n\n        const prediction = this.lstm.predict(\n            features, this.lstmC, this.lstmH, masks, this.lstmTemperature\n        );\n\n        // Update lstm internal state\n        tf.dispose([this.lstmC, this.lstmH]);\n        this.lstmC = prediction.nc.clone();\n        this.lstmH = prediction.nh.clone();\n\n        const actionIdx = <number> tf.tidy(() => prediction.y.argMax().arraySync());\n        const confidence = <number> tf.tidy(() => prediction.y.arraySync()[actionIdx]);\n\n        // Clear the memory\n        tf.dispose([features, masks]);\n        tf.dispose(prediction);\n\n        // Retablish dropout (just in case)\n        this.lstm.dropout = this.lstmDropout;\n\n        this.handleAction(this.actions[actionIdx]);\n\n        return { action: this.actions[actionIdx], confidence };\n    }\n\n    /**\n     * Evaluate the model using stories.\n     *\n     * @param stories Validation stories to evaluate the model.\n     * @returns Validation metrics based on the results from the stories.\n     */\n    async score(stories: Story[]): Promise<Metrics> {\n        const targets: number[] = [];\n        const predictions: number[] = [];\n        const confidences: number[] = [];\n        const failingSamples: number[] = [];\n\n        // For each stories and states, make predictions.\n        for (let storyIdx = 0; storyIdx < stories.length; storyIdx += 1) {\n            this.resetDialog();\n\n            for (let stateIdx = 0; stateIdx < stories[storyIdx].length; stateIdx += 1) {\n                const state = stories[storyIdx][stateIdx];\n\n                // eslint-disable-next-line no-await-in-loop\n                const { action, confidence } = await this.predict(state.query);\n\n                targets.push(this.actions.indexOf(state.action));\n                predictions.push(this.actions.indexOf(action));\n                confidences.push(confidence);\n\n                if (action !== state.action && !failingSamples.includes(storyIdx)) {\n                    failingSamples.push(storyIdx);\n                }\n            }\n        }\n\n        // Build a confusion matrix out of the prediction and build the metrics.\n        const confusionMatrix = tf.tidy(() => tf.math.confusionMatrix(\n            tf.tensor(targets), tf.tensor(predictions), this.outputSize\n        ));\n\n        const truePredictions = tf.tidy(() => confusionMatrix\n            .mul(tf.eye(...confusionMatrix.shape))\n            .sum(0));\n\n        const metrics: Metrics = {\n            failingSamples,\n\n            accuracy: <number> tf.tidy(() => (\n                truePredictions.sum().div(confusionMatrix.sum()).arraySync()\n            )),\n\n            recall: <number> tf.tidy(() => (\n                truePredictions.div(confusionMatrix.sum(1)).mean().arraySync()\n            )),\n\n            precision: <number> tf.tidy(() => (\n                truePredictions.div(confusionMatrix.sum(0)).mean().arraySync()\n            )),\n\n            averageConfidence: confidences.reduce((a, b) => a + b) / confidences.length\n        };\n\n        tf.dispose([truePredictions, confusionMatrix]);\n\n        this.resetDialog();\n        return metrics;\n    }\n\n    /**\n     * Load the models parameters from a JSON formatted string.\n     */\n    load(json: string) {\n        const parameters = JSON.parse(json);\n\n        this.featurizers.forEach((featurizer) => {\n            featurizer.load(parameters[featurizer.id]);\n        });\n\n        this.lstm.load(parameters.lstm);\n\n        this.resetDialog();\n    }\n\n    /**\n     * Export the models parameters in a JSON format.\n     */\n    async export(): Promise<string> {\n        const parameters = { lstm: await this.lstm.export() };\n\n        for (let idx = 0; idx < this.featurizers.length; idx += 1) {\n            const featurizer = this.featurizers[idx];\n\n            // eslint-disable-next-line no-await-in-loop\n            parameters[featurizer.id] = await featurizer.export();\n        }\n\n        return JSON.stringify(parameters);\n    }\n}\n","import { Featurizer } from '../featurizers';\n\n/**\n * An extension of featurizer that holds a value in its state.\n * @abstract\n */\nexport class Slot<Value> extends Featurizer {\n    /**\n     * The list of actions that can be taken by the model only when the slot is defined.\n     */\n    private dependantActions: string[];\n\n    /**\n     * The list of actions that can be taken by the model only when the slot is undefined.\n     */\n    private invDependantActions: string[];\n\n    /**\n     * Stores the value of the slot.\n     */\n    private value: Value;\n\n    /**\n     * @param dependantActions The list of actions that can be taken by the model\n     *                         only when the slot is defined.\n     * @param invDependantActions The list of actions that can be taken by the model\n     *                            only when the slot is undefined.\n     */\n    constructor(dependantActions: string[], invDependantActions: string[]) {\n        super();\n        this.dependantActions = dependantActions;\n        this.invDependantActions = invDependantActions;\n    }\n\n    getActionMask(): boolean[] {\n        return this.actions.map((action) => {\n            const isDefined = this.value !== undefined;\n            const isDependant = this.dependantActions.includes(action);\n            const isInvDependant = this.invDependantActions.includes(action);\n\n            return (!isDefined && (!isDependant || isInvDependant))\n                || (isDefined && !isInvDependant);\n        });\n    }\n\n    resetDialog(): void {\n        this.value = undefined;\n    }\n\n    /**\n     * Retrieves the value of the slot.\n     */\n    getValue(): Value {\n        return this.value;\n    }\n\n    /**\n     * Redefine a new value for the slot.\n     */\n    setValue(value: Value) {\n        this.value = value;\n    }\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { Slot } from './slot';\nimport { fuzzyMatch } from '../utils';\n\ntype Categories = {[category: string]: string[]};\ntype CategoricalValue = { category: string, extract: string, score: number };\n\n/**\n * Parameters for Categorical Slot constructor.\n */\ninterface CategoricalSlotArgs {\n    /**\n     * The name of the slot.\n     */\n    name: string;\n\n    /**\n     * An object with the name of the category as a key and an array of synonyms that belong to\n     * the category as a value.\n     */\n    categories: Categories;\n\n    /**\n     * The list of actions that can be taken by the model only when the slot is defined.\n     */\n    dependantActions?: string[];\n\n    /**\n     * The list of actions that can be taken by the model only when the slot is undefined.\n     */\n    invDependantActions?: string[];\n\n    /**\n     * The minimum similarity to get selected as a value. (based on Leveinshtein Distance)\n     */\n    threshold?: number;\n}\n\n/**\n * A slot that stores a categorical value extracted using fuzzy string matching.\n */\nexport class CategoricalSlot extends Slot<CategoricalValue> {\n    readonly id: string;\n    readonly size: number;\n\n    private categoryNames: string[];\n    private categories: Categories;\n\n    private threshold: number;\n\n    constructor({\n        name,\n        categories,\n        dependantActions = [],\n        invDependantActions = [],\n        threshold = 0.75\n    }: CategoricalSlotArgs) {\n        super(dependantActions, invDependantActions);\n\n        this.categoryNames = Object.keys(categories);\n        this.categories = categories;\n        this.threshold = threshold;\n\n        this.id = `${name}#Categorical`;\n        this.size = 2 * this.categoryNames.length;\n    }\n\n    async init(actions: any[]) {\n        await super.init(actions);\n        this.resetDialog();\n    }\n\n    private oneHotValue(value: CategoricalValue): tf.Tensor1D {\n        const categoryNames = Object.keys(this.categories);\n\n        return <tf.Tensor1D> tf.oneHot(\n            categoryNames.indexOf(value.category),\n            categoryNames.length\n        );\n    }\n\n    async handleQuery(query: string): Promise<tf.Tensor1D> {\n        const previousValue = this.getValue();\n        let bestValue: CategoricalValue = { category: undefined, extract: undefined, score: 0 };\n\n        // For each category...\n        Object.entries(this.categories).forEach(([category, keywords]) => {\n            // Find the best match of the category.\n            const match = keywords\n                .map((keyword) => fuzzyMatch(query.toLowerCase(), keyword.toLowerCase()))\n\n                .filter((m) => m.score >= this.threshold)\n\n                .reduce(\n                    (hm, m) => (hm.score > m.score ? hm : m),\n                    { extract: undefined, score: 0 }\n                );\n\n            // The best match is preferably a match of a different category with the highest score.\n            if (match.extract !== undefined) {\n                const currentUneqPrevious = category !== previousValue.category;\n                const bestEqPrevious = bestValue.category === previousValue.category;\n                const betterScore = match.score > bestValue.score;\n\n                if (bestValue.category === undefined\n                    || (currentUneqPrevious && betterScore)\n                    || (bestEqPrevious && currentUneqPrevious)\n                    || (bestEqPrevious && betterScore)\n                ) {\n                    bestValue = { category, ...match };\n                }\n            }\n        });\n\n        const features = tf.tidy(() => (\n            tf.concat([\n                this.oneHotValue(bestValue),\n                this.oneHotValue(this.getValue())\n            ])\n        ));\n\n        if (bestValue.category !== undefined) {\n            this.setValue(bestValue);\n        }\n\n        return features;\n    }\n\n    getValue(): CategoricalValue {\n        if (super.getValue() === undefined) {\n            return { category: undefined, extract: undefined, score: 0 };\n        }\n\n        return super.getValue();\n    }\n}\n","import { HCN } from '../models';\nimport { Slot } from '../slots';\n\n/**\n * Parameters for NLUFormatter.\n */\ninterface NLUFormatterArgs {\n    model: HCN;\n    slots?: Slot<any>[];\n    LUSAction?: string;\n}\n\n/**\n * An NLU digests containing the input query, the list of bot action to take,\n * the overall confidence of the turn (product of action's confidences) and slots values.\n */\ninterface NLUDigest {\n    /**\n     * The raw input query.\n     */\n    query: string;\n\n    /**\n     * An array of actions names.\n     */\n    actions: string[];\n\n    /**\n     * The overall confidence of the turn.\n     */\n    turnConfidence: number;\n\n    /**\n     * The value of each slot.\n     */\n    slots: {[slot: string]: any};\n}\n\n/**\n * An utility class using HCN methods and Slots to offer an higher level API\n * looking like NLU librairies.\n */\nexport class NLUFormatter {\n    private model: HCN;\n    private slots: Slot<any>[];\n    private LUSAction: string;\n\n    constructor({\n        model,\n        slots = [],\n        LUSAction = 'LUS'\n    }: NLUFormatterArgs) {\n        this.model = model;\n        this.slots = slots;\n        this.LUSAction = LUSAction;\n    }\n\n    /**\n     * Turn a query into a NLU digest.\n     */\n    async ask(query: string): Promise<NLUDigest> {\n        const digest: NLUDigest = {\n            query,\n            actions: [],\n            turnConfidence: 1,\n            slots: {}\n        };\n\n        let { action, confidence } = await this.model.predict(query);\n\n        while (action !== this.LUSAction) {\n            digest.actions.push(action);\n            digest.turnConfidence *= confidence;\n\n            // eslint-disable-next-line no-await-in-loop\n            ({ action, confidence } = await this.model.predict(''));\n        }\n\n        this.slots.forEach((slot) => {\n            digest.slots[slot.id] = slot.getValue();\n        });\n\n        return digest;\n    }\n\n    /**\n     * Reset the model state.\n     */\n    resetDialog() {\n        this.model.resetDialog();\n    }\n}\n","import { Story, State } from '../utils';\n\n/**\n * Parse a source string formatted according the Wisty Training Story syntax.\n * Usually, this source string is extracted using fetch or from a file.\n */\nexport function parseStories(source: string): Story[] {\n    const stories: Story[] = [];\n    let story: Story = [];\n    let inputAnswered = true;\n\n    source.split('\\n').forEach((line) => {\n        const newStory = /^## *([^#].*)?$/gm.exec(line);\n        const newInput = /^> *(.*)$/gm.exec(line);\n        const newAction = /^- *(\\w*)$/gm.exec(line);\n\n        /*\n            New story\n            ## Story name\n        */\n        if (newStory != null && story.length > 0) {\n            story.push(<State> { query: '', action: 'LUS' });\n            stories.push(story); // Push previous story.\n            story = [];\n\n        /*\n            New user input\n            > user input\n        */\n        } else if (newInput != null) {\n            // Add a LUS action to mark the end of the previous turn.\n            if (story.length > 0) {\n                if (story[story.length - 1].action === undefined) {\n                    story[story.length - 1].action = 'LUS';\n                } else {\n                    story.push(<State> { query: '', action: 'LUS' });\n                }\n            }\n\n            story.push(<State> { query: newInput[1], action: undefined });\n            inputAnswered = false;\n\n        /*\n            New bot action\n            (> user input)\n            (- action_name)\n            - action_name\n        */\n        } else if (newAction != null && inputAnswered) {\n            story.push(<State> { query: '', action: newAction[1] });\n\n        /*\n            New bot action (first answer)\n            (> user input)\n            - action_name\n        */\n        } else if (newAction != null && !inputAnswered) {\n            // eslint-disable-next-line prefer-destructuring\n            story[story.length - 1].action = newAction[1];\n            inputAnswered = true;\n        }\n    });\n\n    // Finalize the last story\n    if (story.length > 0) {\n        story.push(<State> { query: '', action: 'LUS' });\n        stories.push(story);\n    }\n\n    return stories;\n}\n","import { Story } from '../utils';\n\n/**\n * Split a list of stories into random train and test subsets.\n *\n * @param stories A list of stories.\n * @param testSize The proportion of stories to put in the test subset.\n */\nexport function trainTestSplit(stories: Story[], testSize: number):\n    {train: Story[], test: Story[]} {\n    const test: Story[] = [];\n\n    for (let i = 0; i / stories.length < testSize; i += 1) {\n        test.push(...stories.splice(\n            Math.floor(Math.random() * stories.length), 1\n        ));\n    }\n\n    return { train: stories, test };\n}\n","import * as tf from '@tensorflow/tfjs';\nimport { levenshteinDistance } from '../utils';\n\n/**\n * Parameters for KeyedVectors.\n */\ninterface KeyedVectorsArgs {\n    loaderFunction(): Promise<string>;\n    size: number;\n    tokenization?: 'word' | 'byte_pair';\n    cased?: boolean;\n    maxDistance?: number;\n    unknownKey?: string;\n}\n\n/**\n * A reusable class storing words embeddings for functions and class that needs it.\n */\nexport class KeyedVectors {\n    private vectors: {[key: string]: number[]};\n\n    private loaderFunction: () => Promise<string>;\n\n    readonly size: number;\n    private tokenization: 'word' | 'byte_pair';\n    private cased: boolean;\n\n    private maxDistance: number;\n    private unknownKey: string;\n\n    /**\n     * Build a KeyedVector.\n     */\n    constructor({\n        loaderFunction,\n        size,\n        tokenization = 'word',\n        cased = false,\n        maxDistance = 0.5,\n        unknownKey = undefined\n    }: KeyedVectorsArgs) {\n        if (!['word', 'byte_pair'].includes(tokenization)) {\n            throw new Error('KeyedVector tokenization setting must be \"word\" or \"byte_pair\"');\n        }\n\n        this.loaderFunction = loaderFunction;\n\n        this.size = size;\n        this.tokenization = tokenization;\n        this.cased = cased;\n\n        this.maxDistance = maxDistance;\n        this.unknownKey = unknownKey;\n    }\n\n    /**\n     * Load the word embeddings.\n     */\n    async load() {\n        this.vectors = JSON.parse(await this.loaderFunction());\n    }\n\n    /**\n     * Check if the word embeddings were loaded.\n     */\n    isLoaded(): boolean {\n        return this.vectors !== undefined;\n    }\n\n    /**\n     * Return every keys stored as an array.\n     */\n    keys(): string[] {\n        return Object.keys(this.vectors);\n    }\n\n    /**\n     * Return the vector associated with a key.\n     * If the key is not part of the vocabulary, it will use a similar key according to\n     * the leveinshtein distance.\n     * If no similar keys are below `maxDistance`, it will return the unknown key vector or\n     * undefined.\n     */\n    get(key: string): tf.Tensor1D {\n        // If the token in in the vocabulary, just use its embedding.\n        if (this.keys().includes(key)) {\n            return tf.tensor1d(this.vectors[key]);\n        }\n\n        // If the token is out of vocabulary, use the most similarly spelled token instead.\n        let bestKey: string;\n        let lowestDistance: number = Infinity;\n\n        this.keys().forEach((knownKey) => {\n            const distance = levenshteinDistance(knownKey, key);\n\n            if (distance < lowestDistance) {\n                bestKey = knownKey;\n                lowestDistance = distance;\n            }\n        });\n\n        if (lowestDistance <= this.maxDistance) {\n            return tf.tensor(this.vectors[bestKey]);\n        }\n\n        // If no tokens are enough similar, return the unknownKey vector or undefined.\n        if (this.unknownKey !== undefined) {\n            return tf.tensor(this.vectors[this.unknownKey]);\n        }\n\n        return undefined;\n    }\n\n    /**\n     * Tokenize a string at each non-word character.\n     *\n     * @param text  A non tokenized text string.\n     */\n    // eslint-disable-next-line class-methods-use-this\n    private wordTokenize(text: string): string[] {\n        return text.split(/\\W/g).filter((token) => token.length > 0);\n    }\n\n    /**\n     * Tokenize a string based on the vocabulary.\n     *\n     * @param text A non tokenized text string.\n     */\n    private bytePairTokenize(text: string): string[] {\n        const toktext = ` ${text}`.split(' ').join('▁');\n        const maxLength = this.keys().reduce((acc, curr) => acc + curr.length, 0);\n\n        const tokens = [];\n        let startPos = 0;\n        let endPos = maxLength;\n\n        while (startPos < maxLength) {\n            const token = toktext.substring(startPos, endPos);\n\n            // Simplest case : the actual token is in the vocabulary.\n            if (this.keys().includes(token)) {\n                tokens.push(token); // Add the token\n\n                // Skip the added token and start over.\n                startPos = endPos;\n                endPos = startPos + maxLength;\n\n            // No tokens were found in the vocabulary starting with the character at startPos.\n            } else if (token === '') {\n                // Add an unknown token except if the previous token is also an unknown token.\n                if (tokens[tokens.length - 1] !== this.unknownKey) tokens.push(this.unknownKey);\n\n                // Start over, at the next character.\n                startPos += 1;\n                endPos = startPos + maxLength;\n\n            // The token is a space.\n            } else if (token === '▁') {\n                // Skip the space and start over.\n                startPos += 1;\n                endPos = startPos + maxLength;\n\n            // Else, we just try removing a character at the end.\n            } else {\n                endPos -= 1;\n            }\n        }\n\n        return tokens;\n    }\n\n    /**\n     * Tokenize a string based on the settings.\n     *\n     * @param text A raw text string.\n     */\n    tokenize(text: string): string[] {\n        if (this.cased) {\n            // eslint-disable-next-line no-param-reassign\n            text = text.toLowerCase();\n        }\n\n        switch (this.tokenization) {\n        case 'word':\n            return this.wordTokenize(text);\n\n        case 'byte_pair':\n            return this.bytePairTokenize(text);\n\n        default:\n            throw new Error('KeyedVector tokenization setting must be \"word\" or \"byte_pair\"');\n        }\n    }\n}\n"],"names":["Featurizer","this","init","actions","handleQuery","query","getOptimizableFeatures","data","undefined","tf","handleAction","action","getActionMask","map","resetDialog","load","parameters","levenshteinDistance","s1","s2","d","Array","from","length","fill","i","j","Math","min","max","fuzzyMatch","text","substring","bestMatch","extract","score","aMatch","hashcode","input","hash","charCodeAt","initializeVariable","shape","scalar","initializer","heNormal","zeros","randomNormal","Error","randomTensor","apply","asScalar","variable","LSTM","inputSize","hiddenSize","outputSize","dropout","lstmKernel","lstmBias","lstmForgetBias","lstmInitH","lstmInitC","denseWeights","denseBias","initLSTM","clone","c","h","predict","x","mask","temperature","_this","nc","nh","y","matMul","add","squeeze","div","softmax","mul","weights","_this2","_this4","array","maskLUS","maskPreviousAction","LUSAction","_this3","size","embeddings","_this5","userTalked","indexOf","previousAction","includes","indexes","toLowerCase","split","word","asType","sum","use","encoder","encodeQuery","emptyEncoding","embed","squeezedEmbed","vectors","isLoaded","tokenize","token","get","filter","v","embeddingsMatrix","mean","bind","pact","_settle","featurizers","optimizer","adam","lstmTemperature","lstmDropout","Promise","all","featurizer","reduce","acc","lstm","forEach","lstmC","lstmH","features","idx","push","_this6","fitStory","story","_this8","minimize","predictions","inputs","statePred","masks","targetsMatrix","targets","predictionsMatrix","lossScalar","categoricalCrossentropy","argMax","loss","arraySync","isFailing","categoricalAccuracy","stateIdx","state","_push","train","stories","nEpochs","onEpochEnd","epochMetrics","_this10","epoch","confusionMatrix","allTargets","allPredictions","truePredictions","failingSamples","accuracy","recall","precision","allLosses","a","b","storyIdx","storyData","_this12","prediction","actionIdx","confidence","_this14","metrics","averageConfidence","confidences","json","JSON","parse","id","_this16","stringify","Slot","dependantActions","invDependantActions","isDefined","value","isDependant","isInvDependant","getValue","setValue","name","categories","threshold","categoryNames","Object","keys","oneHotValue","category","previousValue","bestValue","entries","match","keyword","m","hm","currentUneqPrevious","bestEqPrevious","betterScore","_Slot","result","source","inputAnswered","line","newStory","exec","newInput","newAction","testSize","test","splice","floor","random","slots","model","ask","digest","turnConfidence","slot","loaderFunction","tokenization","cased","maxDistance","unknownKey","key","bestKey","lowestDistance","Infinity","knownKey","distance","wordTokenize","bytePairTokenize","toktext","join","maxLength","curr","tokens","startPos","endPos"],"mappings":"6FASaA,aAAb,aAeaC,UAAe,EAf5B,2BAuBUC,cAAKC,cACPF,KAAKE,QAAUA,oBAxBvB,sCAqCUC,qBAAYC,+BAelBC,uBAAA,SAAuBC,GACnB,YAAaC,IAATD,EACOE,QAAS,CAAC,IAGAF,KAOzBG,aAAA,SAAaC,OAQbC,cAAA,WACI,YAAYT,QAAQU,IAAI,yBAO5BC,YAAA,eAMAC,KAAA,SAAKC,yBAOD,uBAAO,iUClGCC,EAAoBC,EAAYC,GAM5C,IALA,IAAMC,EAAIC,MAAMC,KACZD,MAAMH,EAAGK,OAAS,GAClB,sBAAUF,MAAMF,EAAGI,OAAS,GAAGC,KAAK,KAG/BC,EAAI,EAAGA,GAAKP,EAAGK,OAAQE,GAAK,EACjCL,EAAEK,GAAG,GAAKA,EAGd,IAAK,IAAIC,EAAI,EAAGA,GAAKP,EAAGI,OAAQG,GAAK,EACjCN,EAAE,GAAGM,GAAKA,EAGd,IAAK,IAAIA,EAAI,EAAGA,EAAIP,EAAGI,OAAQG,GAAK,EAChC,IAAK,IAAID,EAAI,EAAGA,EAAIP,EAAGK,OAAQE,GAAK,EAGhCL,EAAEK,EAAI,GAAGC,EAAI,GAAKC,KAAKC,IACnBR,EAAEK,GAAGC,EAAI,GAAK,EACdN,EAAEK,EAAI,GAAGC,GAAK,EACdN,EAAEK,GAAGC,IALiBR,EAAGO,KAAON,EAAGO,GAAM,EAAI,IAUzD,OAAON,EAAEF,EAAGK,QAAQJ,EAAGI,QAAUI,KAAKE,IAAIX,EAAGK,OAAQJ,EAAGI,OAAQ,YC1BpDO,EAAWC,EAAcC,GAGrC,IAFA,IAAIC,EAAmB,CAAEC,aAAS1B,EAAW2B,MAAO,GAE3CV,EAAI,EAAGA,EAAIM,EAAKR,OAAQE,GAAK,EAAG,CACrC,IAAMS,EAAUH,EAAKC,UAAUP,EAAGA,EAAIO,EAAUT,QAE1Ca,EAAS,CACXF,QAAAA,EACAC,MAAO,EAAIlB,EAAoBiB,EAASF,IAG5C,GAAqB,IAAjBI,EAAOD,MACP,OAAOC,EAGPA,EAAOD,MAAQF,EAAUE,QACzBF,EAAYG,GAIpB,OAAOH,WClBKI,EAASC,GAGrB,IAFA,IAAIC,EAAO,EAEFd,EAAI,EAAGA,EAAIa,EAAMf,OAAQE,GAAK,EAGnCc,GAASA,GAAQ,GAAKA,EAFVD,EAAME,WAAWf,GAG7Bc,GAAQ,EAGZ,OAAOA,WCXKE,EAAmBC,EAAiBC,EAChDzC,GACA,gBAFgDyC,IAAAA,GAAkB,YAClEzC,IAAAA,EAA8B,MACvBO,OAAQ,WACX,IAAImC,EAEJ,OAAQ1C,GACR,IAAK,KACD0C,EAAcnC,eAAgBoC,SAAS,IACvC,MAEJ,IAAK,QACDD,EAAcnC,eAAgBqC,QAC9B,MAEJ,IAAK,SACDF,EAAcnC,eAAgBsC,aAAa,IAC3C,MAEJ,QACI,UAAUC,8EACkE9C,QAIhF,IAAI+C,EAAeL,EAAYM,MAAMR,GAGrC,OAFIC,IAAQM,EAAeA,EAAaE,YAEjCF,EAAaG,iBCxBfC,aAqBT,WAAYC,EAAmBC,EAAoBC,EAAoBC,YAAAA,IAAAA,EAAkB,IACrFxD,KAAKyD,WAAajB,EAAmB,CAACa,EAAYC,EAAyB,EAAbA,IAC9DtD,KAAK0D,SAAWlB,EAAmB,CAAc,EAAbc,IAAiB,EAAO,SAC5DtD,KAAK2D,eAAiBnB,EAAmB,CAAC,IAAI,EAAM,SACpDxC,KAAK4D,UAAYpB,EAAmB,CAAC,EAAGc,IACxCtD,KAAK6D,UAAYrB,EAAmB,CAAC,EAAGc,IAExCtD,KAAK8D,aAAetB,EAAmB,CAACc,EAAYC,IACpDvD,KAAK+D,UAAYvB,EAAmB,CAACe,IAAa,EAAO,SAEzDvD,KAAKwD,QAAUA,EA/BvB,2BAuCIQ,SAAA,SAASC,GACL,gBADKA,IAAAA,GAAiB,GACf,CACHC,EAAkBD,EAAQjE,KAAK6D,UAAUI,QAAUjE,KAAK6D,UACxDM,EAAkBF,EAAQjE,KAAK4D,UAAUK,QAAUjE,KAAK4D,cAWhEQ,QAAA,SAAQC,EAAgBH,EAAgBC,EAAgBG,EACpDC,cACA,gBADAA,IAAAA,EAAsB,GACf/D,OAAQ,iBAEMA,gBACDgE,EAAKb,eACHa,EAAKf,WACLe,EAAKd,SACLlD,QAAS,CAAC6D,IACxBF,EAAGD,GALAO,OAAIC,OASPC,EAAkBnE,UACTkE,EAAIF,EAAKhB,SACjBoB,OAAOJ,EAAKV,cACZe,IAAIL,EAAKT,WACTe,UACAC,IAAIR,GACJS,UACAC,UAAIX,EAAAA,EAAQ,GAKjB,MAAO,CAAEK,EAFTA,EAAIA,EAAEI,IAAIvE,MAAOmE,IAELF,GAAAA,EAAIC,GAAAA,QAOxB5D,KAAA,SAAKoE,cACD1E,OAAQ,WAEJ2E,EAAK1B,WAAajD,SAAU0E,EAAQzB,YAAYN,WAChDgC,EAAKzB,SAAWlD,SAAU0E,EAAQxB,UAAUP,WAC5CgC,EAAKxB,eAAiBnD,SAAU0E,EAAQvB,gBAAgBR,WACxDgC,EAAKvB,UAAYpD,SAAU0E,EAAQtB,WAAWT,WAC9CgC,EAAKtB,UAAYrD,SAAU0E,EAAQrB,WAAWV,WAC9CgC,EAAKrB,aAAetD,SAAU0E,EAAQpB,cAAcX,WACpDgC,EAAKpB,UAAYvD,SAAU0E,EAAQnB,WAAWZ,4CAS5BnD,4BAAAoF,EAAK3B,WAAW4B,iDAClBD,EAAK1B,SAAS2B,iDACRD,EAAKzB,eAAe0B,iDACzBD,EAAKxB,UAAUyB,iDACfD,EAAKvB,UAAUwB,iDACZD,EAAKtB,aAAauB,iDACrBD,EAAKrB,UAAUsB,0BAGpC,MAVgB,CACZ5B,aACAC,WACAC,iBACAC,YACAC,YACAC,eACAC,2BA7GZ,kMCyCI,iCAI0B,CAAEuB,SAAS,EAAMC,oBAAoB,EAAMC,UAAW,aAH5EF,QAAAA,oBACAC,mBAAAA,oBACAC,UAAAA,aAAY,eAEZhB,yBAjBU,oBAkBVA,EAAKc,QAAUA,EACfd,EAAKe,mBAAqBA,EAC1Bf,EAAKgB,UAAYA,EAEjBhB,EAAK3D,uBAvBb,2BA0BUZ,cAAKC,qDACKD,YAAKC,oBAEjBuF,EAAKC,KAAOxF,EAAQoB,OACpBmE,EAAKE,WAAanD,EAAmB,CAACiD,EAAKC,KAAMD,EAAKC,SA9B9D,sCAiCUvF,qBAAYC,aAEVJ,KADJ,uBAAOQ,OAAQ,WAIX,OAHAoF,EAAKC,WAAuB,KAAVzF,EAGGI,SACjB,CAACoF,EAAK1F,QAAQ4F,QAAQF,EAAKG,iBAC3BH,EAAK1F,QAAQoB,WAxC7B,sCA6CIjB,uBAAA,SAAuBC,GACnB,OAAqBA,EAAKsE,OAAO5E,KAAK2F,YAAYb,aAGtDrE,aAAA,SAAaC,GAETV,KAAK+F,eAAiBrF,IAAWV,KAAKwF,UAAY9E,EAASV,KAAK+F,kBAGpEpF,cAAA,WACI,IAAM2D,cAAa3D,yBAYnB,OATIX,KAAKsF,SAAWtF,KAAK6F,aACrBvB,EAAKtE,KAAKE,QAAQ4F,QAAQ9F,KAAKwF,aAAc,GAI7CxF,KAAKuF,oBAAsBvF,KAAKE,QAAQ8F,SAAShG,KAAK+F,kBACtDzB,EAAKtE,KAAKE,QAAQ4F,QAAQ9F,KAAK+F,kBAAmB,GAG/CzB,KAGXzD,YAAA,WACIb,KAAK6F,YAAa,EAClB7F,KAAK+F,oBAAiBxF,KAG1BO,KAAA,SAAKC,GACDf,KAAK2F,WAAanF,OAAQ,kBAAMA,SAAUO,EAAW4E,YAAYxC,6DAK9BnD,KAAK2F,WAAWN,0BADnD,MAAO,CACHM,gBAjFZ,uCAAsC5F,mBCnBlC,WAAY2F,gBACRlB,yBAPU,eAQVA,EAAKkB,KAAOA,8BAGVvF,qBAAYC,aAI0BJ,KAHxC,uBAAOQ,OAAQ,WACX,IAAMyF,EAAU7F,EAAM8F,cACjBC,MAAM,OACNvF,IAAI,SAACwF,UAAShE,EAASgE,GAAQX,EAAKC,OAEzC,OAAqBlF,SAAUyF,EAASR,EAAKC,MAAMW,OAAO,WAAWC,IAAI,MAlBrF,uCAAyBvG,mBCHzB,8DACkB,6BAKLyE,OAAO,aANpB,2BAQUvE,cAAKC,qDACKD,YAAKC,2CACIqG,kCAArBd,EAAKe,0BAGsBf,EAAKgB,YAAY,sBAA5ChB,EAAKiB,sBAbb,sCAmBkBD,qBAAYrG,8BACFJ,KAAKwG,QAAQG,MAAM,CAACvG,mBAAlCuG,GACN,IAAMC,EAA8BD,EAAM7B,UAG1C,OAFAtE,UAAWmG,GAEJC,IAxBf,sCA2BUzG,qBAAYC,OAEd,uBAAKA,EACMJ,KAGCyG,YAAYrG,GAHbJ,KAAK0G,cAAczC,SA9BtC,uCAAyBlE,6BCcrB,WAAY8G,gBACRrC,yBATU,iBAUVA,EAAKkB,KAAO,EAAImB,EAAQnB,KACxBlB,EAAKqC,QAAUA,WAZvB,2BAeU5G,cAAKC,qDACKD,YAAKC,qCAEjB,IAAKuF,EAAKoB,QAAQC,kCAAkBrB,EAAKoB,QAAQ/F,yEAlBzD,sCAqBUX,qBAAYC,aAEKJ,KADnB,uBAAqBQ,OAAQ,WACzB,IACMmF,EADSC,EAAKiB,QAAQE,SAAS3G,GAEhCQ,IAAI,SAACoG,YAAeH,QAAQI,IAAID,KAChCE,OAAO,SAACC,eAAY5G,IAAN4G,IAGnB,GAA0B,IAAtBxB,EAAWrE,OACX,OAAOd,QAAS,CAACoF,EAAKF,OAG1B,IAAM0B,EAAmB5G,QAASmF,GAElC,OAAOnF,SAAU,CAAC4G,EAAiBC,KAAK,GAAID,EAAiBxF,IAAI,QAnC7E,uCAAmC7B,ICsI5B,sDAaH,kBAHkBuH,KACV,qEAOuB,8CASFC,aA3K1B,8GAmFJ,4GAqCK/C,IACHgD,qKAgW8BL,qmBArYnC,qCAoBI,kBACIjH,IAAAA,QACAuH,IAAAA,gBACAnE,WAAAA,aAAa,SACboE,UAAAA,aAAYlH,QAASmH,KAAK,WAC1BpD,YAAAA,aAAc,QACdf,QAAAA,aAAU,IAEVxD,KAAKE,QAAUA,EACfF,KAAKyH,YAAcA,EACnBzH,KAAK0H,UAAYA,EAEjB1H,KAAKsD,WAAaA,EAClBtD,KAAKuD,WAAarD,EAAQoB,OAE1BtB,KAAK4H,gBAAkBrD,EACvBvE,KAAK6H,YAAcrE,EApC3B,2BA0CUvD,0BAGED,4BADE8H,QAAQC,IACV5C,EAAKsC,YAAY7G,IAAI,SAACoH,UAAeA,EAAW/H,KAAKkF,EAAKjF,6BAI9DiF,EAAK9B,UAAY8B,EAAKsC,YACjB7G,IAAI,SAACoH,UAAeA,EAAWtC,OAC/BuC,OAAO,SAACC,EAAKxC,UAASwC,EAAMxC,GAAM,GAEvCP,EAAKgD,KAAO,IAAI/E,EAAK+B,EAAK9B,UAAW8B,EAAK7B,WAAY6B,EAAK5B,WAAY4B,EAAK0C,aAE5E1C,EAAKtE,gBAvDb,sCA6DIA,YAAA,WACIb,KAAKyH,YAAYW,QAAQ,SAACJ,UAAeA,EAAWnH,sBAChBb,KAAKmI,KAAKnE,WAAxChE,KAAKqI,QAARnE,EAAkBlE,KAAKsI,QAARnE,KAMRhE,qBAAYC,OACtB,OAAO0H,QAAQC,IACX/H,KAAKyH,YAAY7G,IAAI,SAACoH,UAAeA,EAAW7H,YAAYC,MAvExE,sCA8EYC,uBAAA,SAAuBkI,cAC3B,OAAO/H,OAAQ,WACX,IAAMmF,EAAaC,EAAK6B,YAAY7G,IAChC,SAACoH,EAAYQ,UAAQR,EAAW3H,uBAAuBkI,EAASC,MAMpE,OAFA7C,EAAW8C,KAAKjI,QAAS,CAAC,KAELA,SAAUmF,QAO/BlF,aAAA,SAAaC,GACjBV,KAAKyH,YAAY7G,IAAI,SAACoH,UAAeA,EAAWvH,aAAaC,QAMzDC,cAAA,sBACJ,OAAOH,OAAQ,kBAAMkI,EAAKjB,YAErB7G,IAAI,SAACoH,UAA6BxH,SAC/BwH,EAAWrH,qBACXJ,EAAW,aAGd0H,OAAO,SAACC,EAAK5D,UAAS9D,MAAO0H,EAAK5D,IAAO9D,OAAQ,CAACkI,EAAKxI,QAAQoB,eAM1DqH,kBAASC,wBA6BnB,IAAItI,EA8CJ,OA5CAuI,EAAKnB,UAAUoB,SAAS,iBACLD,EAAKV,KAAKnE,UAAS,GAA5BE,IAAAA,EAAGC,IAAAA,EAGH4E,EAAcC,EAAOpI,IAAI,SAAC2H,EAAUC,GACtC,IAAMS,EAAYJ,EAAKV,KAAK/D,QACVyE,EAAKxI,uBAAuBkI,GAC5BrE,EACAC,EACA+E,EAAMV,IAMxB,OAHAtE,EAAI+E,EAAUxE,GACdN,EAAI8E,EAAUvE,GAEPuE,EAAUtE,IAGfwE,EAAgB3I,QAAS4I,GACzBC,EAAoB7I,QAASuI,GAG7BO,EAAyB9I,UAAW+I,wBACtCJ,EAAeE,GACjBhC,OAcF,OAXA/G,EAAO,CACH8I,QAAS5I,OAAQ2I,EAAcK,OAAO,IACtCT,YAAavI,OAAQ6I,EAAkBG,OAAO,IAC9CC,KAAeH,EAAWI,YAC1BC,UAAWnJ,UACNoJ,oBAAoBT,EAAeE,GACnChC,OACAqC,YAAc,MAIhBJ,IAIX9I,UAAW,CAACwI,EAAQI,IAEb9I,KA1EPN,KAAA6I,EAAKhI,cAGL,IAAMmI,EAAkB,GAClBE,EAAuB,GACvBE,EAAyB,GAGtBS,EAAW,wBAAGA,EAAWjB,EAAMtH,4BAAQuI,GAAY,eACxD,IAAMC,EAAQlB,EAAMiB,KAIpBb,EAAOP,4BAAWI,EAAK1I,YAAY2J,EAAM1J,yBAAzC2J,OAAAf,KAEAE,EAAMT,KAAKI,EAAKlI,iBAEhByI,EAAQX,KACUjI,SACVqI,EAAK3I,QAAQ4F,QAAQgE,EAAMpJ,QAC3BmI,EAAKtF,aAIbsF,EAAKpI,aAAaqJ,EAAMpJ,4DA5IpC,sCAsMUsJ,sBAAQC,IAAAA,YAASC,QAAAA,aAAU,SAAIC,WAAAA,kBAAa5J,YAC1C6J,eAiEJ,OADAC,EAAKxJ,cACEuJ,KApDyBpK,KAVvBsK,EAAQ,wBAAGA,EAAQJ,uBAASI,GAAS,4BAsB1C,IAAMC,EAAkB/J,OAAQ,kBAAMA,OAAQ+J,gBAC1C/J,SAAUgK,GACVhK,SAAUiK,GACVJ,EAAK9G,cAGHmH,EAAkBlK,OAAQ,kBAAM+J,EACjCtF,IAAIzE,YAAAA,EAAU+J,EAAgB9H,QAC9B6D,IAAI,KAET8D,EAAe,CACXE,MAAAA,EACAK,eAAAA,EAEAC,SAAmBpK,OAAQ,kBACvBkK,EAAgBpE,MAAMvB,IAAIwF,EAAgBjE,OAAOoD,cAGrDmB,OAAiBrK,OAAQ,kBACrBkK,EAAgB3F,IAAIwF,EAAgBjE,IAAI,IAAIe,OAAOqC,cAGvDoB,UAAoBtK,OAAQ,kBACxBkK,EAAgB3F,IAAIwF,EAAgBjE,IAAI,IAAIe,OAAOqC,cAGvDD,KAAMsB,EAAU9C,OAAO,SAAC+C,EAAGC,UAAMD,EAAIC,IAAKF,EAAUzJ,QAIxDd,UAAWgK,GACXhK,UAAWiK,GACXjK,UAAW,CAACkK,EAAiBH,SAEVhK,IAAf4J,GACAA,EAAWC,GAxDf,IAAMI,EAA4B,GAC5BC,EAAgC,GAChCM,EAAsB,GACtBJ,EAA2B,GAGxBO,EAAW,wBAAGA,EAAWjB,EAAQ3I,4BAAQ4J,GAAY,sCAGlCb,EAAK1B,SAASsB,EAAQiB,mBAAxCC,GAENX,EAAW/B,KAAK0C,EAAU/B,SAC1BqB,EAAehC,KAAK0C,EAAUpC,aAC9BgC,EAAUtC,KAAK0C,EAAU1B,MAErB0B,EAAUxB,WACVgB,EAAelC,KAAKyC,wFA3NxC,sCAiRU9G,iBAAQhE,aAGVJ,KAAAoL,EAAKjD,KAAK3E,QAAU,QAEH4H,EAAK/K,8CAA6B+K,EAAKjL,YAAYC,qBAApE,IAAMmI,cACAW,EAAQkC,EAAKzK,gBAEb0K,EAAaD,EAAKjD,KAAK/D,QACzBmE,EAAU6C,EAAK/C,MAAO+C,EAAK9C,MAAOY,EAAOkC,EAAKxD,iBAIlDpH,UAAW,CAAC4K,EAAK/C,MAAO+C,EAAK9C,QAC7B8C,EAAK/C,MAAQgD,EAAW5G,GAAGR,QAC3BmH,EAAK9C,MAAQ+C,EAAW3G,GAAGT,QAE3B,IAAMqH,EAAqB9K,OAAQ,kBAAM6K,EAAW1G,EAAE6E,SAASE,cACzD6B,EAAsB/K,OAAQ,kBAAM6K,EAAW1G,EAAE+E,YAAY4B,KAWnE,OARA9K,UAAW,CAAC+H,EAAUW,IACtB1I,UAAW6K,GAGXD,EAAKjD,KAAK3E,QAAU4H,EAAKvD,YAEzBuD,EAAK3K,aAAa2K,EAAKlL,QAAQoL,IAExB,CAAE5K,OAAQ0K,EAAKlL,QAAQoL,GAAYC,WAAAA,KA9SlD,sCAuTUrJ,eAAM+H,wBA2BR,IAAMM,EAAkB/J,OAAQ,kBAAMA,OAAQ+J,gBAC1C/J,SAAU4I,GAAU5I,SAAUuI,GAAcyC,EAAKjI,cAG/CmH,EAAkBlK,OAAQ,kBAAM+J,EACjCtF,IAAIzE,YAAAA,EAAU+J,EAAgB9H,QAC9B6D,IAAI,KAEHmF,EAAmB,CACrBd,eAAAA,EAEAC,SAAmBpK,OAAQ,kBACvBkK,EAAgBpE,MAAMvB,IAAIwF,EAAgBjE,OAAOoD,cAGrDmB,OAAiBrK,OAAQ,kBACrBkK,EAAgB3F,IAAIwF,EAAgBjE,IAAI,IAAIe,OAAOqC,cAGvDoB,UAAoBtK,OAAQ,kBACxBkK,EAAgB3F,IAAIwF,EAAgBjE,IAAI,IAAIe,OAAOqC,cAGvDgC,kBAAmBC,EAAY1D,OAAO,SAAC+C,EAAGC,UAAMD,EAAIC,IAAKU,EAAYrK,QAMzE,OAHAd,UAAW,CAACkK,EAAiBH,IAE7BiB,EAAK3K,cACE4K,KAhDHzL,KAPEoJ,EAAoB,GACpBL,EAAwB,GACxB4C,EAAwB,GACxBhB,EAA2B,GAGxBO,EAAW,wBAAGA,EAAWjB,EAAQ3I,4BAAQ4J,GAAY,eAC1DM,EAAK3K,cAEA,IAAIgJ,EAAW,wBAAGA,EAAWI,EAAQiB,GAAU5J,4BAAQuI,GAAY,eACpE,IAAMC,EAAQG,EAAQiB,GAAUrB,GADuC,uBAIlC2B,EAAKpH,QAAQ0F,EAAM1J,6BAAhDM,IAAAA,OAAQ6K,IAAAA,WAEhBnC,EAAQX,KAAK+C,EAAKtL,QAAQ4F,QAAQgE,EAAMpJ,SACxCqI,EAAYN,KAAK+C,EAAKtL,QAAQ4F,QAAQpF,IACtCiL,EAAYlD,KAAK8C,GAEb7K,IAAWoJ,EAAMpJ,QAAWiK,EAAe3E,SAASkF,IACpDP,EAAelC,KAAKyC,kGA5UxC,sCAqXIpK,KAAA,SAAK8K,GACD,IAAM7K,EAAa8K,KAAKC,MAAMF,GAE9B5L,KAAKyH,YAAYW,QAAQ,SAACJ,GACtBA,EAAWlH,KAAKC,EAAWiH,EAAW+D,OAG1C/L,KAAKmI,KAAKrH,KAAKC,EAAWoH,MAE1BnI,KAAKa,6CAO4Bb,4BAAAgM,EAAK7D,6CAStC,OAAO0D,KAAKI,UAAUlL,GATtB,IAAMA,EAAa,CAAEoH,QAEZK,EAAM,wBAAGA,EAAMwD,EAAKvE,YAAYnG,4BAAQkH,GAAO,eACpD,IAAMR,EAAagE,EAAKvE,YAAYe,GADmB,uBAIrBR,6BAAlCjH,EAAWiH,EAAW+D,2CA3YlC,0CChFaG,cAsBT,WAAYC,EAA4BC,gBACpC5H,sBACK2H,iBAAmBA,EACxB3H,EAAK4H,oBAAsBA,WAzBnC,2BA4BIzL,cAAA,sBACI,YAAYT,QAAQU,IAAI,SAACF,GACrB,IAAM2L,OAA2B9L,IAAf4E,EAAKmH,MACjBC,EAAcpH,EAAKgH,iBAAiBnG,SAAStF,GAC7C8L,EAAiBrH,EAAKiH,oBAAoBpG,SAAStF,GAEzD,OAAS2L,KAAeE,GAAeC,IAC/BH,IAAcG,OAI9B3L,YAAA,WACIb,KAAKsM,WAAQ/L,KAMjBkM,SAAA,WACI,YAAYH,SAMhBI,SAAA,SAASJ,GACLtM,KAAKsM,MAAQA,MAtDYvM,wDC4C7B,oBACI4M,IAAAA,KACAC,IAAAA,eACAT,qBACAC,wBACAS,UAAAA,aAAY,aAEZrI,yBAJmB,gBACG,aAKjBsI,cAAgBC,OAAOC,KAAKJ,GACjCpI,EAAKoI,WAAaA,EAClBpI,EAAKqI,UAAYA,EAEjBrI,EAAKuH,GAAQY,iBACbnI,EAAKkB,KAAO,EAAIlB,EAAKsI,cAAcxL,gBAvB3C,2BA0BUrB,cAAKC,qDACKD,YAAKC,oBACjBuF,EAAK5E,gBA5Bb,sCA+BYoM,YAAA,SAAYX,GAChB,IAAMQ,EAAgBC,OAAOC,KAAKhN,KAAK4M,YAEvC,OAAqBpM,SACjBsM,EAAchH,QAAQwG,EAAMY,UAC5BJ,EAAcxL,WAIhBnB,qBAAYC,aACQJ,KAAhBmN,EAAgBvH,EAAK6G,WACvBW,EAA8B,CAAEF,cAAU3M,EAAW0B,aAAS1B,EAAW2B,MAAO,GAGpF6K,OAAOM,QAAQzH,EAAKgH,YAAYxE,QAAQ,gBAAE8E,OAEhCI,OACD1M,IAAI,SAAC2M,UAAY1L,EAAWzB,EAAM8F,cAAeqH,EAAQrH,iBAEzDgB,OAAO,SAACsG,UAAMA,EAAEtL,OAAS0D,EAAKiH,YAE9B5E,OACG,SAACwF,EAAID,UAAOC,EAAGvL,MAAQsL,EAAEtL,MAAQuL,EAAKD,GACtC,CAAEvL,aAAS1B,EAAW2B,MAAO,IAIrC,QAAsB3B,IAAlB+M,EAAMrL,QAAuB,CAC7B,IAAMyL,EAAsBR,IAAaC,EAAcD,SACjDS,EAAiBP,EAAUF,WAAaC,EAAcD,SACtDU,EAAcN,EAAMpL,MAAQkL,EAAUlL,YAEjB3B,IAAvB6M,EAAUF,UACNQ,GAAuBE,GACvBD,GAAkBD,GAClBC,GAAkBC,KAEtBR,KAAcF,SAAAA,GAAaI,OAKvC,IAAM/E,EAAW/H,OAAQ,kBACrBA,SAAU,CACNoF,EAAKqH,YAAYG,GACjBxH,EAAKqH,YAAYrH,EAAK6G,gBAQ9B,YAJ2BlM,IAAvB6M,EAAUF,UACVtH,EAAK8G,SAASU,mBAGX7E,GApFf,sCAuFIkE,SAAA,WACI,YAAyBlM,IAArBsN,YAAMpB,oBACC,CAAES,cAAU3M,EAAW0B,aAAS1B,EAAW2B,MAAO,eAGhDuK,wBA5FgBP,oSCMrBlM,YAKH,qBAGT,oGAgBc,0BAIL8N,EAAQ,IAAcxB,MAGtBwB,8EApCb,4CCpC6BC,GACzB,IAAM9D,EAAmB,GACrBrB,EAAe,GACfoF,GAAgB,EA4DpB,OA1DAD,EAAO5H,MAAM,MAAMiC,QAAQ,SAAC6F,GACxB,IAAMC,EAAW,oBAAoBC,KAAKF,GACpCG,EAAW,cAAcD,KAAKF,GAC9BI,EAAY,eAAeF,KAAKF,GAMtB,MAAZC,GAAoBtF,EAAMtH,OAAS,GACnCsH,EAAMH,KAAa,CAAErI,MAAO,GAAIM,OAAQ,QACxCuJ,EAAQxB,KAAKG,GACbA,EAAQ,IAMW,MAAZwF,GAEHxF,EAAMtH,OAAS,SACwBf,IAAnCqI,EAAMA,EAAMtH,OAAS,GAAGZ,OACxBkI,EAAMA,EAAMtH,OAAS,GAAGZ,OAAS,MAEjCkI,EAAMH,KAAa,CAAErI,MAAO,GAAIM,OAAQ,SAIhDkI,EAAMH,KAAa,CAAErI,MAAOgO,EAAS,GAAI1N,YAAQH,IACjDyN,GAAgB,GAQI,MAAbK,GAAqBL,EAC5BpF,EAAMH,KAAa,CAAErI,MAAO,GAAIM,OAAQ2N,EAAU,KAO9B,MAAbA,GAAsBL,IAE7BpF,EAAMA,EAAMtH,OAAS,GAAGZ,OAAS2N,EAAU,GAC3CL,GAAgB,KAKpBpF,EAAMtH,OAAS,IACfsH,EAAMH,KAAa,CAAErI,MAAO,GAAIM,OAAQ,QACxCuJ,EAAQxB,KAAKG,IAGVqB,2BC7DoBA,EAAkBqE,GAI7C,IAFA,IAAMC,EAAgB,GAEb/M,EAAI,EAAGA,EAAIyI,EAAQ3I,OAASgN,EAAU9M,GAAK,EAChD+M,EAAK9F,WAAL8F,EAAatE,EAAQuE,OACjB9M,KAAK+M,MAAM/M,KAAKgN,SAAWzE,EAAQ3I,QAAS,IAIpD,MAAO,CAAE0I,MAAOC,EAASsE,KAAAA,4BF6BzB,sBAEII,MAAAA,aAAQ,SACRnJ,UAAAA,aAAY,QAEZxF,KAAK4O,QAJLA,MAKA5O,KAAK2O,MAAQA,EACb3O,KAAKwF,UAAYA,EAZzB,2BAkBUqJ,aAAIzO,aAQ6BJ,KAP7B8O,EAAoB,CACtB1O,MAAAA,EACAF,QAAS,GACT6O,eAAgB,EAChBJ,MAAO,2BAGwBxJ,EAAKyJ,MAAMxK,QAAQhE,yBAAhDM,IAAAA,OAAQ6K,IAAAA,wBAcd,OAJApG,EAAKwJ,MAAMvG,QAAQ,SAAC4G,GAChBF,EAAOH,MAAMK,EAAKjD,IAAMiD,EAAKvC,aAG1BqC,ukBAZApO,IAAWyE,EAAKK,wBAAW,OAC9BsJ,EAAO5O,QAAQuI,KAAK/H,GACpBoO,EAAOC,gBAAkBxD,kBAGOpG,EAAKyJ,MAAMxK,QAAQ,sBAAhD1D,IAAAA,OAAQ6K,IAAAA,gDAjCvB,sCA8CI1K,YAAA,WACIb,KAAK4O,MAAM/N,4CGxDf,kBACIoO,IAAAA,eACAvJ,IAAAA,SACAwJ,aAAAA,aAAe,aACfC,MAAAA,oBACAC,YAAAA,aAAc,SACdC,WAAAA,kBAAa9O,IAEb,IAAK,CAAC,OAAQ,aAAayF,SAASkJ,GAChC,UAAUnM,MAAM,kEAGpB/C,KAAKiP,eAAiBA,EAEtBjP,KAAK0F,KAAOA,EACZ1F,KAAKkP,aAAeA,EACpBlP,KAAKmP,MAAQA,EAEbnP,KAAKoP,YAAcA,EACnBpP,KAAKqP,WAAaA,EAlC1B,2BAwCUvO,0BACFd,4BAAgCmF,EAAK8J,mCAArC9J,EAAK0B,QAAUgF,KAAKC,WAzC5B,sCA+CIhF,SAAA,WACI,YAAwBvG,SAAZsG,WAMhBmG,KAAA,WACI,OAAOD,OAAOC,KAAKhN,KAAK6G,YAU5BI,IAAA,SAAIqI,GAEA,GAAItP,KAAKgN,OAAOhH,SAASsJ,GACrB,OAAO9O,WAAYR,KAAK6G,QAAQyI,IAIpC,IAAIC,EACAC,EAAyBC,SAW7B,OATAzP,KAAKgN,OAAO5E,QAAQ,SAACsH,GACjB,IAAMC,EAAW3O,EAAoB0O,EAAUJ,GAE3CK,EAAWH,IACXD,EAAUG,EACVF,EAAiBG,KAIrBH,GAAkBxP,KAAKoP,YAChB5O,SAAUR,KAAK6G,QAAQ0I,SAIVhP,IAApBP,KAAKqP,WACE7O,SAAUR,KAAK6G,QAAQ7G,KAAKqP,kBADvC,KAaIO,aAAA,SAAa9N,GACjB,OAAOA,EAAKqE,MAAM,OAAOe,OAAO,SAACF,UAAUA,EAAM1F,OAAS,OAQtDuO,iBAAA,SAAiB/N,GAQrB,IAPA,IAAMgO,OAAchO,GAAOqE,MAAM,KAAK4J,KAAK,KACrCC,EAAYhQ,KAAKgN,OAAO/E,OAAO,SAACC,EAAK+H,UAAS/H,EAAM+H,EAAK3O,QAAQ,GAEjE4O,EAAS,GACXC,EAAW,EACXC,EAASJ,EAENG,EAAWH,GAAW,CACzB,IAAMhJ,EAAQ8I,EAAQ/N,UAAUoO,EAAUC,GAGtCpQ,KAAKgN,OAAOhH,SAASgB,IACrBkJ,EAAOzH,KAAKzB,GAIZoJ,GADAD,EAAWC,GACSJ,GAGH,KAAVhJ,GAEHkJ,EAAOA,EAAO5O,OAAS,KAAOtB,KAAKqP,YAAYa,EAAOzH,KAAKzI,KAAKqP,YAIpEe,GADAD,GAAY,GACQH,GAGH,MAAVhJ,EAGPoJ,GADAD,GAAY,GACQH,EAIpBI,GAAU,EAIlB,OAAOF,KAQXnJ,SAAA,SAASjF,GAML,OALI9B,KAAKmP,QAELrN,EAAOA,EAAKoE,eAGRlG,KAAKkP,cACb,IAAK,OACD,YAAYU,aAAa9N,GAE7B,IAAK,YACD,YAAY+N,iBAAiB/N,GAEjC,QACI,UAAUiB,MAAM"}